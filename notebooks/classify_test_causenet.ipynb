{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_path = \"..\"\n",
    "sys.path.append(os.path.abspath(parent_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import pickle5\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import health_causenet\n",
    "from health_causenet import constants\n",
    "from health_causenet.causenet import (\n",
    "    CauseNet,\n",
    "    contrastive_weight,\n",
    "    term_domain_specificity,\n",
    "    discriminative_weight,\n",
    ")\n",
    "import quickumls\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "from health_bert import health_bert\n",
    "\n",
    "\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb53f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_5_to_4(path):\n",
    "    with open(path, \"rb\") as fh:\n",
    "        data = pickle5.load(fh)\n",
    "    data.to_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe12b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_5_to_4(os.path.join(constants.CEPH_PATH, \"test_causenet_predictions.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test causenet\n",
    "\n",
    "full_causenet = pd.DataFrame()\n",
    "paths = sorted(pathlib.Path(constants.CAUSENET_PARQUET_PATH).glob(\"causenet_*.parquet\"))\n",
    "pg = tqdm(paths)\n",
    "for path in pg:\n",
    "    from_file = pd.read_parquet(\n",
    "        path, columns=[\"cause\", \"effect\", \"support\", \"reference\", \"sentence\"]\n",
    "    )\n",
    "    full_causenet = pd.concat([full_causenet, from_file])\n",
    "full_causenet = full_causenet.reset_index(drop=True)\n",
    "print(\"parsing domain...\")\n",
    "print(\"computing counts...\")\n",
    "causenet = full_causenet.groupby([\"cause\", \"effect\", \"support\"]).size()\n",
    "causenet.name = \"count\"\n",
    "causenet = causenet.reset_index()\n",
    "\n",
    "print(\"sorting by support...\")\n",
    "test_causenet_support = causenet.sort_values(\n",
    "    [\"support\", \"cause\", \"effect\"], ascending=False\n",
    ").reset_index(drop=True)\n",
    "test_causenet_support = test_causenet_support.iloc[:1000].copy()\n",
    "test_causenet_support[\"dataset\"] = \"support\"\n",
    "test_causenet_random_high = (\n",
    "    causenet.loc[causenet.support >= 2].sample(1000, random_state=42).copy()\n",
    ")\n",
    "test_causenet_random_high[\"dataset\"] = \"random_support\"\n",
    "test_causenet_random_low = (\n",
    "    causenet.loc[causenet.support == 1].sample(1000, random_state=42).copy()\n",
    ")\n",
    "test_causenet_random_low[\"dataset\"] = \"random_full\"\n",
    "test_causenet = pd.concat(\n",
    "    [test_causenet_support, test_causenet_random_high, test_causenet_random_low]\n",
    ")\n",
    "test_causenet = test_causenet.reset_index(drop=True)\n",
    "\n",
    "# print(\"adding wikidata...\")\n",
    "# test_wikidata = (\n",
    "#     pd.read_csv(os.path.join(constants.WIKIDATA_PATH, \"wikidata-test.csv\"), index_col=0)\n",
    "#     .drop_duplicates()\n",
    "#     .dropna(subset=[\"cause\", \"effect\"])\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "# test_wikidata[\"dataset\"] = \"wikidata\"\n",
    "# test_causenet = test_causenet.append(\n",
    "#     test_wikidata.loc[\n",
    "#         :, [\"cause\", \"effect\", \"dataset\", \"cause_origin\", \"effect_origin\"]\n",
    "#     ]\n",
    "# ).reset_index(drop=True)\n",
    "\n",
    "test_causenet = test_causenet.drop_duplicates([\"cause\", \"effect\", \"dataset\"])\n",
    "# Load and label evaluations\n",
    "\n",
    "ignore_origins = [\n",
    "    #     \"wd:Q39833\",  # microorganism\n",
    "    #     \"wd:Q178694\",  # heredity\n",
    "    #     \"wd:Q289472\",  # biogenic substance\n",
    "    #     \"wd:Q796194\",  # medical procedure\n",
    "    #     \"wd:Q2826767\",  # disease causative agent\n",
    "    #     \"wd:Q2996394\",  # biological process\n",
    "    #     \"wd:Q5850078\",  # etiology\n",
    "    #     \"wd:Q7189713\",  # physiological condition\n",
    "    #     \"wd:Q15788410\",  # state of consciousness\n",
    "    #     \"wd:Q86746756\",  # medicinal product\n",
    "    #     \"wd:Q87075524\",  # health risk\n",
    "]\n",
    "\n",
    "with open(constants.MANUAL_EVALUATION_PATH, \"r\") as file:\n",
    "    manual_eval_dict = json.load(file)\n",
    "data = []\n",
    "for key, value in manual_eval_dict.items():\n",
    "    cause, effect = key.split(\"->\")\n",
    "    data.append({\"cause\": cause, \"effect\": effect, \"evaluation\": value})\n",
    "test_causenet_manual = test_causenet.loc[\n",
    "    test_causenet.dataset.isin([\"support\", \"random_support\", \"random_full\", \"count\"])\n",
    "]\n",
    "manual_eval = pd.DataFrame(data)\n",
    "evaluation = manual_eval.set_index([\"cause\", \"effect\"]).reindex(\n",
    "    test_causenet_manual.set_index([\"cause\", \"effect\"]).index\n",
    ")\n",
    "\n",
    "end = False\n",
    "to_label = test_causenet_manual.loc[evaluation.isna().values].values\n",
    "for idx, row in enumerate(to_label):\n",
    "    cause = row[0]\n",
    "    effect = row[1]\n",
    "    while True:\n",
    "        clear_output(wait=True)\n",
    "        inp = input(\n",
    "            f\"{idx+1}/{len(to_label)} ({len(to_label) - idx}) [{cause}] -> [{effect}]\"\n",
    "        )\n",
    "        if inp == \"c\":\n",
    "            end = True\n",
    "            break\n",
    "        try:\n",
    "            val = int(inp)\n",
    "            if val in (0, 1):\n",
    "                key = f\"{cause}->{effect}\"\n",
    "                manual_eval_dict[key] = val\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"invalid input: {inp}, needs to be either 1 or 0\")\n",
    "    if end:\n",
    "        break\n",
    "\n",
    "with open(constants.MANUAL_EVALUATION_PATH, \"w\") as file:\n",
    "    json.dump(manual_eval_dict, file, indent=4)\n",
    "\n",
    "data = []\n",
    "for key, value in manual_eval_dict.items():\n",
    "    cause, effect = key.split(\"->\")\n",
    "    data.append({\"cause\": cause, \"effect\": effect, \"evaluation\": value})\n",
    "manual_eval = pd.DataFrame(data)\n",
    "evaluation = manual_eval.set_index([\"cause\", \"effect\"]).reindex(\n",
    "    test_causenet_manual.set_index([\"cause\", \"effect\"]).index\n",
    ")\n",
    "\n",
    "print(f\"eval missing for {evaluation.isna().sum().values[0]} relations\")\n",
    "\n",
    "evaluation = evaluation.evaluation\n",
    "# evaluation = evaluation.append(test_wikidata.set_index([\"cause\", \"effect\"]).evaluation)\n",
    "test_causenet[\"evaluation\"] = evaluation.values\n",
    "\n",
    "print(\"parsing sentence test causenet\")\n",
    "sentences = (\n",
    "    full_causenet.set_index([\"cause\", \"effect\", \"support\"])\n",
    "    .loc[\n",
    "        test_causenet.loc[\n",
    "            ~test_causenet.support.isna(), [\"cause\", \"effect\", \"support\"]\n",
    "        ].values.tolist(),\n",
    "        \"sentence\",\n",
    "    ]\n",
    "    .reset_index()\n",
    "    .drop_duplicates([\"cause\", \"effect\", \"sentence\"])\n",
    ")\n",
    "sentence_test_causenet = test_causenet.merge(\n",
    "    sentences, on=[\"cause\", \"effect\", \"support\"]\n",
    ")\n",
    "sentence_test_causenet = sentence_test_causenet.drop_duplicates([\"dataset\", \"sentence\"])\n",
    "sentence_test_causenet = sentence_test_causenet.drop_duplicates(\n",
    "    [\"cause\", \"effect\", \"dataset\"]\n",
    ")\n",
    "sentence_test_causenet_evaluation = pd.read_csv(\n",
    "    os.path.join(constants.BASE_PATH, \"sentence_test_causenet_evaluations.csv\"),\n",
    "    index_col=0,\n",
    ").rename({\"label\": \"manual_evaluation\"}, axis=1)\n",
    "sentence_test_causenet = sentence_test_causenet.merge(\n",
    "    sentence_test_causenet_evaluation, on=[\"cause\", \"effect\", \"sentence\"], how=\"left\"\n",
    ")\n",
    "\n",
    "del causenet\n",
    "del full_causenet\n",
    "\n",
    "# print(\"creating ctakes and metamap data\")\n",
    "# relations = pd.Series(\n",
    "#     pd.unique(test_causenet.loc[:, [\"cause\", \"effect\"]].values.ravel())\n",
    "# )\n",
    "# relations = relations.loc[~relations.isin([\"\", \" \"])]\n",
    "# sentence_relations = pd.Series(\n",
    "#     pd.unique(sentence_test_causenet.loc[:, [\"sentence\"]].values.ravel())\n",
    "# )\n",
    "# relations = pd.concat([relations, sentence_relations]).reset_index(drop=True)\n",
    "# ctakes_path = pathlib.Path(constants.TEST_CAUSENET_PATH).joinpath(\"ctakes\")\n",
    "# ctakes_path.mkdir(exist_ok=True)\n",
    "# metamap_path = pathlib.Path(constants.TEST_CAUSENET_PATH).joinpath(\"metamap\")\n",
    "# metamap_path.mkdir(exist_ok=True)\n",
    "# for idx, relation in enumerate(relations.values):\n",
    "#     with ctakes_path.joinpath(f\"relation_{idx + 1}.txt\").open(\"w\") as file:\n",
    "#         file.write(relation)\n",
    "# with metamap_path.joinpath(\"relations.txt\").open(\"w\") as file:\n",
    "#     file.write(\"\\n\".join(f\"{idx + 1}|{relation}\" for idx, relation in enumerate(relations.values)))\n",
    "\n",
    "test_causenet.to_pickle(os.path.join(constants.TEST_CAUSENET_PATH, \"test_causenet.pkl\"))\n",
    "sentence_test_causenet.to_pickle(\n",
    "    os.path.join(constants.TEST_CAUSENET_PATH, \"sentence_test_causenet.pkl\")\n",
    ")\n",
    "test_causenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c072a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test_causenet\n",
    "test_causenet = pd.read_pickle(os.path.join(constants.TEST_CAUSENET_PATH, \"test_causenet.pkl\"))\n",
    "sentence_test_causenet = pd.read_pickle(\n",
    "    os.path.join(constants.TEST_CAUSENET_PATH, \"sentence_test_causenet.pkl\")\n",
    ")\n",
    "test_causenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628bd3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cf and compute termhood scores\n",
    "cf = pd.read_parquet(os.path.join(constants.CF_PATH, \"cf.parquet\"))\n",
    "cf.loc[cf.num_terms == 1].sum().astype(str)\n",
    "medical_termhood = {}\n",
    "for corpus in list(cf.filter(regex=r\".*_frequency_(?!open_domain)\")):\n",
    "    _cf = cf.loc[:, [\"corpus_frequency_open_domain\", corpus, \"num_terms\"]]\n",
    "    corpus = corpus.replace(\"corpus_frequency_\", \"\")\n",
    "    medical_termhood[corpus] = {}\n",
    "    print(f\"{corpus}: computing term domain specificity...\")\n",
    "    medical_termhood[corpus][\"term_domain_specificity\"] = term_domain_specificity(\n",
    "        _cf, np.e\n",
    "    )\n",
    "    print(f\"{corpus}: computing contrastive weight...\")\n",
    "    medical_termhood[corpus][\"contrastive_weight\"] = contrastive_weight(_cf, np.e, 1)\n",
    "    print(f\"{corpus}: computing_discriminative weight...\")\n",
    "    medical_termhood[corpus][\"discriminative_weight\"] = (\n",
    "        medical_termhood[corpus][\"contrastive_weight\"]\n",
    "        * medical_termhood[corpus][\"term_domain_specificity\"]\n",
    "    )\n",
    "del cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8304c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed test\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "times = {\n",
    "    \"sentence\": {\n",
    "        \"termhood\": {1: [], 2: [], 3: []},\n",
    "        \"bert\": {\" \": []}, \n",
    "        \"quickumls\": {\"full\": [], \"rx_sno\": []}, \n",
    "        \"scispacy\": {\"full\": [], \"rx_sno\": []}, \n",
    "        \"metamap\": {\"full\": [160.638], \"rx_sno\": [120.277]},\n",
    "        \"ctakes\": {\"full\": [0], \"rx_sno\": [212.711]},\n",
    "    }, \n",
    "    \"phrase\": {\n",
    "        \"termhood\": {1: [], 2: [], 3: []},\n",
    "        \"bert\": {\" \": []}, \n",
    "        \"quickumls\": {\"full\": [], \"rx_sno\": []}, \n",
    "        \"scispacy\": {\"full\": [], \"rx_sno\": []},\n",
    "        \"metamap\": {\"full\": [70.639], \"rx_sno\": [49.636]},\n",
    "        \"ctakes\": {\"full\": [0], \"rx_sno\": [119.684]},\n",
    "    }\n",
    "}\n",
    "\n",
    "n = 10\n",
    "\n",
    "model = health_bert.HealthBert.load_from_checkpoint(\n",
    "    constants.BASE_PATH + \"models/health_bert/\" + \"pubmedbert_pubmed_sentence.ckpt\"\n",
    ")\n",
    "\n",
    "for _ in tqdm(range(n), total=n):\n",
    "    # termhood\n",
    "    for n_gram in (1, 2, 3):\n",
    "        start = time.perf_counter()\n",
    "        health_causenet.causenet._contrastive_score(\n",
    "            test_causenet.loc[test_causenet.dataset == \"random_full\"],\n",
    "            medical_termhood[\"encyclopedia\"][\"discriminative_weight\"],\n",
    "            r=1,\n",
    "            n_gram_size=(1, n_gram),\n",
    "            verbose=False,\n",
    "        )\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times[\"phrase\"][\"termhood\"][n_gram].append(elapsed)\n",
    "        start = time.perf_counter()\n",
    "        health_causenet.causenet._contrastive_score(\n",
    "            sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"].assign(\n",
    "                cause=sentence_test_causenet.sentence, effect=\"\"\n",
    "            ),\n",
    "            medical_termhood[\"encyclopedia\"][\"discriminative_weight\"],\n",
    "            p=1,\n",
    "            n_gram_size=(1, n_gram),\n",
    "            verbose=False,\n",
    "        )\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times[\"sentence\"][\"termhood\"][n_gram].append(elapsed)\n",
    "\n",
    "    # bert\n",
    "    start = time.perf_counter()\n",
    "    health_causenet.causenet._health_bert(\n",
    "        test_causenet.loc[test_causenet.dataset == \"random_full\"],\n",
    "        model,\n",
    "        verbose=False,\n",
    "        batch_size=1,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    start = time.perf_counter()\n",
    "    times[\"phrase\"][\"bert\"][\" \"].append(elapsed)\n",
    "    health_causenet.causenet._health_bert(\n",
    "        sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"].assign(\n",
    "            cause=sentence_test_causenet.sentence, effect=\"\"\n",
    "        ),\n",
    "        model,\n",
    "        batch_size=1,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"sentence\"][\"bert\"][\" \"].append(elapsed)\n",
    "\n",
    "    # quickumls\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        test_causenet.loc[test_causenet.dataset == \"random_full\"],\n",
    "        \"quickumls\",\n",
    "        jaccard_threshold=0.9,\n",
    "        umls_subset=\"full\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"phrase\"][\"quickumls\"][\"full\"].append(elapsed)\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        test_causenet.loc[test_causenet.dataset == \"random_full\"],\n",
    "        \"quickumls\",\n",
    "        jaccard_threshold=0.9,\n",
    "        umls_subset=\"rx_sno\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"phrase\"][\"quickumls\"][\"rx_sno\"].append(elapsed)\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"].assign(\n",
    "            cause=sentence_test_causenet.sentence, effect=\"\"\n",
    "        ),\n",
    "        \"quickumls\",\n",
    "        jaccard_threshold=0.9,\n",
    "        umls_subset=\"full\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"sentence\"][\"quickumls\"][\"full\"].append(elapsed)\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"].assign(\n",
    "            cause=sentence_test_causenet.sentence, effect=\"\"\n",
    "        ),\n",
    "        \"quickumls\",\n",
    "        jaccard_threshold=0.9,\n",
    "        umls_subset=\"rx_sno\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"sentence\"][\"quickumls\"][\"rx_sno\"].append(elapsed)\n",
    "\n",
    "    # scispacy\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        test_causenet.loc[test_causenet.dataset == \"random_full\"],\n",
    "        \"scispacy\",\n",
    "        threshold=0.9,\n",
    "        umls_subset=\"full\",\n",
    "        model=\"en_core_sci_sm\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"phrase\"][\"scispacy\"][\"full\"].append(elapsed)\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        test_causenet.loc[test_causenet.dataset == \"random_full\"],\n",
    "        \"scispacy\",\n",
    "        threshold=0.9,\n",
    "        umls_subset=\"rx_sno\",\n",
    "        model=\"en_core_sci_sm\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"phrase\"][\"scispacy\"][\"rx_sno\"].append(elapsed)\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"].assign(\n",
    "            cause=sentence_test_causenet.sentence, effect=\"\"\n",
    "        ),\n",
    "        \"scispacy\",\n",
    "        threshold=0.9,\n",
    "        umls_subset=\"full\",\n",
    "        model=\"en_core_sci_sm\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"sentence\"][\"scispacy\"][\"full\"].append(elapsed)\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"].assign(\n",
    "            cause=sentence_test_causenet.sentence, effect=\"\"\n",
    "        ),\n",
    "        \"scispacy\",\n",
    "        threshold=0.9,\n",
    "        umls_subset=\"rx_sno\",\n",
    "        model=\"en_core_sci_sm\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"sentence\"][\"scispacy\"][\"rx_sno\"].append(elapsed)\n",
    "\n",
    "\n",
    "def create_index(index, dictionary, index_terms):\n",
    "    for key, value in dictionary.items():\n",
    "        index_terms.append(key)\n",
    "        if isinstance(value, list):\n",
    "            index.append(tuple(index_terms))\n",
    "            index_terms = index_terms[:-1]\n",
    "        else:\n",
    "            _, index_terms = create_index(index, value, index_terms)\n",
    "    index_terms = index_terms[:-1]\n",
    "    return index, index_terms\n",
    "\n",
    "def grab_values(dictionary):\n",
    "    values = []\n",
    "    for value in dictionary.values():\n",
    "        if isinstance(value, dict):\n",
    "            values.extend(grab_values(value))\n",
    "        else:\n",
    "            values.append(sum(value) / len(value))\n",
    "    return values\n",
    "\n",
    "index, _ = create_index([], times, [])\n",
    "index\n",
    "values = grab_values(times)\n",
    "times_df = pd.DataFrame(values, pd.MultiIndex.from_tuples(index, names=[\"data\", \"method\", \"n-gram\"]), columns=[\"time\"])\n",
    "num_samples = pd.Series(\n",
    "    [\n",
    "        test_causenet.loc[test_causenet.dataset == \"random_full\"].shape[0],\n",
    "        sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"].shape[0]\n",
    "    ],\n",
    "    index=pd.Index([\"phrase\", \"sentence\"], name=\"data\"),\n",
    ")\n",
    "times_df[\"time_per_iter\"] = (times_df[\"time\"] / num_samples) * 1000\n",
    "ratios = times_df[\"time\"].values[:, None] / times_df[\"time\"].values[None, :]\n",
    "ratio_df = pd.DataFrame(ratios, index=times_df.index, columns=times_df.index)\n",
    "\n",
    "from IPython.display import display\n",
    "display(times_df)\n",
    "display(ratio_df.loc[\"sentence\", \"sentence\"])\n",
    "display(ratio_df.loc[\"phrase\", \"phrase\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd9f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(times_df)\n",
    "display(ratio_df.loc[\"sentence\", \"sentence\"])\n",
    "display(ratio_df.loc[\"phrase\", \"phrase\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(test_causenet_predictions, label):\n",
    "    drop_labels = [\"medical_score-cause\", \"medical_score-effect\"]\n",
    "    pg.set_description(\" \".join(label))\n",
    "    st21pv = \"st21pv\" in label\n",
    "    if label[0] == \"health_bert\":\n",
    "        _, name, corpus, text_type = label\n",
    "        model_path = constants.BASE_PATH + \"models/health_bert/\"\n",
    "        model = health_bert.HealthBert.load_from_checkpoint(\n",
    "            model_path + f\"{name}_{corpus}_{text_type}.ckpt\"\n",
    "        )\n",
    "        medical_score = health_causenet.causenet._health_bert(\n",
    "            test_causenet_predictions,\n",
    "            model,\n",
    "            verbose=False,\n",
    "        )\n",
    "    elif label[0] in (\"metamap\", \"ctakes\"):\n",
    "        json_path = os.path.join(\n",
    "            constants.BASE_PATH, \"tagger_jsons\", f\"{label[0]}-{label[1]}.jsonl\"\n",
    "        )\n",
    "        medical_score = CauseNet.is_medical(\n",
    "            test_causenet_predictions, \"tagger\", json_path=json_path, st21pv=st21pv\n",
    "        )\n",
    "    elif label[0] == \"scispacy\":\n",
    "        umls_subset, model, threshold = label[1:4]\n",
    "        medical_score = CauseNet.is_medical(\n",
    "            test_causenet_predictions,\n",
    "            \"scispacy\",\n",
    "            umls_subset=umls_subset,\n",
    "            model=model,\n",
    "            threshold=float(threshold),\n",
    "            verbose=False,\n",
    "            st21pv=st21pv,\n",
    "        )\n",
    "    elif label[0] == \"quickumls\":\n",
    "        umls_subset, jaccard_threshold = label[1:3]\n",
    "        jaccard_threshold = float(jaccard_threshold)\n",
    "        medical_score = CauseNet.is_medical(\n",
    "            test_causenet_predictions,\n",
    "            \"quickumls\",\n",
    "            jaccard_threshold=jaccard_threshold,\n",
    "            umls_subset=umls_subset,\n",
    "            st21pv=st21pv,\n",
    "            verbose=False,\n",
    "        )\n",
    "    elif label[0] in contrastive_scores:\n",
    "        contrastive_score, corpus, n_gram_size, r_value = label\n",
    "        n_gram_size = tuple(n_gram_size.strip(\"()\").split(\", \"))\n",
    "        n_gram_size = (int(n_gram_size[0]), int(n_gram_size[1]))\n",
    "        neg = \"neg_\" in r_value\n",
    "        if neg:\n",
    "            r_value = r_value[4:]\n",
    "        try:\n",
    "            r_value = int(r_value)\n",
    "        except:\n",
    "            r_value = float(r_value)\n",
    "        if neg:\n",
    "            r_value = r_value * -1\n",
    "        medical_score = health_causenet.causenet._contrastive_score(\n",
    "            test_causenet_predictions,\n",
    "            medical_termhood[corpus][contrastive_score],\n",
    "            r=r_value,\n",
    "            n_gram_size=n_gram_size,\n",
    "            verbose=False,\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(f\"unknown label {label}\")\n",
    "    suffix = \"-\" + \"-\".join(label)\n",
    "    medical_score.index = test_causenet_predictions.index\n",
    "    medical_score = medical_score.add_suffix(suffix)\n",
    "    return medical_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00708738",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_causenet_predictions = pd.read_pickle(\n",
    "        os.path.join(constants.TEST_CAUSENET_PATH, \"test_causenet_predictions.pkl\")\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    test_causenet_predictions = test_causenet.loc[:, [\"cause\", \"effect\", \"dataset\"]]\n",
    "test_causenet_predictions = pd.read_pickle(\n",
    "    os.path.join(constants.TEST_CAUSENET_PATH, \"test_causenet_predictions.pkl\")\n",
    ")\n",
    "test_causenet_predictions = test_causenet_predictions.merge(\n",
    "    test_causenet.drop(\n",
    "        [\"support\", \"count\", \"evaluation\"], axis=1\n",
    "    ),\n",
    "    on=[\"cause\", \"effect\", \"dataset\"],\n",
    "    how=\"outer\",\n",
    ")\n",
    "\n",
    "jaccard_thresholds = [round(thresh, 2) for thresh in np.arange(0.7, 1.01, 0.1)]\n",
    "scispacy_thresholds = [round(thresh, 2) for thresh in np.arange(0.6, 0.9, 0.1)]\n",
    "scispacy_models = [\"en_core_sci_sm\", \"en_core_sci_lg\"]\n",
    "r_values = [-float(\"inf\"), -10, -5, -2, -1, 0, 1, 2, 5, 10, float(\"inf\")]\n",
    "n_gram_sizes = [(1, 1), (1, 2), (1, 3)]\n",
    "medical_corpora = [\n",
    "    \"pubmed\",\n",
    "    \"textbook\",\n",
    "    \"pubmed_central\",\n",
    "    \"encyclopedia\",\n",
    "]\n",
    "umls_subsets = [\n",
    "    \"full\",\n",
    "    \"rx_sno\",\n",
    "]\n",
    "text_types = [\"sentence\", \"noun_phrase\"]\n",
    "contrastive_scores = [\n",
    "    \"term_domain_specificity\",\n",
    "    \"contrastive_weight\",\n",
    "    \"discriminative_weight\",\n",
    "]\n",
    "bert_names = [\"bert\", \"scibert\", \"pubmedbert\"]\n",
    "\n",
    "labels = (\n",
    "    [\n",
    "        f\"quickumls-{umls_subset}-{jaccard_threshold}-st21pv\"\n",
    "        if st21pv\n",
    "        else f\"quickumls-{umls_subset}-{jaccard_threshold}\"\n",
    "        for st21pv in [True, False]\n",
    "        for umls_subset in umls_subsets\n",
    "        for jaccard_threshold in jaccard_thresholds\n",
    "    ]\n",
    "    + [\n",
    "        f\"scispacy-{umls_subset}-{model}-{threshold}-st21pv\"\n",
    "        if st21pv\n",
    "        else f\"scispacy-{umls_subset}-{model}-{threshold}\"\n",
    "        for st21pv in [True, False]\n",
    "        for model in scispacy_models\n",
    "        for umls_subset in umls_subsets\n",
    "        for threshold in scispacy_thresholds\n",
    "    ]\n",
    "    + [\n",
    "        f\"ctakes-{umls_subset}-st21pv\" if st21pv else f\"ctakes-{umls_subset}\"\n",
    "        for st21pv in [True, False]\n",
    "        for umls_subset in umls_subsets\n",
    "    ]\n",
    "    + [\n",
    "        f\"metamap-{umls_subset}-st21pv\" if st21pv else f\"metamap-{umls_subset}\"\n",
    "        for st21pv in [True, False]\n",
    "        for umls_subset in umls_subsets\n",
    "    ]\n",
    "    + [\n",
    "        f\"{contrastive_score}-{medical_corpus}-{n_gram_size}-{r_value}\"\n",
    "        if r_value >= 0\n",
    "        else f\"{contrastive_score}-{medical_corpus}-{n_gram_size}-neg_{-1 * r_value}\"\n",
    "        for contrastive_score in contrastive_scores\n",
    "        for n_gram_size in n_gram_sizes\n",
    "        for r_value in r_values\n",
    "        for medical_corpus in medical_corpora\n",
    "    ]\n",
    "    + [\n",
    "        f\"health_bert-{name}-{medical_corpus}-{text_type}\"\n",
    "        for name in bert_names\n",
    "        for medical_corpus in [\"pubmed\", \"encyclopedia\"]\n",
    "        for text_type in text_types\n",
    "    ]\n",
    ")\n",
    "df_labels = [\n",
    "    f\"medical_score-{relation}-{label}\"\n",
    "    for relation in [\"cause\", \"effect\"]\n",
    "    for label in labels\n",
    "]\n",
    "labels = [label.split(\"-\") for label in labels]\n",
    "new_columns = []\n",
    "for label in df_labels:\n",
    "    if label not in test_causenet_predictions:\n",
    "        new_columns.append(\n",
    "            pd.Series(np.nan, index=test_causenet_predictions.index, name=label)\n",
    "        )\n",
    "test_causenet_predictions = pd.concat([test_causenet_predictions, *new_columns], axis=1)\n",
    "\n",
    "missing_rows = test_causenet_predictions[df_labels].isna().all(1)\n",
    "if missing_rows.any():\n",
    "    print(f\"parsing {missing_rows.sum()} missing rows\")\n",
    "    pg = tqdm(labels)\n",
    "    for label in pg:\n",
    "        replace_rows = classify_test_set(\n",
    "            test_causenet_predictions.loc[missing_rows, [\"cause\", \"effect\"]], label,\n",
    "        )\n",
    "        label = \"-\".join(label)\n",
    "        columns = [f\"medical_score-cause-{label}\", f\"medical_score-effect-{label}\"]\n",
    "        replace_rows = replace_rows.loc[:, columns]\n",
    "        test_causenet_predictions.loc[missing_rows, columns] = replace_rows.values\n",
    "        test_causenet_predictions.to_pickle(\n",
    "            os.path.join(constants.TEST_CAUSENET_PATH, \"test_causenet_predictions.pkl\")\n",
    "        )\n",
    "\n",
    "score_column = test_causenet_predictions.columns.str.startswith(\"medical_score-\")\n",
    "isna = test_causenet_predictions.isna().any(0)\n",
    "missing_columns = list(test_causenet_predictions.loc[:, score_column & isna])\n",
    "missing_columns = [\n",
    "    \"-\".join(missing_column.split(\"-\")[2:]) for missing_column in missing_columns\n",
    "]\n",
    "missing_columns = list(set(missing_columns))\n",
    "missing_columns = sorted([missing_column.split(\"-\") for missing_column in missing_columns])\n",
    "\n",
    "if missing_columns:\n",
    "    print(\"parsing missing columns\")\n",
    "    for missing_column in missing_columns:\n",
    "        print(\" \".join(missing_column))\n",
    "    pg = tqdm(missing_columns)\n",
    "    for label in pg:\n",
    "        replace_columns = classify_test_set(\n",
    "            test_causenet_predictions.loc[:, [\"cause\", \"effect\"]], label,\n",
    "        )\n",
    "        label = \"-\".join(label)\n",
    "        columns = [f\"medical_score-cause-{label}\", f\"medical_score-effect-{label}\"]\n",
    "        replace_columns = replace_columns.loc[:, columns]\n",
    "        test_causenet_predictions.loc[:, columns] = replace_columns.values\n",
    "        assert not test_causenet_predictions.loc[:, columns].isna().any().any()\n",
    "        test_causenet_predictions.to_pickle(\n",
    "            os.path.join(constants.TEST_CAUSENET_PATH, \"test_causenet_predictions.pkl\")\n",
    "        )\n",
    "\n",
    "assert not test_causenet_predictions.loc[:, score_column].isna().any().any()\n",
    "test_causenet.merge(\n",
    "    test_causenet_predictions, on=[\"cause\", \"effect\", \"dataset\"], how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081afecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sentence_test_causenet_predictions = pd.read_pickle(\n",
    "        os.path.join(constants.TEST_CAUSENET_PATH, \"sentence_test_causenet_predictions.pkl\")\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    sentence_test_causenet_predictions = sentence_test_causenet.loc[\n",
    "        :, [\"cause\", \"effect\", \"dataset\", \"sentence\"]\n",
    "    ]\n",
    "\n",
    "sentence_test_causenet_predictions = sentence_test_causenet_predictions.merge(\n",
    "    sentence_test_causenet.drop(\n",
    "        [\n",
    "            \"support\",\n",
    "            \"count\",\n",
    "            \"evaluation\",\n",
    "            \"manual_evaluation\"\n",
    "        ],\n",
    "        axis=1,\n",
    "    ),\n",
    "    on=[\"cause\", \"effect\", \"sentence\", \"dataset\"],\n",
    "    how=\"outer\",\n",
    ")\n",
    "\n",
    "jaccard_thresholds = [round(thresh, 2) for thresh in np.arange(0.7, 1.01, 0.1)]\n",
    "scispacy_thresholds = [round(thresh, 2) for thresh in np.arange(0.6, 0.9, 0.1)]\n",
    "scispacy_models = [\"en_core_sci_sm\", \"en_core_sci_lg\"]\n",
    "r_values = [-float(\"inf\"), -10, -5, -2, -1, 0, 1, 2, 5, 10, float(\"inf\")]\n",
    "n_gram_sizes = [(1, 1), (1, 2), (1, 3)]\n",
    "medical_corpora = [\n",
    "    \"pubmed\",\n",
    "    \"textbook\",\n",
    "    \"pubmed_central\",\n",
    "    \"encyclopedia\",\n",
    "]\n",
    "umls_subsets = [\n",
    "    \"full\",\n",
    "    \"rx_sno\",\n",
    "]\n",
    "text_types = [\"sentence\", \"noun_phrase\"]\n",
    "contrastive_scores = [\n",
    "    \"term_domain_specificity\",\n",
    "    \"contrastive_weight\",\n",
    "    \"discriminative_weight\",\n",
    "]\n",
    "bert_names = [\"bert\", \"scibert\", \"pubmedbert\"]\n",
    "\n",
    "labels = (\n",
    "    [\n",
    "        f\"quickumls-{umls_subset}-{jaccard_threshold}-st21pv\"\n",
    "        if st21pv\n",
    "        else f\"quickumls-{umls_subset}-{jaccard_threshold}\"\n",
    "        for st21pv in [True, False]\n",
    "        for umls_subset in umls_subsets\n",
    "        for jaccard_threshold in jaccard_thresholds\n",
    "    ]\n",
    "    + [\n",
    "        f\"scispacy-{umls_subset}-{model}-{threshold}-st21pv\"\n",
    "        if st21pv\n",
    "        else f\"scispacy-{umls_subset}-{model}-{threshold}\"\n",
    "        for st21pv in [True, False]\n",
    "        for model in scispacy_models\n",
    "        for umls_subset in umls_subsets\n",
    "        for threshold in scispacy_thresholds\n",
    "    ]\n",
    "    + [\n",
    "        f\"ctakes-{umls_subset}-st21pv\" if st21pv else f\"ctakes-{umls_subset}\"\n",
    "        for st21pv in [True, False]\n",
    "        for umls_subset in umls_subsets\n",
    "    ]\n",
    "    + [\n",
    "        f\"metamap-{umls_subset}-st21pv\" if st21pv else f\"metamap-{umls_subset}\"\n",
    "        for st21pv in [True, False]\n",
    "        for umls_subset in umls_subsets\n",
    "    ]\n",
    "    + [\n",
    "        f\"{contrastive_score}-{medical_corpus}-{n_gram_size}-{r_value}\"\n",
    "        if r_value >= 0\n",
    "        else f\"{contrastive_score}-{medical_corpus}-{n_gram_size}-neg_{-1 * r_value}\"\n",
    "        for contrastive_score in contrastive_scores\n",
    "        for n_gram_size in n_gram_sizes\n",
    "        for r_value in r_values\n",
    "        for medical_corpus in medical_corpora\n",
    "    ]\n",
    "    + [\n",
    "        f\"health_bert-{name}-{medical_corpus}-{text_type}\"\n",
    "        for name in bert_names\n",
    "        for medical_corpus in [\"pubmed\", \"encyclopedia\"]\n",
    "        for text_type in text_types\n",
    "    ]\n",
    ")\n",
    "df_labels = [\n",
    "    f\"medical_score-{relation}-{label}\"\n",
    "    for relation in [\"cause\", \"effect\"]\n",
    "    for label in labels\n",
    "]\n",
    "labels = [label.split(\"-\") for label in labels]\n",
    "new_columns = []\n",
    "for label in df_labels:\n",
    "    if label not in sentence_test_causenet_predictions:\n",
    "        new_columns.append(\n",
    "            pd.Series(\n",
    "                np.nan, index=sentence_test_causenet_predictions.index, name=label\n",
    "            )\n",
    "        )\n",
    "sentence_test_causenet_predictions = pd.concat(\n",
    "    [sentence_test_causenet_predictions, *new_columns], axis=1\n",
    ")\n",
    "\n",
    "prediction_sentence_test_causenet = sentence_test_causenet_predictions.copy()\n",
    "prediction_sentence_test_causenet.cause = prediction_sentence_test_causenet.sentence\n",
    "prediction_sentence_test_causenet.effect = \"\"\n",
    "prediction_sentence_test_causenet = prediction_sentence_test_causenet.drop(\n",
    "    [\"sentence\",], axis=1,\n",
    ")\n",
    "\n",
    "missing_rows = sentence_test_causenet_predictions[df_labels].isna().all(1)\n",
    "if missing_rows.any():\n",
    "    print(f\"parsing {missing_rows.sum()} missing rows\")\n",
    "    pg = tqdm(labels)\n",
    "    for label in pg:\n",
    "        replace_rows = classify_test_set(\n",
    "            prediction_sentence_test_causenet.loc[missing_rows, [\"cause\", \"effect\"]],\n",
    "            label,\n",
    "        )\n",
    "        label = \"-\".join(label)\n",
    "        columns = [f\"medical_score-cause-{label}\", f\"medical_score-effect-{label}\"]\n",
    "        replace_rows = replace_rows.loc[:, columns]\n",
    "        sentence_test_causenet_predictions.loc[\n",
    "            missing_rows, columns\n",
    "        ] = replace_rows.values\n",
    "        \n",
    "        effect_columns = sentence_test_causenet_predictions.columns[\n",
    "            sentence_test_causenet_predictions.columns.str.startswith(\"medical_score-effect\")\n",
    "        ]\n",
    "        sentence_test_causenet_predictions.loc[:, effect_columns] = 0\n",
    "        sentence_test_causenet_predictions.to_pickle(\n",
    "            os.path.join(constants.TEST_CAUSENET_PATH, \"sentence_test_causenet_predictions.pkl\")\n",
    "        )\n",
    "         \n",
    "score_column = sentence_test_causenet_predictions.columns.str.startswith(\n",
    "    \"medical_score-\"\n",
    ")\n",
    "isna = sentence_test_causenet_predictions.isna().any(0)\n",
    "missing_columns = list(sentence_test_causenet_predictions.loc[:, score_column & isna])\n",
    "missing_columns = [\"-\".join(missing_column.split(\"-\")[2:]) for missing_column in missing_columns]\n",
    "missing_columns = list(set(missing_columns))\n",
    "missing_columns = sorted([missing_column.split(\"-\") for missing_column in missing_columns])\n",
    "\n",
    "if missing_columns:\n",
    "    print(\"parsing missing columns\")\n",
    "    for missing_column in missing_columns:\n",
    "        print(\" \".join(missing_column))\n",
    "    pg = tqdm(missing_columns)\n",
    "    for label in pg:\n",
    "        replace_columns = classify_test_set(\n",
    "            prediction_sentence_test_causenet.loc[:, [\"cause\", \"effect\"]], label,\n",
    "        )\n",
    "        label = \"-\".join(label)\n",
    "        columns = [f\"medical_score-cause-{label}\", f\"medical_score-effect-{label}\"]\n",
    "        replace_columns = replace_columns.loc[:, columns]\n",
    "        sentence_test_causenet_predictions.loc[:, columns] = replace_columns.values\n",
    "        assert not sentence_test_causenet_predictions.loc[:, columns].isna().any().any()\n",
    "\n",
    "        effect_columns = sentence_test_causenet_predictions.columns[\n",
    "            sentence_test_causenet_predictions.columns.str.startswith(\"medical_score-effect\")\n",
    "        ]\n",
    "        sentence_test_causenet_predictions.loc[:, effect_columns] = 0\n",
    "        sentence_test_causenet_predictions.to_pickle(\n",
    "            os.path.join(constants.TEST_CAUSENET_PATH, \"sentence_test_causenet_predictions.pkl\")\n",
    "        )\n",
    "        \n",
    "assert not sentence_test_causenet_predictions.loc[:, score_column].isna().any().any()\n",
    "\n",
    "sentence_test_causenet.merge(\n",
    "    sentence_test_causenet_predictions,\n",
    "    on=[\"cause\", \"effect\", \"sentence\", \"dataset\"],\n",
    "    how=\"left\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
