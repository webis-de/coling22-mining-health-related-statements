{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01152e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['medical_score-cause-term_domain_specificity-pubmed-(1, 1)-0', 'medical_score-cause-term_domain_specificity-textbook-(1, 1)-0', 'medical_score-cause-term_domain_specificity-pubmed_central-(1, 1)-0', 'medical_score-cause-term_domain_specificity-encyclopedia-(1, 1)-0', 'medical_score-cause-term_domain_specificity-pubmed-(1, 2)-0', 'medical_score-cause-term_domain_specificity-textbook-(1, 2)-0', 'medical_score-cause-term_domain_specificity-pubmed_central-(1, 2)-0', 'medical_score-cause-term_domain_specificity-encyclopedia-(1, 2)-0', 'medical_score-cause-term_domain_specificity-pubmed-(1, 3)-0', 'medical_score-cause-term_domain_specificity-textbook-(1, 3)-0', 'medical_score-cause-term_domain_specificity-pubmed_central-(1, 3)-0', 'medical_score-cause-term_domain_specificity-encyclopedia-(1, 3)-0', 'medical_score-cause-contrastive_weight-pubmed-(1, 1)-0', 'medical_score-cause-contrastive_weight-textbook-(1, 1)-0', 'medical_score-cause-contrastive_weight-pubmed_central-(1, 1)-0', 'medical_score-cause-contrastive_weight-encyclopedia-(1, 1)-0', 'medical_score-cause-contrastive_weight-pubmed-(1, 2)-0', 'medical_score-cause-contrastive_weight-textbook-(1, 2)-0', 'medical_score-cause-contrastive_weight-pubmed_central-(1, 2)-0', 'medical_score-cause-contrastive_weight-encyclopedia-(1, 2)-0', 'medical_score-cause-contrastive_weight-pubmed-(1, 3)-0', 'medical_score-cause-contrastive_weight-textbook-(1, 3)-0', 'medical_score-cause-contrastive_weight-pubmed_central-(1, 3)-0', 'medical_score-cause-contrastive_weight-encyclopedia-(1, 3)-0', 'medical_score-cause-discriminative_weight-pubmed-(1, 1)-0', 'medical_score-cause-discriminative_weight-textbook-(1, 1)-0', 'medical_score-cause-discriminative_weight-pubmed_central-(1, 1)-0', 'medical_score-cause-discriminative_weight-encyclopedia-(1, 1)-0', 'medical_score-cause-discriminative_weight-pubmed-(1, 2)-0', 'medical_score-cause-discriminative_weight-textbook-(1, 2)-0', 'medical_score-cause-discriminative_weight-pubmed_central-(1, 2)-0', 'medical_score-cause-discriminative_weight-encyclopedia-(1, 2)-0', 'medical_score-cause-discriminative_weight-pubmed-(1, 3)-0', 'medical_score-cause-discriminative_weight-textbook-(1, 3)-0', 'medical_score-cause-discriminative_weight-pubmed_central-(1, 3)-0', 'medical_score-cause-discriminative_weight-encyclopedia-(1, 3)-0', 'medical_score-effect-term_domain_specificity-pubmed-(1, 1)-0', 'medical_score-effect-term_domain_specificity-textbook-(1, 1)-0', 'medical_score-effect-term_domain_specificity-pubmed_central-(1, 1)-0', 'medical_score-effect-term_domain_specificity-encyclopedia-(1, 1)-0', 'medical_score-effect-term_domain_specificity-pubmed-(1, 2)-0', 'medical_score-effect-term_domain_specificity-textbook-(1, 2)-0', 'medical_score-effect-term_domain_specificity-pubmed_central-(1, 2)-0', 'medical_score-effect-term_domain_specificity-encyclopedia-(1, 2)-0', 'medical_score-effect-term_domain_specificity-pubmed-(1, 3)-0', 'medical_score-effect-term_domain_specificity-textbook-(1, 3)-0', 'medical_score-effect-term_domain_specificity-pubmed_central-(1, 3)-0', 'medical_score-effect-term_domain_specificity-encyclopedia-(1, 3)-0', 'medical_score-effect-contrastive_weight-pubmed-(1, 1)-0', 'medical_score-effect-contrastive_weight-textbook-(1, 1)-0', 'medical_score-effect-contrastive_weight-pubmed_central-(1, 1)-0', 'medical_score-effect-contrastive_weight-encyclopedia-(1, 1)-0', 'medical_score-effect-contrastive_weight-pubmed-(1, 2)-0', 'medical_score-effect-contrastive_weight-textbook-(1, 2)-0', 'medical_score-effect-contrastive_weight-pubmed_central-(1, 2)-0', 'medical_score-effect-contrastive_weight-encyclopedia-(1, 2)-0', 'medical_score-effect-contrastive_weight-pubmed-(1, 3)-0', 'medical_score-effect-contrastive_weight-textbook-(1, 3)-0', 'medical_score-effect-contrastive_weight-pubmed_central-(1, 3)-0', 'medical_score-effect-contrastive_weight-encyclopedia-(1, 3)-0', 'medical_score-effect-discriminative_weight-pubmed-(1, 1)-0', 'medical_score-effect-discriminative_weight-textbook-(1, 1)-0', 'medical_score-effect-discriminative_weight-pubmed_central-(1, 1)-0', 'medical_score-effect-discriminative_weight-encyclopedia-(1, 1)-0', 'medical_score-effect-discriminative_weight-pubmed-(1, 2)-0', 'medical_score-effect-discriminative_weight-textbook-(1, 2)-0', 'medical_score-effect-discriminative_weight-pubmed_central-(1, 2)-0', 'medical_score-effect-discriminative_weight-encyclopedia-(1, 2)-0', 'medical_score-effect-discriminative_weight-pubmed-(1, 3)-0', 'medical_score-effect-discriminative_weight-textbook-(1, 3)-0', 'medical_score-effect-discriminative_weight-pubmed_central-(1, 3)-0', 'medical_score-effect-discriminative_weight-encyclopedia-(1, 3)-0']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# remove columns; careful when running, will remove columns from prediction dataset\n",
    "pattern = \"-0$\"\n",
    "\n",
    "# test_causenet_predictions = pd.read_pickle(\n",
    "#     os.path.join(constants.CEPH_PATH, \"test_causenet_predictions.pkl\")\n",
    "# )\n",
    "# print(list(test_causenet_predictions.filter(regex=pattern)))\n",
    "# columns = list(test_causenet_predictions.filter(regex=pattern))\n",
    "# test_causenet_predictions = test_causenet_predictions.drop(columns, axis=1)\n",
    "# test_causenet_predictions.to_pickle(\n",
    "#     os.path.join(constants.CEPH_PATH, \"test_causenet_predictions.pkl\")\n",
    "# )\n",
    "# print(list(test_causenet_predictions.filter(regex=pattern)))\n",
    "\n",
    "# sentence_test_causenet_predictions = pd.read_pickle(\n",
    "#     os.path.join(constants.CEPH_PATH, \"sentence_test_causenet_predictions.pkl\")\n",
    "# )\n",
    "# columns = list(sentence_test_causenet_predictions.filter(regex=pattern))\n",
    "# sentence_test_causenet_predictions = sentence_test_causenet_predictions.drop(columns, axis=1)\n",
    "# sentence_test_causenet_predictions.to_pickle(\n",
    "#     os.path.join(constants.CEPH_PATH, \"sentence_test_causenet_predictions.pkl\")\n",
    "# )\n",
    "# print(list(sentence_test_causenet_predictions.filter(regex=pattern)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5297ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_path = \"..\"\n",
    "sys.path.append(os.path.abspath(parent_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8081bbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /mnt/ceph/storage/data-\n",
      "[nltk_data]     tmp/current//fschlatt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2022-08-30 12:16:54.236695: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-08-30 12:17:36.806356: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-08-30 12:17:36.814721: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-08-30 12:17:36.814749: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gammaweb06\n",
      "2022-08-30 12:17:36.814755: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gammaweb06\n",
      "2022-08-30 12:17:36.814843: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\n",
      "2022-08-30 12:17:36.814872: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.106.0\n",
      "Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2\n",
      "For maximum performance, you can install NMSLIB from sources \n",
      "pip install --no-binary :all: nmslib\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import pickle5\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import health_causenet\n",
    "from health_causenet import constants\n",
    "from health_causenet.causenet import (\n",
    "    CauseNet,\n",
    "    contrastive_weight,\n",
    "    term_domain_specificity,\n",
    "    discriminative_weight,\n",
    ")\n",
    "import quickumls\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "from health_bert import health_bert\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffb53f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_5_to_4(path):\n",
    "    with open(path, \"rb\") as fh:\n",
    "        data = pickle5.load(fh)\n",
    "    data.to_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6fe12b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_5_to_4(os.path.join(constants.CEPH_PATH, \"test_causenet_predictions.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test causenet\n",
    "\n",
    "full_causenet = pd.DataFrame()\n",
    "paths = sorted(pathlib.Path(constants.CAUSENET_PARQUET_PATH).glob(\"causenet_*.parquet\"))\n",
    "for path in tqdm(paths):\n",
    "    from_file = pd.read_parquet(\n",
    "        path, columns=[\"cause\", \"effect\", \"support\", \"reference\", \"sentence\"]\n",
    "    )\n",
    "    full_causenet = full_causenet.append(from_file)\n",
    "full_causenet = full_causenet.reset_index(drop=True)\n",
    "print(\"parsing domain...\")\n",
    "print(\"computing counts...\")\n",
    "causenet = full_causenet.groupby([\"cause\", \"effect\", \"support\"]).size()\n",
    "causenet.name = \"count\"\n",
    "causenet = causenet.reset_index()\n",
    "\n",
    "print(\"sorting by support...\")\n",
    "test_causenet_support = causenet.sort_values(\n",
    "    [\"support\", \"cause\", \"effect\"], ascending=False\n",
    ").reset_index(drop=True)\n",
    "test_causenet_support = test_causenet_support.iloc[:1000].copy()\n",
    "test_causenet_support[\"dataset\"] = \"support\"\n",
    "test_causenet_random_high = (\n",
    "    causenet.loc[causenet.support >= 2].sample(1000, random_state=42).copy()\n",
    ")\n",
    "test_causenet_random_high[\"dataset\"] = \"random_support\"\n",
    "test_causenet_random_low = (\n",
    "    causenet.loc[causenet.support == 1].sample(1000, random_state=42).copy()\n",
    ")\n",
    "test_causenet_random_low[\"dataset\"] = \"random_full\"\n",
    "test_causenet = pd.concat(\n",
    "    [test_causenet_support, test_causenet_random_high, test_causenet_random_low]\n",
    ")\n",
    "test_causenet = test_causenet.reset_index(drop=True)\n",
    "\n",
    "print(\"adding wikidata...\")\n",
    "test_wikidata = (\n",
    "    pd.read_csv(os.path.join(constants.WIKIDATA_PATH, \"wikidata-test.csv\"), index_col=0)\n",
    "    .drop_duplicates()\n",
    "    .dropna(subset=[\"cause\", \"effect\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "test_wikidata[\"dataset\"] = \"wikidata\"\n",
    "test_causenet = test_causenet.append(\n",
    "    test_wikidata.loc[\n",
    "        :, [\"cause\", \"effect\", \"dataset\", \"cause_origin\", \"effect_origin\"]\n",
    "    ]\n",
    ").reset_index(drop=True)\n",
    "test_causenet = test_causenet.drop_duplicates([\"cause\", \"effect\", \"dataset\"])\n",
    "\n",
    "print(\"adding practitioner...\")\n",
    "test_practitioner = pd.read_csv(\n",
    "    os.path.join(constants.CEPH_PATH, \"practitioner.csv\"), index_col=0\n",
    ")\n",
    "test_practitioner = test_practitioner.dropna(subset=[\"medical\"])\n",
    "test_practitioner[\"evaluation\"] = test_practitioner.medical.str.contains(\"y\")\n",
    "test_practitioner[\"dataset\"] = \"practitioner_full\"\n",
    "test_causenet = test_causenet.append(\n",
    "    test_practitioner.loc[:, [\"cause\", \"effect\", \"dataset\"]]\n",
    ").reset_index(drop=True)\n",
    "test_practitioner.loc[\n",
    "    ~test_practitioner.comment.isna(), \"dataset\"\n",
    "] = \"practitioner_unsure\"\n",
    "test_practitioner.loc[test_practitioner.comment.isna(), \"dataset\"] = \"practitioner_sure\"\n",
    "test_causenet = test_causenet.append(\n",
    "    test_practitioner.loc[:, [\"cause\", \"effect\", \"dataset\"]]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Load and label evaluations\n",
    "\n",
    "ignore_origins = [\n",
    "    #     \"wd:Q39833\",  # microorganism\n",
    "    #     \"wd:Q178694\",  # heredity\n",
    "    #     \"wd:Q289472\",  # biogenic substance\n",
    "    #     \"wd:Q796194\",  # medical procedure\n",
    "    #     \"wd:Q2826767\",  # disease causative agent\n",
    "    #     \"wd:Q2996394\",  # biological process\n",
    "    #     \"wd:Q5850078\",  # etiology\n",
    "    #     \"wd:Q7189713\",  # physiological condition\n",
    "    #     \"wd:Q15788410\",  # state of consciousness\n",
    "    #     \"wd:Q86746756\",  # medicinal product\n",
    "    #     \"wd:Q87075524\",  # health risk\n",
    "]\n",
    "\n",
    "with open(constants.MANUAL_EVALUATION_PATH, \"r\") as file:\n",
    "    manual_eval_dict = json.load(file)\n",
    "data = []\n",
    "for key, value in manual_eval_dict.items():\n",
    "    cause, effect = key.split(\"->\")\n",
    "    data.append({\"cause\": cause, \"effect\": effect, \"evaluation\": value})\n",
    "test_causenet_manual = test_causenet.loc[\n",
    "    test_causenet.dataset.isin([\"support\", \"random_support\", \"random_full\", \"count\"])\n",
    "]\n",
    "manual_eval = pd.DataFrame(data)\n",
    "evaluation = manual_eval.set_index([\"cause\", \"effect\"]).reindex(\n",
    "    test_causenet_manual.set_index([\"cause\", \"effect\"]).index\n",
    ")\n",
    "\n",
    "end = False\n",
    "to_label = test_causenet_manual.loc[evaluation.isna().values].values\n",
    "for idx, row in enumerate(to_label):\n",
    "    cause = row[0]\n",
    "    effect = row[1]\n",
    "    while True:\n",
    "        clear_output(wait=True)\n",
    "        inp = input(\n",
    "            f\"{idx+1}/{len(to_label)} ({len(to_label) - idx}) [{cause}] -> [{effect}]\"\n",
    "        )\n",
    "        if inp == \"c\":\n",
    "            end = True\n",
    "            break\n",
    "        try:\n",
    "            val = int(inp)\n",
    "            if val in (0, 1):\n",
    "                key = f\"{cause}->{effect}\"\n",
    "                manual_eval_dict[key] = val\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"invalid input: {inp}, needs to be either 1 or 0\")\n",
    "    if end:\n",
    "        break\n",
    "\n",
    "with open(constants.MANUAL_EVALUATION_PATH, \"w\") as file:\n",
    "    json.dump(manual_eval_dict, file, indent=4)\n",
    "\n",
    "data = []\n",
    "for key, value in manual_eval_dict.items():\n",
    "    cause, effect = key.split(\"->\")\n",
    "    data.append({\"cause\": cause, \"effect\": effect, \"evaluation\": value})\n",
    "manual_eval = pd.DataFrame(data)\n",
    "evaluation = manual_eval.set_index([\"cause\", \"effect\"]).reindex(\n",
    "    test_causenet_manual.set_index([\"cause\", \"effect\"]).index\n",
    ")\n",
    "\n",
    "print(f\"eval missing for {evaluation.isna().sum().values[0]} relations\")\n",
    "\n",
    "evaluation = evaluation.evaluation\n",
    "\n",
    "evaluation = evaluation.append(test_wikidata.set_index([\"cause\", \"effect\"]).evaluation)\n",
    "evaluation = evaluation.append(\n",
    "    test_practitioner.set_index([\"cause\", \"effect\"]).evaluation\n",
    ")\n",
    "evaluation = evaluation.append(\n",
    "    test_practitioner.set_index([\"cause\", \"effect\"]).evaluation\n",
    ")\n",
    "\n",
    "test_causenet[\"evaluation\"] = evaluation.values\n",
    "\n",
    "print(\"parsing sentence test causenet\")\n",
    "sentences = (\n",
    "    full_causenet.set_index([\"cause\", \"effect\", \"support\"])\n",
    "    .loc[\n",
    "        test_causenet.loc[\n",
    "            ~test_causenet.support.isna(), [\"cause\", \"effect\", \"support\"]\n",
    "        ].values.tolist(),\n",
    "        \"sentence\",\n",
    "    ]\n",
    "    .reset_index()\n",
    "    .drop_duplicates([\"cause\", \"effect\", \"sentence\"])\n",
    ")\n",
    "sentence_test_causenet = test_causenet.merge(\n",
    "    sentences, on=[\"cause\", \"effect\", \"support\"]\n",
    ")\n",
    "sentence_test_causenet = sentence_test_causenet.drop_duplicates([\"dataset\", \"sentence\"])\n",
    "sentence_test_causenet = sentence_test_causenet.drop_duplicates(\n",
    "    [\"cause\", \"effect\", \"dataset\"]\n",
    ")\n",
    "sentence_test_causenet_evaluation = pd.read_csv(\n",
    "    os.path.join(constants.CEPH_PATH, \"sentence_test_causenet_evaluations.csv\"),\n",
    "    index_col=0,\n",
    ").rename({\"label\": \"manual_evaluation\"}, axis=1)\n",
    "sentence_test_causenet = sentence_test_causenet.merge(\n",
    "    sentence_test_causenet_evaluation, on=[\"cause\", \"effect\", \"sentence\"], how=\"left\"\n",
    ")\n",
    "\n",
    "del causenet\n",
    "del full_causenet\n",
    "\n",
    "# print(\"creating ctakes and metamap data\")\n",
    "# relations = pd.Series(\n",
    "#     pd.unique(test_causenet.loc[:, [\"cause\", \"effect\"]].values.ravel())\n",
    "# )\n",
    "# relations = relations.loc[~relations.isin([\"\", \" \"])]\n",
    "# sentence_relations = pd.Series(\n",
    "#     pd.unique(sentence_test_causenet.loc[:, [\"sentence\"]].values.ravel())\n",
    "# )\n",
    "# relations = pd.concat([relations, sentence_relations]).reset_index(drop=True)\n",
    "# ctakes_path = pathlib.Path(constants.CEPH_PATH).joinpath(\"ctakes\")\n",
    "# ctakes_path.mkdir(exist_ok=True)\n",
    "# metamap_path = pathlib.Path(constants.CEPH_PATH).joinpath(\"metamap\")\n",
    "# metamap_path.mkdir(exist_ok=True)\n",
    "# for idx, relation in enumerate(relations.values):\n",
    "#     with ctakes_path.joinpath(f\"relation_{idx + 1}.txt\").open(\"w\") as file:\n",
    "#         file.write(relation)\n",
    "# with metamap_path.joinpath(\"relations.txt\").open(\"w\") as file:\n",
    "#     file.write(\"\\n\".join(f\"{idx + 1}|{relation}\" for idx, relation in enumerate(relations.values)))\n",
    "\n",
    "test_causenet.to_pickle(os.path.join(constants.CEPH_PATH, \"test_causenet.pkl\"))\n",
    "sentence_test_causenet.to_pickle(\n",
    "    os.path.join(constants.CEPH_PATH, \"sentence_test_causenet.pkl\")\n",
    ")\n",
    "test_causenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c072a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cause</th>\n",
       "      <th>effect</th>\n",
       "      <th>support</th>\n",
       "      <th>count</th>\n",
       "      <th>dataset</th>\n",
       "      <th>cause_origin</th>\n",
       "      <th>effect_origin</th>\n",
       "      <th>evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accident</td>\n",
       "      <td>death</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1458.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pneumonia</td>\n",
       "      <td>death</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disease</td>\n",
       "      <td>death</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>illness</td>\n",
       "      <td>death</td>\n",
       "      <td>36.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heart attack</td>\n",
       "      <td>death</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14311</th>\n",
       "      <td>initial oxidation at carbon 5</td>\n",
       "      <td>thymine glycol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>practitioner_unsure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14312</th>\n",
       "      <td>thyroid hormone deficits</td>\n",
       "      <td>language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>practitioner_sure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14313</th>\n",
       "      <td>neuropeptide profile</td>\n",
       "      <td>decreased food intake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>practitioner_sure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14314</th>\n",
       "      <td>disorder</td>\n",
       "      <td>white heads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>practitioner_sure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14315</th>\n",
       "      <td>deletion of pten in murine models</td>\n",
       "      <td>expansion of the prostate stem/progenitor cell...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>practitioner_unsure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14316 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   cause  \\\n",
       "0                               accident   \n",
       "1                              pneumonia   \n",
       "2                                disease   \n",
       "3                                illness   \n",
       "4                           heart attack   \n",
       "...                                  ...   \n",
       "14311      initial oxidation at carbon 5   \n",
       "14312           thyroid hormone deficits   \n",
       "14313               neuropeptide profile   \n",
       "14314                           disorder   \n",
       "14315  deletion of pten in murine models   \n",
       "\n",
       "                                                  effect  support   count  \\\n",
       "0                                                  death     38.0  1458.0   \n",
       "1                                                  death     37.0  1185.0   \n",
       "2                                                  death     37.0  1344.0   \n",
       "3                                                  death     36.0   571.0   \n",
       "4                                                  death     36.0  1005.0   \n",
       "...                                                  ...      ...     ...   \n",
       "14311                                     thymine glycol      NaN     NaN   \n",
       "14312                                           language      NaN     NaN   \n",
       "14313                              decreased food intake      NaN     NaN   \n",
       "14314                                        white heads      NaN     NaN   \n",
       "14315  expansion of the prostate stem/progenitor cell...      NaN     NaN   \n",
       "\n",
       "                   dataset cause_origin effect_origin  evaluation  \n",
       "0                  support          NaN           NaN           0  \n",
       "1                  support          NaN           NaN           1  \n",
       "2                  support          NaN           NaN           1  \n",
       "3                  support          NaN           NaN           1  \n",
       "4                  support          NaN           NaN           1  \n",
       "...                    ...          ...           ...         ...  \n",
       "14311  practitioner_unsure          NaN           NaN           1  \n",
       "14312    practitioner_sure          NaN           NaN           0  \n",
       "14313    practitioner_sure          NaN           NaN           0  \n",
       "14314    practitioner_sure          NaN           NaN           0  \n",
       "14315  practitioner_unsure          NaN           NaN           1  \n",
       "\n",
       "[14316 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test_causenet\n",
    "test_causenet = pd.read_pickle(os.path.join(constants.BASE_PATH, \"test_causenet.pkl\"))\n",
    "sentence_test_causenet = pd.read_pickle(\n",
    "    os.path.join(constants.BASE_PATH, \"sentence_test_causenet.pkl\")\n",
    ")\n",
    "test_causenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628bd3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pubmed: computing term domain specificity...\n",
      "pubmed: computing contrastive weight...\n",
      "pubmed: computing_discriminative weight...\n",
      "pubmed_central: computing term domain specificity...\n",
      "pubmed_central: computing contrastive weight...\n",
      "pubmed_central: computing_discriminative weight...\n",
      "textbook: computing term domain specificity...\n",
      "textbook: computing contrastive weight...\n",
      "textbook: computing_discriminative weight...\n",
      "encyclopedia: computing term domain specificity...\n",
      "encyclopedia: computing contrastive weight...\n",
      "encyclopedia: computing_discriminative weight...\n"
     ]
    }
   ],
   "source": [
    "# load cf and compute termhood scores\n",
    "cf = pd.read_parquet(os.path.join(constants.CF_PATH, \"cf.parquet\"))\n",
    "cf.loc[cf.num_terms == 1].sum().astype(str)\n",
    "medical_termhood = {}\n",
    "for corpus in list(cf.filter(regex=r\".*_frequency_(?!open_domain)\")):\n",
    "    _cf = cf.loc[:, [\"corpus_frequency_open_domain\", corpus, \"num_terms\"]]\n",
    "    corpus = corpus.replace(\"corpus_frequency_\", \"\")\n",
    "    medical_termhood[corpus] = {}\n",
    "    print(f\"{corpus}: computing term domain specificity...\")\n",
    "    medical_termhood[corpus][\"term_domain_specificity\"] = term_domain_specificity(\n",
    "        _cf, np.e\n",
    "    )\n",
    "    print(f\"{corpus}: computing contrastive weight...\")\n",
    "    medical_termhood[corpus][\"contrastive_weight\"] = contrastive_weight(_cf, np.e, 1)\n",
    "    print(f\"{corpus}: computing_discriminative weight...\")\n",
    "    medical_termhood[corpus][\"discriminative_weight\"] = (\n",
    "        medical_termhood[corpus][\"contrastive_weight\"]\n",
    "        * medical_termhood[corpus][\"term_domain_specificity\"]\n",
    "    )\n",
    "del cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8304c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67704a42eee7474cac021fa18eabe302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/tmp/ipykernel_506937/1052718644.py:215: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  ratios = times_df[\"time\"].values[:, None] / times_df[\"time\"].values[None, :]\n",
      "/tmp/ipykernel_506937/1052718644.py:215: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ratios = times_df[\"time\"].values[:, None] / times_df[\"time\"].values[None, :]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>time_per_iter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th>method</th>\n",
       "      <th>n-gram</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">sentence</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">termhood</th>\n",
       "      <th>1</th>\n",
       "      <td>1.020202</td>\n",
       "      <td>1.020202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.972861</td>\n",
       "      <td>1.972861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.820309</td>\n",
       "      <td>2.820309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert</th>\n",
       "      <th></th>\n",
       "      <td>47.768734</td>\n",
       "      <td>47.768734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">quickumls</th>\n",
       "      <th>full</th>\n",
       "      <td>11.414734</td>\n",
       "      <td>11.414734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>8.980932</td>\n",
       "      <td>8.980932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">scispacy</th>\n",
       "      <th>full</th>\n",
       "      <td>16.795120</td>\n",
       "      <td>16.795120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>15.958199</td>\n",
       "      <td>15.958199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">metamap</th>\n",
       "      <th>full</th>\n",
       "      <td>160.638000</td>\n",
       "      <td>160.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>120.277000</td>\n",
       "      <td>120.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ctakes</th>\n",
       "      <th>full</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>212.711000</td>\n",
       "      <td>212.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">phrase</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">termhood</th>\n",
       "      <th>1</th>\n",
       "      <td>0.564155</td>\n",
       "      <td>0.564155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.928234</td>\n",
       "      <td>0.928234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.265505</td>\n",
       "      <td>1.265505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert</th>\n",
       "      <th></th>\n",
       "      <td>60.190908</td>\n",
       "      <td>60.190908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">quickumls</th>\n",
       "      <th>full</th>\n",
       "      <td>8.210610</td>\n",
       "      <td>8.210610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>7.229493</td>\n",
       "      <td>7.229493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">scispacy</th>\n",
       "      <th>full</th>\n",
       "      <td>17.543986</td>\n",
       "      <td>17.543986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>16.383440</td>\n",
       "      <td>16.383440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">metamap</th>\n",
       "      <th>full</th>\n",
       "      <td>70.639000</td>\n",
       "      <td>70.639000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>49.636000</td>\n",
       "      <td>49.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ctakes</th>\n",
       "      <th>full</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>119.684000</td>\n",
       "      <td>119.684000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 time  time_per_iter\n",
       "data     method    n-gram                           \n",
       "sentence termhood  1         1.020202       1.020202\n",
       "                   2         1.972861       1.972861\n",
       "                   3         2.820309       2.820309\n",
       "         bert               47.768734      47.768734\n",
       "         quickumls full     11.414734      11.414734\n",
       "                   rx_sno    8.980932       8.980932\n",
       "         scispacy  full     16.795120      16.795120\n",
       "                   rx_sno   15.958199      15.958199\n",
       "         metamap   full    160.638000     160.638000\n",
       "                   rx_sno  120.277000     120.277000\n",
       "         ctakes    full      0.000000       0.000000\n",
       "                   rx_sno  212.711000     212.711000\n",
       "phrase   termhood  1         0.564155       0.564155\n",
       "                   2         0.928234       0.928234\n",
       "                   3         1.265505       1.265505\n",
       "         bert               60.190908      60.190908\n",
       "         quickumls full      8.210610       8.210610\n",
       "                   rx_sno    7.229493       7.229493\n",
       "         scispacy  full     17.543986      17.543986\n",
       "                   rx_sno   16.383440      16.383440\n",
       "         metamap   full     70.639000      70.639000\n",
       "                   rx_sno   49.636000      49.636000\n",
       "         ctakes    full      0.000000       0.000000\n",
       "                   rx_sno  119.684000     119.684000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_506937/1052718644.py:220: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  display(ratio_df.loc[\"sentence\", \"sentence\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th colspan=\"3\" halign=\"left\">termhood</th>\n",
       "      <th>bert</th>\n",
       "      <th colspan=\"2\" halign=\"left\">quickumls</th>\n",
       "      <th colspan=\"2\" halign=\"left\">scispacy</th>\n",
       "      <th colspan=\"2\" halign=\"left\">metamap</th>\n",
       "      <th colspan=\"2\" halign=\"left\">ctakes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>n-gram</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th></th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th>n-gram</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">termhood</th>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.517118</td>\n",
       "      <td>0.361734</td>\n",
       "      <td>0.021357</td>\n",
       "      <td>0.089376</td>\n",
       "      <td>0.113596</td>\n",
       "      <td>0.060744</td>\n",
       "      <td>0.063930</td>\n",
       "      <td>0.006351</td>\n",
       "      <td>0.008482</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.933794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.699519</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>0.172835</td>\n",
       "      <td>0.219672</td>\n",
       "      <td>0.117466</td>\n",
       "      <td>0.123627</td>\n",
       "      <td>0.012281</td>\n",
       "      <td>0.016403</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.009275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.764461</td>\n",
       "      <td>1.429553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059041</td>\n",
       "      <td>0.247076</td>\n",
       "      <td>0.314033</td>\n",
       "      <td>0.167924</td>\n",
       "      <td>0.176731</td>\n",
       "      <td>0.017557</td>\n",
       "      <td>0.023448</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.013259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert</th>\n",
       "      <th></th>\n",
       "      <td>46.822815</td>\n",
       "      <td>24.212928</td>\n",
       "      <td>16.937413</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.184831</td>\n",
       "      <td>5.318906</td>\n",
       "      <td>2.844203</td>\n",
       "      <td>2.993366</td>\n",
       "      <td>0.297369</td>\n",
       "      <td>0.397156</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.224571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">quickumls</th>\n",
       "      <th>full</th>\n",
       "      <td>11.188698</td>\n",
       "      <td>5.785879</td>\n",
       "      <td>4.047335</td>\n",
       "      <td>0.238958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.270997</td>\n",
       "      <td>0.679646</td>\n",
       "      <td>0.715290</td>\n",
       "      <td>0.071059</td>\n",
       "      <td>0.094904</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.053663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>8.803091</td>\n",
       "      <td>4.552238</td>\n",
       "      <td>3.184379</td>\n",
       "      <td>0.188009</td>\n",
       "      <td>0.786784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.534735</td>\n",
       "      <td>0.562779</td>\n",
       "      <td>0.055908</td>\n",
       "      <td>0.074669</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.042221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">scispacy</th>\n",
       "      <th>full</th>\n",
       "      <td>16.462542</td>\n",
       "      <td>8.513079</td>\n",
       "      <td>5.955064</td>\n",
       "      <td>0.351592</td>\n",
       "      <td>1.471355</td>\n",
       "      <td>1.870086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.052445</td>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.139637</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.078957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>15.642194</td>\n",
       "      <td>8.088863</td>\n",
       "      <td>5.658316</td>\n",
       "      <td>0.334072</td>\n",
       "      <td>1.398035</td>\n",
       "      <td>1.776898</td>\n",
       "      <td>0.950169</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099343</td>\n",
       "      <td>0.132679</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.075023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">metamap</th>\n",
       "      <th>full</th>\n",
       "      <td>157.457035</td>\n",
       "      <td>81.423893</td>\n",
       "      <td>56.957592</td>\n",
       "      <td>3.362827</td>\n",
       "      <td>14.072865</td>\n",
       "      <td>17.886562</td>\n",
       "      <td>9.564564</td>\n",
       "      <td>10.066173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.335567</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.755194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>117.895267</td>\n",
       "      <td>60.965784</td>\n",
       "      <td>42.646748</td>\n",
       "      <td>2.517902</td>\n",
       "      <td>10.536996</td>\n",
       "      <td>13.392485</td>\n",
       "      <td>7.161426</td>\n",
       "      <td>7.537003</td>\n",
       "      <td>0.748746</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.565448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ctakes</th>\n",
       "      <th>full</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>208.498882</td>\n",
       "      <td>107.818559</td>\n",
       "      <td>75.421173</td>\n",
       "      <td>4.452934</td>\n",
       "      <td>18.634776</td>\n",
       "      <td>23.684735</td>\n",
       "      <td>12.665048</td>\n",
       "      <td>13.329261</td>\n",
       "      <td>1.324164</td>\n",
       "      <td>1.768509</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "method              termhood                             bert  quickumls  \\\n",
       "n-gram                     1           2          3                 full   \n",
       "method    n-gram                                                           \n",
       "termhood  1         1.000000    0.517118   0.361734  0.021357   0.089376   \n",
       "          2         1.933794    1.000000   0.699519  0.041300   0.172835   \n",
       "          3         2.764461    1.429553   1.000000  0.059041   0.247076   \n",
       "bert               46.822815   24.212928  16.937413  1.000000   4.184831   \n",
       "quickumls full     11.188698    5.785879   4.047335  0.238958   1.000000   \n",
       "          rx_sno    8.803091    4.552238   3.184379  0.188009   0.786784   \n",
       "scispacy  full     16.462542    8.513079   5.955064  0.351592   1.471355   \n",
       "          rx_sno   15.642194    8.088863   5.658316  0.334072   1.398035   \n",
       "metamap   full    157.457035   81.423893  56.957592  3.362827  14.072865   \n",
       "          rx_sno  117.895267   60.965784  42.646748  2.517902  10.536996   \n",
       "ctakes    full      0.000000    0.000000   0.000000  0.000000   0.000000   \n",
       "          rx_sno  208.498882  107.818559  75.421173  4.452934  18.634776   \n",
       "\n",
       "method                        scispacy              metamap           ctakes  \\\n",
       "n-gram               rx_sno       full     rx_sno      full    rx_sno   full   \n",
       "method    n-gram                                                               \n",
       "termhood  1        0.113596   0.060744   0.063930  0.006351  0.008482    inf   \n",
       "          2        0.219672   0.117466   0.123627  0.012281  0.016403    inf   \n",
       "          3        0.314033   0.167924   0.176731  0.017557  0.023448    inf   \n",
       "bert               5.318906   2.844203   2.993366  0.297369  0.397156    inf   \n",
       "quickumls full     1.270997   0.679646   0.715290  0.071059  0.094904    inf   \n",
       "          rx_sno   1.000000   0.534735   0.562779  0.055908  0.074669    inf   \n",
       "scispacy  full     1.870086   1.000000   1.052445  0.104553  0.139637    inf   \n",
       "          rx_sno   1.776898   0.950169   1.000000  0.099343  0.132679    inf   \n",
       "metamap   full    17.886562   9.564564  10.066173  1.000000  1.335567    inf   \n",
       "          rx_sno  13.392485   7.161426   7.537003  0.748746  1.000000    inf   \n",
       "ctakes    full     0.000000   0.000000   0.000000  0.000000  0.000000    NaN   \n",
       "          rx_sno  23.684735  12.665048  13.329261  1.324164  1.768509    inf   \n",
       "\n",
       "method                      \n",
       "n-gram              rx_sno  \n",
       "method    n-gram            \n",
       "termhood  1       0.004796  \n",
       "          2       0.009275  \n",
       "          3       0.013259  \n",
       "bert              0.224571  \n",
       "quickumls full    0.053663  \n",
       "          rx_sno  0.042221  \n",
       "scispacy  full    0.078957  \n",
       "          rx_sno  0.075023  \n",
       "metamap   full    0.755194  \n",
       "          rx_sno  0.565448  \n",
       "ctakes    full    0.000000  \n",
       "          rx_sno  1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_506937/1052718644.py:221: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  display(ratio_df.loc[\"phrase\", \"phrase\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th colspan=\"3\" halign=\"left\">termhood</th>\n",
       "      <th>bert</th>\n",
       "      <th colspan=\"2\" halign=\"left\">quickumls</th>\n",
       "      <th colspan=\"2\" halign=\"left\">scispacy</th>\n",
       "      <th colspan=\"2\" halign=\"left\">metamap</th>\n",
       "      <th colspan=\"2\" halign=\"left\">ctakes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>n-gram</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th></th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th>n-gram</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">termhood</th>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.607773</td>\n",
       "      <td>0.445795</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.068711</td>\n",
       "      <td>0.078035</td>\n",
       "      <td>0.032157</td>\n",
       "      <td>0.034434</td>\n",
       "      <td>0.007986</td>\n",
       "      <td>0.011366</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.004714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.645352</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733489</td>\n",
       "      <td>0.015421</td>\n",
       "      <td>0.113053</td>\n",
       "      <td>0.128395</td>\n",
       "      <td>0.052909</td>\n",
       "      <td>0.056657</td>\n",
       "      <td>0.013141</td>\n",
       "      <td>0.018701</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.007756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.243186</td>\n",
       "      <td>1.363347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.154130</td>\n",
       "      <td>0.175048</td>\n",
       "      <td>0.072133</td>\n",
       "      <td>0.077243</td>\n",
       "      <td>0.017915</td>\n",
       "      <td>0.025496</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.010574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert</th>\n",
       "      <th></th>\n",
       "      <td>106.692117</td>\n",
       "      <td>64.844540</td>\n",
       "      <td>47.562765</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.330870</td>\n",
       "      <td>8.325744</td>\n",
       "      <td>3.430857</td>\n",
       "      <td>3.673887</td>\n",
       "      <td>0.852092</td>\n",
       "      <td>1.212646</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.502915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">quickumls</th>\n",
       "      <th>full</th>\n",
       "      <td>14.553815</td>\n",
       "      <td>8.845409</td>\n",
       "      <td>6.488011</td>\n",
       "      <td>0.136409</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.135710</td>\n",
       "      <td>0.468001</td>\n",
       "      <td>0.501153</td>\n",
       "      <td>0.116233</td>\n",
       "      <td>0.165416</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.068602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>12.814725</td>\n",
       "      <td>7.788438</td>\n",
       "      <td>5.712735</td>\n",
       "      <td>0.120109</td>\n",
       "      <td>0.880506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.412078</td>\n",
       "      <td>0.441268</td>\n",
       "      <td>0.102344</td>\n",
       "      <td>0.145650</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.060405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">scispacy</th>\n",
       "      <th>full</th>\n",
       "      <td>31.097803</td>\n",
       "      <td>18.900391</td>\n",
       "      <td>13.863231</td>\n",
       "      <td>0.291472</td>\n",
       "      <td>2.136746</td>\n",
       "      <td>2.426724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.070837</td>\n",
       "      <td>0.248361</td>\n",
       "      <td>0.353453</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.146586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>29.040663</td>\n",
       "      <td>17.650118</td>\n",
       "      <td>12.946169</td>\n",
       "      <td>0.272191</td>\n",
       "      <td>1.995399</td>\n",
       "      <td>2.266195</td>\n",
       "      <td>0.933849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.231932</td>\n",
       "      <td>0.330072</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.136889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">metamap</th>\n",
       "      <th>full</th>\n",
       "      <td>125.212008</td>\n",
       "      <td>76.100422</td>\n",
       "      <td>55.818831</td>\n",
       "      <td>1.173583</td>\n",
       "      <td>8.603381</td>\n",
       "      <td>9.770948</td>\n",
       "      <td>4.026394</td>\n",
       "      <td>4.311610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.423140</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.590213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>87.982888</td>\n",
       "      <td>53.473585</td>\n",
       "      <td>39.222293</td>\n",
       "      <td>0.824643</td>\n",
       "      <td>6.045349</td>\n",
       "      <td>6.865765</td>\n",
       "      <td>2.829232</td>\n",
       "      <td>3.029645</td>\n",
       "      <td>0.702671</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.414725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ctakes</th>\n",
       "      <th>full</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>212.147312</td>\n",
       "      <td>128.937314</td>\n",
       "      <td>94.574117</td>\n",
       "      <td>1.988407</td>\n",
       "      <td>14.576750</td>\n",
       "      <td>16.554964</td>\n",
       "      <td>6.821939</td>\n",
       "      <td>7.305182</td>\n",
       "      <td>1.694305</td>\n",
       "      <td>2.411234</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "method              termhood                             bert  quickumls  \\\n",
       "n-gram                     1           2          3                 full   \n",
       "method    n-gram                                                           \n",
       "termhood  1         1.000000    0.607773   0.445795  0.009373   0.068711   \n",
       "          2         1.645352    1.000000   0.733489  0.015421   0.113053   \n",
       "          3         2.243186    1.363347   1.000000  0.021025   0.154130   \n",
       "bert              106.692117   64.844540  47.562765  1.000000   7.330870   \n",
       "quickumls full     14.553815    8.845409   6.488011  0.136409   1.000000   \n",
       "          rx_sno   12.814725    7.788438   5.712735  0.120109   0.880506   \n",
       "scispacy  full     31.097803   18.900391  13.863231  0.291472   2.136746   \n",
       "          rx_sno   29.040663   17.650118  12.946169  0.272191   1.995399   \n",
       "metamap   full    125.212008   76.100422  55.818831  1.173583   8.603381   \n",
       "          rx_sno   87.982888   53.473585  39.222293  0.824643   6.045349   \n",
       "ctakes    full      0.000000    0.000000   0.000000  0.000000   0.000000   \n",
       "          rx_sno  212.147312  128.937314  94.574117  1.988407  14.576750   \n",
       "\n",
       "method                       scispacy             metamap           ctakes  \\\n",
       "n-gram               rx_sno      full    rx_sno      full    rx_sno   full   \n",
       "method    n-gram                                                             \n",
       "termhood  1        0.078035  0.032157  0.034434  0.007986  0.011366    inf   \n",
       "          2        0.128395  0.052909  0.056657  0.013141  0.018701    inf   \n",
       "          3        0.175048  0.072133  0.077243  0.017915  0.025496    inf   \n",
       "bert               8.325744  3.430857  3.673887  0.852092  1.212646    inf   \n",
       "quickumls full     1.135710  0.468001  0.501153  0.116233  0.165416    inf   \n",
       "          rx_sno   1.000000  0.412078  0.441268  0.102344  0.145650    inf   \n",
       "scispacy  full     2.426724  1.000000  1.070837  0.248361  0.353453    inf   \n",
       "          rx_sno   2.266195  0.933849  1.000000  0.231932  0.330072    inf   \n",
       "metamap   full     9.770948  4.026394  4.311610  1.000000  1.423140    inf   \n",
       "          rx_sno   6.865765  2.829232  3.029645  0.702671  1.000000    inf   \n",
       "ctakes    full     0.000000  0.000000  0.000000  0.000000  0.000000    NaN   \n",
       "          rx_sno  16.554964  6.821939  7.305182  1.694305  2.411234    inf   \n",
       "\n",
       "method                      \n",
       "n-gram              rx_sno  \n",
       "method    n-gram            \n",
       "termhood  1       0.004714  \n",
       "          2       0.007756  \n",
       "          3       0.010574  \n",
       "bert              0.502915  \n",
       "quickumls full    0.068602  \n",
       "          rx_sno  0.060405  \n",
       "scispacy  full    0.146586  \n",
       "          rx_sno  0.136889  \n",
       "metamap   full    0.590213  \n",
       "          rx_sno  0.414725  \n",
       "ctakes    full    0.000000  \n",
       "          rx_sno  1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# speed test\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "times = {\n",
    "    \"sentence\": {\n",
    "        \"termhood\": {1: [], 2: [], 3: []},\n",
    "        \"bert\": {\" \": []}, \n",
    "        \"quickumls\": {\"full\": [], \"rx_sno\": []}, \n",
    "        \"scispacy\": {\"full\": [], \"rx_sno\": []}, \n",
    "        \"metamap\": {\"full\": [160.638], \"rx_sno\": [120.277]},\n",
    "        \"ctakes\": {\"full\": [0], \"rx_sno\": [212.711]},\n",
    "    }, \n",
    "    \"phrase\": {\n",
    "        \"termhood\": {1: [], 2: [], 3: []},\n",
    "        \"bert\": {\" \": []}, \n",
    "        \"quickumls\": {\"full\": [], \"rx_sno\": []}, \n",
    "        \"scispacy\": {\"full\": [], \"rx_sno\": []},\n",
    "        \"metamap\": {\"full\": [70.639], \"rx_sno\": [49.636]},\n",
    "        \"ctakes\": {\"full\": [0], \"rx_sno\": [119.684]},\n",
    "    }\n",
    "}\n",
    "\n",
    "n = 10\n",
    "\n",
    "model = health_bert.HealthBert.load_from_checkpoint(\n",
    "    constants.BASE_PATH + \"models/health_bert/\" + \"pubmedbert_pubmed_sentence.ckpt\"\n",
    ")\n",
    "\n",
    "for _ in tqdm(range(n), total=n):\n",
    "    # termhood\n",
    "    for n_gram in (1, 2, 3):\n",
    "        start = time.perf_counter()\n",
    "        health_causenet.causenet._contrastive_score(\n",
    "            test_causenet.loc[test_causenet.dataset == \"random_full\"],\n",
    "            medical_termhood[\"encyclopedia\"][\"discriminative_weight\"],\n",
    "            p=1,\n",
    "            n_gram_size=(1, n_gram),\n",
    "            verbose=False,\n",
    "        )\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times[\"phrase\"][\"termhood\"][n_gram].append(elapsed)\n",
    "        start = time.perf_counter()\n",
    "        health_causenet.causenet._contrastive_score(\n",
    "            sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"].assign(\n",
    "                cause=sentence_test_causenet.sentence, effect=\"\"\n",
    "            ),\n",
    "            medical_termhood[\"encyclopedia\"][\"discriminative_weight\"],\n",
    "            p=1,\n",
    "            n_gram_size=(1, n_gram),\n",
    "            verbose=False,\n",
    "        )\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times[\"sentence\"][\"termhood\"][n_gram].append(elapsed)\n",
    "\n",
    "    # bert\n",
    "    start = time.perf_counter()\n",
    "    health_causenet.causenet._health_bert(\n",
    "        test_causenet.loc[test_causenet.dataset == \"random_full\"],\n",
    "        model,\n",
    "        verbose=False,\n",
    "        batch_size=1,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    start = time.perf_counter()\n",
    "    times[\"phrase\"][\"bert\"][\" \"].append(elapsed)\n",
    "    health_causenet.causenet._health_bert(\n",
    "        sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"].assign(\n",
    "            cause=sentence_test_causenet.sentence, effect=\"\"\n",
    "        ),\n",
    "        model,\n",
    "        batch_size=1,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"sentence\"][\"bert\"][\" \"].append(elapsed)\n",
    "\n",
    "    # quickumls\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        test_causenet.loc[test_causenet.dataset == \"random_full\"],\n",
    "        \"quickumls\",\n",
    "        jaccard_threshold=0.9,\n",
    "        umls_subset=\"full\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"phrase\"][\"quickumls\"][\"full\"].append(elapsed)\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        test_causenet.loc[test_causenet.dataset == \"random_full\"],\n",
    "        \"quickumls\",\n",
    "        jaccard_threshold=0.9,\n",
    "        umls_subset=\"rx_sno\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"phrase\"][\"quickumls\"][\"rx_sno\"].append(elapsed)\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"].assign(\n",
    "            cause=sentence_test_causenet.sentence, effect=\"\"\n",
    "        ),\n",
    "        \"quickumls\",\n",
    "        jaccard_threshold=0.9,\n",
    "        umls_subset=\"full\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"sentence\"][\"quickumls\"][\"full\"].append(elapsed)\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"].assign(\n",
    "            cause=sentence_test_causenet.sentence, effect=\"\"\n",
    "        ),\n",
    "        \"quickumls\",\n",
    "        jaccard_threshold=0.9,\n",
    "        umls_subset=\"rx_sno\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"sentence\"][\"quickumls\"][\"rx_sno\"].append(elapsed)\n",
    "\n",
    "    # scispacy\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        test_causenet.loc[test_causenet.dataset == \"random_full\"],\n",
    "        \"scispacy\",\n",
    "        threshold=0.9,\n",
    "        umls_subset=\"full\",\n",
    "        model=\"en_core_sci_sm\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"phrase\"][\"scispacy\"][\"full\"].append(elapsed)\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        test_causenet.loc[test_causenet.dataset == \"random_full\"],\n",
    "        \"scispacy\",\n",
    "        threshold=0.9,\n",
    "        umls_subset=\"rx_sno\",\n",
    "        model=\"en_core_sci_sm\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"phrase\"][\"scispacy\"][\"rx_sno\"].append(elapsed)\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"].assign(\n",
    "            cause=sentence_test_causenet.sentence, effect=\"\"\n",
    "        ),\n",
    "        \"scispacy\",\n",
    "        threshold=0.9,\n",
    "        umls_subset=\"full\",\n",
    "        model=\"en_core_sci_sm\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"sentence\"][\"scispacy\"][\"full\"].append(elapsed)\n",
    "    start = time.perf_counter()\n",
    "    CauseNet.is_medical(\n",
    "        sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"].assign(\n",
    "            cause=sentence_test_causenet.sentence, effect=\"\"\n",
    "        ),\n",
    "        \"scispacy\",\n",
    "        threshold=0.9,\n",
    "        umls_subset=\"rx_sno\",\n",
    "        model=\"en_core_sci_sm\",\n",
    "        st21pv=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start\n",
    "    times[\"sentence\"][\"scispacy\"][\"rx_sno\"].append(elapsed)\n",
    "\n",
    "\n",
    "def create_index(index, dictionary, index_terms):\n",
    "    for key, value in dictionary.items():\n",
    "        index_terms.append(key)\n",
    "        if isinstance(value, list):\n",
    "            index.append(tuple(index_terms))\n",
    "            index_terms = index_terms[:-1]\n",
    "        else:\n",
    "            _, index_terms = create_index(index, value, index_terms)\n",
    "    index_terms = index_terms[:-1]\n",
    "    return index, index_terms\n",
    "\n",
    "def grab_values(dictionary):\n",
    "    values = []\n",
    "    for value in dictionary.values():\n",
    "        if isinstance(value, dict):\n",
    "            values.extend(grab_values(value))\n",
    "        else:\n",
    "            values.append(sum(value) / len(value))\n",
    "    return values\n",
    "\n",
    "index, _ = create_index([], times, [])\n",
    "index\n",
    "values = grab_values(times)\n",
    "times_df = pd.DataFrame(values, pd.MultiIndex.from_tuples(index, names=[\"data\", \"method\", \"n-gram\"]), columns=[\"time\"])\n",
    "num_samples = pd.Series(\n",
    "    [\n",
    "        test_causenet.loc[test_causenet.dataset == \"random_full\"].shape[0],\n",
    "        sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"].shape[0]\n",
    "    ],\n",
    "    index=pd.Index([\"phrase\", \"sentence\"], name=\"data\"),\n",
    ")\n",
    "times_df[\"time_per_iter\"] = (times_df[\"time\"] / num_samples) * 1000\n",
    "ratios = times_df[\"time\"].values[:, None] / times_df[\"time\"].values[None, :]\n",
    "ratio_df = pd.DataFrame(ratios, index=times_df.index, columns=times_df.index)\n",
    "\n",
    "from IPython.display import display\n",
    "display(times_df)\n",
    "display(ratio_df.loc[\"sentence\", \"sentence\"])\n",
    "display(ratio_df.loc[\"phrase\", \"phrase\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cd9f4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>time_per_iter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th>method</th>\n",
       "      <th>n-gram</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">sentence</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">termhood</th>\n",
       "      <th>1</th>\n",
       "      <td>1.020202</td>\n",
       "      <td>1.020202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.972861</td>\n",
       "      <td>1.972861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.820309</td>\n",
       "      <td>2.820309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert</th>\n",
       "      <th></th>\n",
       "      <td>47.768734</td>\n",
       "      <td>47.768734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">quickumls</th>\n",
       "      <th>full</th>\n",
       "      <td>11.414734</td>\n",
       "      <td>11.414734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>8.980932</td>\n",
       "      <td>8.980932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">scispacy</th>\n",
       "      <th>full</th>\n",
       "      <td>16.795120</td>\n",
       "      <td>16.795120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>15.958199</td>\n",
       "      <td>15.958199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">metamap</th>\n",
       "      <th>full</th>\n",
       "      <td>160.638000</td>\n",
       "      <td>160.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>120.277000</td>\n",
       "      <td>120.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ctakes</th>\n",
       "      <th>full</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>212.711000</td>\n",
       "      <td>212.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">phrase</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">termhood</th>\n",
       "      <th>1</th>\n",
       "      <td>0.564155</td>\n",
       "      <td>0.564155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.928234</td>\n",
       "      <td>0.928234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.265505</td>\n",
       "      <td>1.265505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert</th>\n",
       "      <th></th>\n",
       "      <td>60.190908</td>\n",
       "      <td>60.190908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">quickumls</th>\n",
       "      <th>full</th>\n",
       "      <td>8.210610</td>\n",
       "      <td>8.210610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>7.229493</td>\n",
       "      <td>7.229493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">scispacy</th>\n",
       "      <th>full</th>\n",
       "      <td>17.543986</td>\n",
       "      <td>17.543986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>16.383440</td>\n",
       "      <td>16.383440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">metamap</th>\n",
       "      <th>full</th>\n",
       "      <td>70.639000</td>\n",
       "      <td>70.639000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>49.636000</td>\n",
       "      <td>49.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ctakes</th>\n",
       "      <th>full</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>119.684000</td>\n",
       "      <td>119.684000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 time  time_per_iter\n",
       "data     method    n-gram                           \n",
       "sentence termhood  1         1.020202       1.020202\n",
       "                   2         1.972861       1.972861\n",
       "                   3         2.820309       2.820309\n",
       "         bert               47.768734      47.768734\n",
       "         quickumls full     11.414734      11.414734\n",
       "                   rx_sno    8.980932       8.980932\n",
       "         scispacy  full     16.795120      16.795120\n",
       "                   rx_sno   15.958199      15.958199\n",
       "         metamap   full    160.638000     160.638000\n",
       "                   rx_sno  120.277000     120.277000\n",
       "         ctakes    full      0.000000       0.000000\n",
       "                   rx_sno  212.711000     212.711000\n",
       "phrase   termhood  1         0.564155       0.564155\n",
       "                   2         0.928234       0.928234\n",
       "                   3         1.265505       1.265505\n",
       "         bert               60.190908      60.190908\n",
       "         quickumls full      8.210610       8.210610\n",
       "                   rx_sno    7.229493       7.229493\n",
       "         scispacy  full     17.543986      17.543986\n",
       "                   rx_sno   16.383440      16.383440\n",
       "         metamap   full     70.639000      70.639000\n",
       "                   rx_sno   49.636000      49.636000\n",
       "         ctakes    full      0.000000       0.000000\n",
       "                   rx_sno  119.684000     119.684000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_506937/2313220437.py:2: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  display(ratio_df.loc[\"sentence\", \"sentence\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th colspan=\"3\" halign=\"left\">termhood</th>\n",
       "      <th>bert</th>\n",
       "      <th colspan=\"2\" halign=\"left\">quickumls</th>\n",
       "      <th colspan=\"2\" halign=\"left\">scispacy</th>\n",
       "      <th colspan=\"2\" halign=\"left\">metamap</th>\n",
       "      <th colspan=\"2\" halign=\"left\">ctakes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>n-gram</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th></th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th>n-gram</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">termhood</th>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.517118</td>\n",
       "      <td>0.361734</td>\n",
       "      <td>0.021357</td>\n",
       "      <td>0.089376</td>\n",
       "      <td>0.113596</td>\n",
       "      <td>0.060744</td>\n",
       "      <td>0.063930</td>\n",
       "      <td>0.006351</td>\n",
       "      <td>0.008482</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.933794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.699519</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>0.172835</td>\n",
       "      <td>0.219672</td>\n",
       "      <td>0.117466</td>\n",
       "      <td>0.123627</td>\n",
       "      <td>0.012281</td>\n",
       "      <td>0.016403</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.009275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.764461</td>\n",
       "      <td>1.429553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059041</td>\n",
       "      <td>0.247076</td>\n",
       "      <td>0.314033</td>\n",
       "      <td>0.167924</td>\n",
       "      <td>0.176731</td>\n",
       "      <td>0.017557</td>\n",
       "      <td>0.023448</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.013259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert</th>\n",
       "      <th></th>\n",
       "      <td>46.822815</td>\n",
       "      <td>24.212928</td>\n",
       "      <td>16.937413</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.184831</td>\n",
       "      <td>5.318906</td>\n",
       "      <td>2.844203</td>\n",
       "      <td>2.993366</td>\n",
       "      <td>0.297369</td>\n",
       "      <td>0.397156</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.224571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">quickumls</th>\n",
       "      <th>full</th>\n",
       "      <td>11.188698</td>\n",
       "      <td>5.785879</td>\n",
       "      <td>4.047335</td>\n",
       "      <td>0.238958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.270997</td>\n",
       "      <td>0.679646</td>\n",
       "      <td>0.715290</td>\n",
       "      <td>0.071059</td>\n",
       "      <td>0.094904</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.053663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>8.803091</td>\n",
       "      <td>4.552238</td>\n",
       "      <td>3.184379</td>\n",
       "      <td>0.188009</td>\n",
       "      <td>0.786784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.534735</td>\n",
       "      <td>0.562779</td>\n",
       "      <td>0.055908</td>\n",
       "      <td>0.074669</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.042221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">scispacy</th>\n",
       "      <th>full</th>\n",
       "      <td>16.462542</td>\n",
       "      <td>8.513079</td>\n",
       "      <td>5.955064</td>\n",
       "      <td>0.351592</td>\n",
       "      <td>1.471355</td>\n",
       "      <td>1.870086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.052445</td>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.139637</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.078957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>15.642194</td>\n",
       "      <td>8.088863</td>\n",
       "      <td>5.658316</td>\n",
       "      <td>0.334072</td>\n",
       "      <td>1.398035</td>\n",
       "      <td>1.776898</td>\n",
       "      <td>0.950169</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099343</td>\n",
       "      <td>0.132679</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.075023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">metamap</th>\n",
       "      <th>full</th>\n",
       "      <td>157.457035</td>\n",
       "      <td>81.423893</td>\n",
       "      <td>56.957592</td>\n",
       "      <td>3.362827</td>\n",
       "      <td>14.072865</td>\n",
       "      <td>17.886562</td>\n",
       "      <td>9.564564</td>\n",
       "      <td>10.066173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.335567</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.755194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>117.895267</td>\n",
       "      <td>60.965784</td>\n",
       "      <td>42.646748</td>\n",
       "      <td>2.517902</td>\n",
       "      <td>10.536996</td>\n",
       "      <td>13.392485</td>\n",
       "      <td>7.161426</td>\n",
       "      <td>7.537003</td>\n",
       "      <td>0.748746</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.565448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ctakes</th>\n",
       "      <th>full</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>208.498882</td>\n",
       "      <td>107.818559</td>\n",
       "      <td>75.421173</td>\n",
       "      <td>4.452934</td>\n",
       "      <td>18.634776</td>\n",
       "      <td>23.684735</td>\n",
       "      <td>12.665048</td>\n",
       "      <td>13.329261</td>\n",
       "      <td>1.324164</td>\n",
       "      <td>1.768509</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "method              termhood                             bert  quickumls  \\\n",
       "n-gram                     1           2          3                 full   \n",
       "method    n-gram                                                           \n",
       "termhood  1         1.000000    0.517118   0.361734  0.021357   0.089376   \n",
       "          2         1.933794    1.000000   0.699519  0.041300   0.172835   \n",
       "          3         2.764461    1.429553   1.000000  0.059041   0.247076   \n",
       "bert               46.822815   24.212928  16.937413  1.000000   4.184831   \n",
       "quickumls full     11.188698    5.785879   4.047335  0.238958   1.000000   \n",
       "          rx_sno    8.803091    4.552238   3.184379  0.188009   0.786784   \n",
       "scispacy  full     16.462542    8.513079   5.955064  0.351592   1.471355   \n",
       "          rx_sno   15.642194    8.088863   5.658316  0.334072   1.398035   \n",
       "metamap   full    157.457035   81.423893  56.957592  3.362827  14.072865   \n",
       "          rx_sno  117.895267   60.965784  42.646748  2.517902  10.536996   \n",
       "ctakes    full      0.000000    0.000000   0.000000  0.000000   0.000000   \n",
       "          rx_sno  208.498882  107.818559  75.421173  4.452934  18.634776   \n",
       "\n",
       "method                        scispacy              metamap           ctakes  \\\n",
       "n-gram               rx_sno       full     rx_sno      full    rx_sno   full   \n",
       "method    n-gram                                                               \n",
       "termhood  1        0.113596   0.060744   0.063930  0.006351  0.008482    inf   \n",
       "          2        0.219672   0.117466   0.123627  0.012281  0.016403    inf   \n",
       "          3        0.314033   0.167924   0.176731  0.017557  0.023448    inf   \n",
       "bert               5.318906   2.844203   2.993366  0.297369  0.397156    inf   \n",
       "quickumls full     1.270997   0.679646   0.715290  0.071059  0.094904    inf   \n",
       "          rx_sno   1.000000   0.534735   0.562779  0.055908  0.074669    inf   \n",
       "scispacy  full     1.870086   1.000000   1.052445  0.104553  0.139637    inf   \n",
       "          rx_sno   1.776898   0.950169   1.000000  0.099343  0.132679    inf   \n",
       "metamap   full    17.886562   9.564564  10.066173  1.000000  1.335567    inf   \n",
       "          rx_sno  13.392485   7.161426   7.537003  0.748746  1.000000    inf   \n",
       "ctakes    full     0.000000   0.000000   0.000000  0.000000  0.000000    NaN   \n",
       "          rx_sno  23.684735  12.665048  13.329261  1.324164  1.768509    inf   \n",
       "\n",
       "method                      \n",
       "n-gram              rx_sno  \n",
       "method    n-gram            \n",
       "termhood  1       0.004796  \n",
       "          2       0.009275  \n",
       "          3       0.013259  \n",
       "bert              0.224571  \n",
       "quickumls full    0.053663  \n",
       "          rx_sno  0.042221  \n",
       "scispacy  full    0.078957  \n",
       "          rx_sno  0.075023  \n",
       "metamap   full    0.755194  \n",
       "          rx_sno  0.565448  \n",
       "ctakes    full    0.000000  \n",
       "          rx_sno  1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_506937/2313220437.py:3: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  display(ratio_df.loc[\"phrase\", \"phrase\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th colspan=\"3\" halign=\"left\">termhood</th>\n",
       "      <th>bert</th>\n",
       "      <th colspan=\"2\" halign=\"left\">quickumls</th>\n",
       "      <th colspan=\"2\" halign=\"left\">scispacy</th>\n",
       "      <th colspan=\"2\" halign=\"left\">metamap</th>\n",
       "      <th colspan=\"2\" halign=\"left\">ctakes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>n-gram</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th></th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "      <th>full</th>\n",
       "      <th>rx_sno</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th>n-gram</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">termhood</th>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.607773</td>\n",
       "      <td>0.445795</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.068711</td>\n",
       "      <td>0.078035</td>\n",
       "      <td>0.032157</td>\n",
       "      <td>0.034434</td>\n",
       "      <td>0.007986</td>\n",
       "      <td>0.011366</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.004714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.645352</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733489</td>\n",
       "      <td>0.015421</td>\n",
       "      <td>0.113053</td>\n",
       "      <td>0.128395</td>\n",
       "      <td>0.052909</td>\n",
       "      <td>0.056657</td>\n",
       "      <td>0.013141</td>\n",
       "      <td>0.018701</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.007756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.243186</td>\n",
       "      <td>1.363347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.154130</td>\n",
       "      <td>0.175048</td>\n",
       "      <td>0.072133</td>\n",
       "      <td>0.077243</td>\n",
       "      <td>0.017915</td>\n",
       "      <td>0.025496</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.010574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert</th>\n",
       "      <th></th>\n",
       "      <td>106.692117</td>\n",
       "      <td>64.844540</td>\n",
       "      <td>47.562765</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.330870</td>\n",
       "      <td>8.325744</td>\n",
       "      <td>3.430857</td>\n",
       "      <td>3.673887</td>\n",
       "      <td>0.852092</td>\n",
       "      <td>1.212646</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.502915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">quickumls</th>\n",
       "      <th>full</th>\n",
       "      <td>14.553815</td>\n",
       "      <td>8.845409</td>\n",
       "      <td>6.488011</td>\n",
       "      <td>0.136409</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.135710</td>\n",
       "      <td>0.468001</td>\n",
       "      <td>0.501153</td>\n",
       "      <td>0.116233</td>\n",
       "      <td>0.165416</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.068602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>12.814725</td>\n",
       "      <td>7.788438</td>\n",
       "      <td>5.712735</td>\n",
       "      <td>0.120109</td>\n",
       "      <td>0.880506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.412078</td>\n",
       "      <td>0.441268</td>\n",
       "      <td>0.102344</td>\n",
       "      <td>0.145650</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.060405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">scispacy</th>\n",
       "      <th>full</th>\n",
       "      <td>31.097803</td>\n",
       "      <td>18.900391</td>\n",
       "      <td>13.863231</td>\n",
       "      <td>0.291472</td>\n",
       "      <td>2.136746</td>\n",
       "      <td>2.426724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.070837</td>\n",
       "      <td>0.248361</td>\n",
       "      <td>0.353453</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.146586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>29.040663</td>\n",
       "      <td>17.650118</td>\n",
       "      <td>12.946169</td>\n",
       "      <td>0.272191</td>\n",
       "      <td>1.995399</td>\n",
       "      <td>2.266195</td>\n",
       "      <td>0.933849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.231932</td>\n",
       "      <td>0.330072</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.136889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">metamap</th>\n",
       "      <th>full</th>\n",
       "      <td>125.212008</td>\n",
       "      <td>76.100422</td>\n",
       "      <td>55.818831</td>\n",
       "      <td>1.173583</td>\n",
       "      <td>8.603381</td>\n",
       "      <td>9.770948</td>\n",
       "      <td>4.026394</td>\n",
       "      <td>4.311610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.423140</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.590213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>87.982888</td>\n",
       "      <td>53.473585</td>\n",
       "      <td>39.222293</td>\n",
       "      <td>0.824643</td>\n",
       "      <td>6.045349</td>\n",
       "      <td>6.865765</td>\n",
       "      <td>2.829232</td>\n",
       "      <td>3.029645</td>\n",
       "      <td>0.702671</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.414725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ctakes</th>\n",
       "      <th>full</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rx_sno</th>\n",
       "      <td>212.147312</td>\n",
       "      <td>128.937314</td>\n",
       "      <td>94.574117</td>\n",
       "      <td>1.988407</td>\n",
       "      <td>14.576750</td>\n",
       "      <td>16.554964</td>\n",
       "      <td>6.821939</td>\n",
       "      <td>7.305182</td>\n",
       "      <td>1.694305</td>\n",
       "      <td>2.411234</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "method              termhood                             bert  quickumls  \\\n",
       "n-gram                     1           2          3                 full   \n",
       "method    n-gram                                                           \n",
       "termhood  1         1.000000    0.607773   0.445795  0.009373   0.068711   \n",
       "          2         1.645352    1.000000   0.733489  0.015421   0.113053   \n",
       "          3         2.243186    1.363347   1.000000  0.021025   0.154130   \n",
       "bert              106.692117   64.844540  47.562765  1.000000   7.330870   \n",
       "quickumls full     14.553815    8.845409   6.488011  0.136409   1.000000   \n",
       "          rx_sno   12.814725    7.788438   5.712735  0.120109   0.880506   \n",
       "scispacy  full     31.097803   18.900391  13.863231  0.291472   2.136746   \n",
       "          rx_sno   29.040663   17.650118  12.946169  0.272191   1.995399   \n",
       "metamap   full    125.212008   76.100422  55.818831  1.173583   8.603381   \n",
       "          rx_sno   87.982888   53.473585  39.222293  0.824643   6.045349   \n",
       "ctakes    full      0.000000    0.000000   0.000000  0.000000   0.000000   \n",
       "          rx_sno  212.147312  128.937314  94.574117  1.988407  14.576750   \n",
       "\n",
       "method                       scispacy             metamap           ctakes  \\\n",
       "n-gram               rx_sno      full    rx_sno      full    rx_sno   full   \n",
       "method    n-gram                                                             \n",
       "termhood  1        0.078035  0.032157  0.034434  0.007986  0.011366    inf   \n",
       "          2        0.128395  0.052909  0.056657  0.013141  0.018701    inf   \n",
       "          3        0.175048  0.072133  0.077243  0.017915  0.025496    inf   \n",
       "bert               8.325744  3.430857  3.673887  0.852092  1.212646    inf   \n",
       "quickumls full     1.135710  0.468001  0.501153  0.116233  0.165416    inf   \n",
       "          rx_sno   1.000000  0.412078  0.441268  0.102344  0.145650    inf   \n",
       "scispacy  full     2.426724  1.000000  1.070837  0.248361  0.353453    inf   \n",
       "          rx_sno   2.266195  0.933849  1.000000  0.231932  0.330072    inf   \n",
       "metamap   full     9.770948  4.026394  4.311610  1.000000  1.423140    inf   \n",
       "          rx_sno   6.865765  2.829232  3.029645  0.702671  1.000000    inf   \n",
       "ctakes    full     0.000000  0.000000  0.000000  0.000000  0.000000    NaN   \n",
       "          rx_sno  16.554964  6.821939  7.305182  1.694305  2.411234    inf   \n",
       "\n",
       "method                      \n",
       "n-gram              rx_sno  \n",
       "method    n-gram            \n",
       "termhood  1       0.004714  \n",
       "          2       0.007756  \n",
       "          3       0.010574  \n",
       "bert              0.502915  \n",
       "quickumls full    0.068602  \n",
       "          rx_sno  0.060405  \n",
       "scispacy  full    0.146586  \n",
       "          rx_sno  0.136889  \n",
       "metamap   full    0.590213  \n",
       "          rx_sno  0.414725  \n",
       "ctakes    full    0.000000  \n",
       "          rx_sno  1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(times_df)\n",
    "display(ratio_df.loc[\"sentence\", \"sentence\"])\n",
    "display(ratio_df.loc[\"phrase\", \"phrase\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f372da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(test_causenet_predictions, label):\n",
    "    drop_labels = [\"medical_score-cause\", \"medical_score-effect\"]\n",
    "    pg.set_description(\" \".join(label))\n",
    "    st21pv = \"st21pv\" in label\n",
    "    if label[0] == \"health_bert\":\n",
    "        _, name, corpus, text_type = label\n",
    "        model_path = constants.CEPH_PATH + \"models/health_bert/\"\n",
    "        model = health_bert.HealthBert.load_from_checkpoint(\n",
    "            model_path + f\"{name}_{corpus}_{text_type}.ckpt\"\n",
    "        )\n",
    "        medical_score = health_causenet.causenet._health_bert(\n",
    "            test_causenet_predictions,\n",
    "            model,\n",
    "            verbose=False,\n",
    "        )\n",
    "    elif label[0] in (\"metamap\", \"ctakes\"):\n",
    "        json_path = os.path.join(\n",
    "            constants.CEPH_PATH, \"tagger_jsons\", f\"{label[0]}-{label[1]}.jsonl\"\n",
    "        )\n",
    "        medical_score = CauseNet.is_medical(\n",
    "            test_causenet_predictions, \"tagger\", json_path=json_path, st21pv=st21pv\n",
    "        )\n",
    "    elif label[0] == \"scispacy\":\n",
    "        umls_subset, model, threshold = label[1:4]\n",
    "        medical_score = CauseNet.is_medical(\n",
    "            test_causenet_predictions,\n",
    "            \"scispacy\",\n",
    "            umls_subset=umls_subset,\n",
    "            model=model,\n",
    "            threshold=float(threshold),\n",
    "            verbose=False,\n",
    "            st21pv=st21pv,\n",
    "        )\n",
    "    elif label[0] == \"quickumls\":\n",
    "        umls_subset, jaccard_threshold = label[1:3]\n",
    "        jaccard_threshold = float(jaccard_threshold)\n",
    "        medical_score = CauseNet.is_medical(\n",
    "            test_causenet_predictions,\n",
    "            \"quickumls\",\n",
    "            jaccard_threshold=jaccard_threshold,\n",
    "            umls_subset=umls_subset,\n",
    "            st21pv=st21pv,\n",
    "            verbose=False,\n",
    "        )\n",
    "    elif label[0] in contrastive_scores:\n",
    "        contrastive_score, corpus, n_gram_size, p_value = label\n",
    "        n_gram_size = tuple(n_gram_size.strip(\"()\").split(\", \"))\n",
    "        n_gram_size = (int(n_gram_size[0]), int(n_gram_size[1]))\n",
    "        neg = \"neg_\" in p_value\n",
    "        if neg:\n",
    "            p_value = p_value[4:]\n",
    "        try:\n",
    "            p_value = int(p_value)\n",
    "        except:\n",
    "            p_value = float(p_value)\n",
    "        if neg:\n",
    "            p_value = p_value * -1\n",
    "        medical_score = health_causenet.causenet._contrastive_score(\n",
    "            test_causenet_predictions,\n",
    "            medical_termhood[corpus][contrastive_score],\n",
    "            p=p_value,\n",
    "            n_gram_size=n_gram_size,\n",
    "            verbose=False,\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(f\"unknown label {label}\")\n",
    "    suffix = \"-\" + \"-\".join(label)\n",
    "    medical_score.index = test_causenet_predictions.index\n",
    "    medical_score = medical_score.add_suffix(suffix)\n",
    "    return medical_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00708738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing missing columns\n",
      "contrastive_weight encyclopedia (1, 1) 0\n",
      "contrastive_weight encyclopedia (1, 1) neg_1\n",
      "contrastive_weight encyclopedia (1, 1) neg_10\n",
      "contrastive_weight encyclopedia (1, 1) neg_2\n",
      "contrastive_weight encyclopedia (1, 1) neg_5\n",
      "contrastive_weight encyclopedia (1, 1) neg_inf\n",
      "contrastive_weight encyclopedia (1, 2) 0\n",
      "contrastive_weight encyclopedia (1, 2) neg_1\n",
      "contrastive_weight encyclopedia (1, 2) neg_10\n",
      "contrastive_weight encyclopedia (1, 2) neg_2\n",
      "contrastive_weight encyclopedia (1, 2) neg_5\n",
      "contrastive_weight encyclopedia (1, 2) neg_inf\n",
      "contrastive_weight encyclopedia (1, 3) 0\n",
      "contrastive_weight encyclopedia (1, 3) neg_1\n",
      "contrastive_weight encyclopedia (1, 3) neg_10\n",
      "contrastive_weight encyclopedia (1, 3) neg_2\n",
      "contrastive_weight encyclopedia (1, 3) neg_5\n",
      "contrastive_weight encyclopedia (1, 3) neg_inf\n",
      "contrastive_weight pubmed (1, 1) 0\n",
      "contrastive_weight pubmed (1, 1) neg_1\n",
      "contrastive_weight pubmed (1, 1) neg_10\n",
      "contrastive_weight pubmed (1, 1) neg_2\n",
      "contrastive_weight pubmed (1, 1) neg_5\n",
      "contrastive_weight pubmed (1, 1) neg_inf\n",
      "contrastive_weight pubmed (1, 2) 0\n",
      "contrastive_weight pubmed (1, 2) neg_1\n",
      "contrastive_weight pubmed (1, 2) neg_10\n",
      "contrastive_weight pubmed (1, 2) neg_2\n",
      "contrastive_weight pubmed (1, 2) neg_5\n",
      "contrastive_weight pubmed (1, 2) neg_inf\n",
      "contrastive_weight pubmed (1, 3) 0\n",
      "contrastive_weight pubmed (1, 3) neg_1\n",
      "contrastive_weight pubmed (1, 3) neg_10\n",
      "contrastive_weight pubmed (1, 3) neg_2\n",
      "contrastive_weight pubmed (1, 3) neg_5\n",
      "contrastive_weight pubmed (1, 3) neg_inf\n",
      "contrastive_weight pubmed_central (1, 1) 0\n",
      "contrastive_weight pubmed_central (1, 1) neg_1\n",
      "contrastive_weight pubmed_central (1, 1) neg_10\n",
      "contrastive_weight pubmed_central (1, 1) neg_2\n",
      "contrastive_weight pubmed_central (1, 1) neg_5\n",
      "contrastive_weight pubmed_central (1, 1) neg_inf\n",
      "contrastive_weight pubmed_central (1, 2) 0\n",
      "contrastive_weight pubmed_central (1, 2) neg_1\n",
      "contrastive_weight pubmed_central (1, 2) neg_10\n",
      "contrastive_weight pubmed_central (1, 2) neg_2\n",
      "contrastive_weight pubmed_central (1, 2) neg_5\n",
      "contrastive_weight pubmed_central (1, 2) neg_inf\n",
      "contrastive_weight pubmed_central (1, 3) 0\n",
      "contrastive_weight pubmed_central (1, 3) neg_1\n",
      "contrastive_weight pubmed_central (1, 3) neg_10\n",
      "contrastive_weight pubmed_central (1, 3) neg_2\n",
      "contrastive_weight pubmed_central (1, 3) neg_5\n",
      "contrastive_weight pubmed_central (1, 3) neg_inf\n",
      "contrastive_weight textbook (1, 1) 0\n",
      "contrastive_weight textbook (1, 1) neg_1\n",
      "contrastive_weight textbook (1, 1) neg_10\n",
      "contrastive_weight textbook (1, 1) neg_2\n",
      "contrastive_weight textbook (1, 1) neg_5\n",
      "contrastive_weight textbook (1, 1) neg_inf\n",
      "contrastive_weight textbook (1, 2) 0\n",
      "contrastive_weight textbook (1, 2) neg_1\n",
      "contrastive_weight textbook (1, 2) neg_10\n",
      "contrastive_weight textbook (1, 2) neg_2\n",
      "contrastive_weight textbook (1, 2) neg_5\n",
      "contrastive_weight textbook (1, 2) neg_inf\n",
      "contrastive_weight textbook (1, 3) 0\n",
      "contrastive_weight textbook (1, 3) neg_1\n",
      "contrastive_weight textbook (1, 3) neg_10\n",
      "contrastive_weight textbook (1, 3) neg_2\n",
      "contrastive_weight textbook (1, 3) neg_5\n",
      "contrastive_weight textbook (1, 3) neg_inf\n",
      "discriminative_weight encyclopedia (1, 1) 0\n",
      "discriminative_weight encyclopedia (1, 1) neg_1\n",
      "discriminative_weight encyclopedia (1, 1) neg_10\n",
      "discriminative_weight encyclopedia (1, 1) neg_2\n",
      "discriminative_weight encyclopedia (1, 1) neg_5\n",
      "discriminative_weight encyclopedia (1, 1) neg_inf\n",
      "discriminative_weight encyclopedia (1, 2) 0\n",
      "discriminative_weight encyclopedia (1, 2) neg_1\n",
      "discriminative_weight encyclopedia (1, 2) neg_10\n",
      "discriminative_weight encyclopedia (1, 2) neg_2\n",
      "discriminative_weight encyclopedia (1, 2) neg_5\n",
      "discriminative_weight encyclopedia (1, 2) neg_inf\n",
      "discriminative_weight encyclopedia (1, 3) 0\n",
      "discriminative_weight encyclopedia (1, 3) neg_1\n",
      "discriminative_weight encyclopedia (1, 3) neg_10\n",
      "discriminative_weight encyclopedia (1, 3) neg_2\n",
      "discriminative_weight encyclopedia (1, 3) neg_5\n",
      "discriminative_weight encyclopedia (1, 3) neg_inf\n",
      "discriminative_weight pubmed (1, 1) 0\n",
      "discriminative_weight pubmed (1, 1) neg_1\n",
      "discriminative_weight pubmed (1, 1) neg_10\n",
      "discriminative_weight pubmed (1, 1) neg_2\n",
      "discriminative_weight pubmed (1, 1) neg_5\n",
      "discriminative_weight pubmed (1, 1) neg_inf\n",
      "discriminative_weight pubmed (1, 2) 0\n",
      "discriminative_weight pubmed (1, 2) neg_1\n",
      "discriminative_weight pubmed (1, 2) neg_10\n",
      "discriminative_weight pubmed (1, 2) neg_2\n",
      "discriminative_weight pubmed (1, 2) neg_5\n",
      "discriminative_weight pubmed (1, 2) neg_inf\n",
      "discriminative_weight pubmed (1, 3) 0\n",
      "discriminative_weight pubmed (1, 3) neg_1\n",
      "discriminative_weight pubmed (1, 3) neg_10\n",
      "discriminative_weight pubmed (1, 3) neg_2\n",
      "discriminative_weight pubmed (1, 3) neg_5\n",
      "discriminative_weight pubmed (1, 3) neg_inf\n",
      "discriminative_weight pubmed_central (1, 1) 0\n",
      "discriminative_weight pubmed_central (1, 1) neg_1\n",
      "discriminative_weight pubmed_central (1, 1) neg_10\n",
      "discriminative_weight pubmed_central (1, 1) neg_2\n",
      "discriminative_weight pubmed_central (1, 1) neg_5\n",
      "discriminative_weight pubmed_central (1, 1) neg_inf\n",
      "discriminative_weight pubmed_central (1, 2) 0\n",
      "discriminative_weight pubmed_central (1, 2) neg_1\n",
      "discriminative_weight pubmed_central (1, 2) neg_10\n",
      "discriminative_weight pubmed_central (1, 2) neg_2\n",
      "discriminative_weight pubmed_central (1, 2) neg_5\n",
      "discriminative_weight pubmed_central (1, 2) neg_inf\n",
      "discriminative_weight pubmed_central (1, 3) 0\n",
      "discriminative_weight pubmed_central (1, 3) neg_1\n",
      "discriminative_weight pubmed_central (1, 3) neg_10\n",
      "discriminative_weight pubmed_central (1, 3) neg_2\n",
      "discriminative_weight pubmed_central (1, 3) neg_5\n",
      "discriminative_weight pubmed_central (1, 3) neg_inf\n",
      "discriminative_weight textbook (1, 1) 0\n",
      "discriminative_weight textbook (1, 1) neg_1\n",
      "discriminative_weight textbook (1, 1) neg_10\n",
      "discriminative_weight textbook (1, 1) neg_2\n",
      "discriminative_weight textbook (1, 1) neg_5\n",
      "discriminative_weight textbook (1, 1) neg_inf\n",
      "discriminative_weight textbook (1, 2) 0\n",
      "discriminative_weight textbook (1, 2) neg_1\n",
      "discriminative_weight textbook (1, 2) neg_10\n",
      "discriminative_weight textbook (1, 2) neg_2\n",
      "discriminative_weight textbook (1, 2) neg_5\n",
      "discriminative_weight textbook (1, 2) neg_inf\n",
      "discriminative_weight textbook (1, 3) 0\n",
      "discriminative_weight textbook (1, 3) neg_1\n",
      "discriminative_weight textbook (1, 3) neg_10\n",
      "discriminative_weight textbook (1, 3) neg_2\n",
      "discriminative_weight textbook (1, 3) neg_5\n",
      "discriminative_weight textbook (1, 3) neg_inf\n",
      "health_bert bert encyclopedia noun_phrase\n",
      "health_bert bert encyclopedia sentence\n",
      "health_bert bert pubmed noun_phrase\n",
      "health_bert bert pubmed sentence\n",
      "health_bert pubmedbert encyclopedia noun_phrase\n",
      "health_bert pubmedbert encyclopedia sentence\n",
      "health_bert pubmedbert pubmed noun_phrase\n",
      "health_bert pubmedbert pubmed sentence\n",
      "health_bert scibert encyclopedia noun_phrase\n",
      "health_bert scibert encyclopedia sentence\n",
      "health_bert scibert pubmed noun_phrase\n",
      "health_bert scibert pubmed sentence\n",
      "term_domain_specificity encyclopedia (1, 1) 0\n",
      "term_domain_specificity encyclopedia (1, 1) neg_1\n",
      "term_domain_specificity encyclopedia (1, 1) neg_10\n",
      "term_domain_specificity encyclopedia (1, 1) neg_2\n",
      "term_domain_specificity encyclopedia (1, 1) neg_5\n",
      "term_domain_specificity encyclopedia (1, 1) neg_inf\n",
      "term_domain_specificity encyclopedia (1, 2) 0\n",
      "term_domain_specificity encyclopedia (1, 2) neg_1\n",
      "term_domain_specificity encyclopedia (1, 2) neg_10\n",
      "term_domain_specificity encyclopedia (1, 2) neg_2\n",
      "term_domain_specificity encyclopedia (1, 2) neg_5\n",
      "term_domain_specificity encyclopedia (1, 2) neg_inf\n",
      "term_domain_specificity encyclopedia (1, 3) 0\n",
      "term_domain_specificity encyclopedia (1, 3) neg_1\n",
      "term_domain_specificity encyclopedia (1, 3) neg_10\n",
      "term_domain_specificity encyclopedia (1, 3) neg_2\n",
      "term_domain_specificity encyclopedia (1, 3) neg_5\n",
      "term_domain_specificity encyclopedia (1, 3) neg_inf\n",
      "term_domain_specificity pubmed (1, 1) 0\n",
      "term_domain_specificity pubmed (1, 1) neg_1\n",
      "term_domain_specificity pubmed (1, 1) neg_10\n",
      "term_domain_specificity pubmed (1, 1) neg_2\n",
      "term_domain_specificity pubmed (1, 1) neg_5\n",
      "term_domain_specificity pubmed (1, 1) neg_inf\n",
      "term_domain_specificity pubmed (1, 2) 0\n",
      "term_domain_specificity pubmed (1, 2) neg_1\n",
      "term_domain_specificity pubmed (1, 2) neg_10\n",
      "term_domain_specificity pubmed (1, 2) neg_2\n",
      "term_domain_specificity pubmed (1, 2) neg_5\n",
      "term_domain_specificity pubmed (1, 2) neg_inf\n",
      "term_domain_specificity pubmed (1, 3) 0\n",
      "term_domain_specificity pubmed (1, 3) neg_1\n",
      "term_domain_specificity pubmed (1, 3) neg_10\n",
      "term_domain_specificity pubmed (1, 3) neg_2\n",
      "term_domain_specificity pubmed (1, 3) neg_5\n",
      "term_domain_specificity pubmed (1, 3) neg_inf\n",
      "term_domain_specificity pubmed_central (1, 1) 0\n",
      "term_domain_specificity pubmed_central (1, 1) neg_1\n",
      "term_domain_specificity pubmed_central (1, 1) neg_10\n",
      "term_domain_specificity pubmed_central (1, 1) neg_2\n",
      "term_domain_specificity pubmed_central (1, 1) neg_5\n",
      "term_domain_specificity pubmed_central (1, 1) neg_inf\n",
      "term_domain_specificity pubmed_central (1, 2) 0\n",
      "term_domain_specificity pubmed_central (1, 2) neg_1\n",
      "term_domain_specificity pubmed_central (1, 2) neg_10\n",
      "term_domain_specificity pubmed_central (1, 2) neg_2\n",
      "term_domain_specificity pubmed_central (1, 2) neg_5\n",
      "term_domain_specificity pubmed_central (1, 2) neg_inf\n",
      "term_domain_specificity pubmed_central (1, 3) 0\n",
      "term_domain_specificity pubmed_central (1, 3) neg_1\n",
      "term_domain_specificity pubmed_central (1, 3) neg_10\n",
      "term_domain_specificity pubmed_central (1, 3) neg_2\n",
      "term_domain_specificity pubmed_central (1, 3) neg_5\n",
      "term_domain_specificity pubmed_central (1, 3) neg_inf\n",
      "term_domain_specificity textbook (1, 1) 0\n",
      "term_domain_specificity textbook (1, 1) neg_1\n",
      "term_domain_specificity textbook (1, 1) neg_10\n",
      "term_domain_specificity textbook (1, 1) neg_2\n",
      "term_domain_specificity textbook (1, 1) neg_5\n",
      "term_domain_specificity textbook (1, 1) neg_inf\n",
      "term_domain_specificity textbook (1, 2) 0\n",
      "term_domain_specificity textbook (1, 2) neg_1\n",
      "term_domain_specificity textbook (1, 2) neg_10\n",
      "term_domain_specificity textbook (1, 2) neg_2\n",
      "term_domain_specificity textbook (1, 2) neg_5\n",
      "term_domain_specificity textbook (1, 2) neg_inf\n",
      "term_domain_specificity textbook (1, 3) 0\n",
      "term_domain_specificity textbook (1, 3) neg_1\n",
      "term_domain_specificity textbook (1, 3) neg_10\n",
      "term_domain_specificity textbook (1, 3) neg_2\n",
      "term_domain_specificity textbook (1, 3) neg_5\n",
      "term_domain_specificity textbook (1, 3) neg_inf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a966e6b222684093bae9dc199ebf398c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cause</th>\n",
       "      <th>effect</th>\n",
       "      <th>support</th>\n",
       "      <th>count</th>\n",
       "      <th>dataset</th>\n",
       "      <th>cause_origin</th>\n",
       "      <th>effect_origin</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>medical_score-cause-term_domain_specificity-pubmed-(1, 1)-1</th>\n",
       "      <th>medical_score-effect-term_domain_specificity-pubmed-(1, 1)-1</th>\n",
       "      <th>...</th>\n",
       "      <th>medical_score-effect-discriminative_weight-pubmed_central-(1, 3)-neg_2</th>\n",
       "      <th>medical_score-effect-discriminative_weight-encyclopedia-(1, 3)-neg_2</th>\n",
       "      <th>medical_score-effect-discriminative_weight-pubmed-(1, 3)-neg_1</th>\n",
       "      <th>medical_score-effect-discriminative_weight-textbook-(1, 3)-neg_1</th>\n",
       "      <th>medical_score-effect-discriminative_weight-pubmed_central-(1, 3)-neg_1</th>\n",
       "      <th>medical_score-effect-discriminative_weight-encyclopedia-(1, 3)-neg_1</th>\n",
       "      <th>medical_score-effect-discriminative_weight-pubmed-(1, 3)-0</th>\n",
       "      <th>medical_score-effect-discriminative_weight-textbook-(1, 3)-0</th>\n",
       "      <th>medical_score-effect-discriminative_weight-pubmed_central-(1, 3)-0</th>\n",
       "      <th>medical_score-effect-discriminative_weight-encyclopedia-(1, 3)-0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accident</td>\n",
       "      <td>death</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1458.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.271897</td>\n",
       "      <td>0.580596</td>\n",
       "      <td>...</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "      <td>65.054640</td>\n",
       "      <td>21.417447</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "      <td>65.054640</td>\n",
       "      <td>21.417447</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pneumonia</td>\n",
       "      <td>death</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2.250561</td>\n",
       "      <td>0.580596</td>\n",
       "      <td>...</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "      <td>65.054640</td>\n",
       "      <td>21.417447</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "      <td>65.054640</td>\n",
       "      <td>21.417447</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disease</td>\n",
       "      <td>death</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.030815</td>\n",
       "      <td>0.580596</td>\n",
       "      <td>...</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "      <td>65.054640</td>\n",
       "      <td>21.417447</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "      <td>65.054640</td>\n",
       "      <td>21.417447</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>illness</td>\n",
       "      <td>death</td>\n",
       "      <td>36.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.511752</td>\n",
       "      <td>0.580596</td>\n",
       "      <td>...</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "      <td>65.054640</td>\n",
       "      <td>21.417447</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "      <td>65.054640</td>\n",
       "      <td>21.417447</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heart attack</td>\n",
       "      <td>death</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.752565</td>\n",
       "      <td>0.580596</td>\n",
       "      <td>...</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "      <td>65.054640</td>\n",
       "      <td>21.417447</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "      <td>65.054640</td>\n",
       "      <td>21.417447</td>\n",
       "      <td>57.204029</td>\n",
       "      <td>18.803064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14311</th>\n",
       "      <td>initial oxidation at carbon 5</td>\n",
       "      <td>thymine glycol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>practitioner_unsure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.309622</td>\n",
       "      <td>2.876154</td>\n",
       "      <td>...</td>\n",
       "      <td>276.292460</td>\n",
       "      <td>127.148997</td>\n",
       "      <td>417.882586</td>\n",
       "      <td>20.809708</td>\n",
       "      <td>286.801086</td>\n",
       "      <td>128.436658</td>\n",
       "      <td>431.847518</td>\n",
       "      <td>21.764573</td>\n",
       "      <td>298.182251</td>\n",
       "      <td>129.764249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14312</th>\n",
       "      <td>thyroid hormone deficits</td>\n",
       "      <td>language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>practitioner_sure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3.480315</td>\n",
       "      <td>0.250982</td>\n",
       "      <td>...</td>\n",
       "      <td>41.967722</td>\n",
       "      <td>4.199330</td>\n",
       "      <td>27.781071</td>\n",
       "      <td>14.799191</td>\n",
       "      <td>41.967722</td>\n",
       "      <td>4.199330</td>\n",
       "      <td>27.781071</td>\n",
       "      <td>14.799191</td>\n",
       "      <td>41.967722</td>\n",
       "      <td>4.199330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14313</th>\n",
       "      <td>neuropeptide profile</td>\n",
       "      <td>decreased food intake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>practitioner_sure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.542437</td>\n",
       "      <td>2.515899</td>\n",
       "      <td>...</td>\n",
       "      <td>238.498888</td>\n",
       "      <td>94.748032</td>\n",
       "      <td>284.867129</td>\n",
       "      <td>58.498232</td>\n",
       "      <td>268.957121</td>\n",
       "      <td>102.036191</td>\n",
       "      <td>338.810479</td>\n",
       "      <td>65.615679</td>\n",
       "      <td>297.029307</td>\n",
       "      <td>110.575203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14314</th>\n",
       "      <td>disorder</td>\n",
       "      <td>white heads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>practitioner_sure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.494475</td>\n",
       "      <td>0.247209</td>\n",
       "      <td>...</td>\n",
       "      <td>32.916396</td>\n",
       "      <td>3.574758</td>\n",
       "      <td>16.025289</td>\n",
       "      <td>6.810649</td>\n",
       "      <td>33.184736</td>\n",
       "      <td>4.423016</td>\n",
       "      <td>18.603061</td>\n",
       "      <td>6.827452</td>\n",
       "      <td>33.459747</td>\n",
       "      <td>6.457744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14315</th>\n",
       "      <td>deletion of pten in murine models</td>\n",
       "      <td>expansion of the prostate stem/progenitor cell...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>practitioner_unsure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2.743805</td>\n",
       "      <td>2.186758</td>\n",
       "      <td>...</td>\n",
       "      <td>81.822848</td>\n",
       "      <td>4.511152</td>\n",
       "      <td>112.034566</td>\n",
       "      <td>11.223560</td>\n",
       "      <td>109.927440</td>\n",
       "      <td>6.176349</td>\n",
       "      <td>190.645837</td>\n",
       "      <td>17.352078</td>\n",
       "      <td>175.014910</td>\n",
       "      <td>17.549714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14316 rows × 936 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   cause  \\\n",
       "0                               accident   \n",
       "1                              pneumonia   \n",
       "2                                disease   \n",
       "3                                illness   \n",
       "4                           heart attack   \n",
       "...                                  ...   \n",
       "14311      initial oxidation at carbon 5   \n",
       "14312           thyroid hormone deficits   \n",
       "14313               neuropeptide profile   \n",
       "14314                           disorder   \n",
       "14315  deletion of pten in murine models   \n",
       "\n",
       "                                                  effect  support   count  \\\n",
       "0                                                  death     38.0  1458.0   \n",
       "1                                                  death     37.0  1185.0   \n",
       "2                                                  death     37.0  1344.0   \n",
       "3                                                  death     36.0   571.0   \n",
       "4                                                  death     36.0  1005.0   \n",
       "...                                                  ...      ...     ...   \n",
       "14311                                     thymine glycol      NaN     NaN   \n",
       "14312                                           language      NaN     NaN   \n",
       "14313                              decreased food intake      NaN     NaN   \n",
       "14314                                        white heads      NaN     NaN   \n",
       "14315  expansion of the prostate stem/progenitor cell...      NaN     NaN   \n",
       "\n",
       "                   dataset cause_origin effect_origin  evaluation  \\\n",
       "0                  support          NaN           NaN           0   \n",
       "1                  support          NaN           NaN           1   \n",
       "2                  support          NaN           NaN           1   \n",
       "3                  support          NaN           NaN           1   \n",
       "4                  support          NaN           NaN           1   \n",
       "...                    ...          ...           ...         ...   \n",
       "14311  practitioner_unsure          NaN           NaN           1   \n",
       "14312    practitioner_sure          NaN           NaN           0   \n",
       "14313    practitioner_sure          NaN           NaN           0   \n",
       "14314    practitioner_sure          NaN           NaN           0   \n",
       "14315  practitioner_unsure          NaN           NaN           1   \n",
       "\n",
       "       medical_score-cause-term_domain_specificity-pubmed-(1, 1)-1  \\\n",
       "0                                               0.271897             \n",
       "1                                               2.250561             \n",
       "2                                               3.030815             \n",
       "3                                               1.511752             \n",
       "4                                               0.752565             \n",
       "...                                                  ...             \n",
       "14311                                           1.309622             \n",
       "14312                                           3.480315             \n",
       "14313                                           2.542437             \n",
       "14314                                           2.494475             \n",
       "14315                                           2.743805             \n",
       "\n",
       "       medical_score-effect-term_domain_specificity-pubmed-(1, 1)-1  ...  \\\n",
       "0                                               0.580596             ...   \n",
       "1                                               0.580596             ...   \n",
       "2                                               0.580596             ...   \n",
       "3                                               0.580596             ...   \n",
       "4                                               0.580596             ...   \n",
       "...                                                  ...             ...   \n",
       "14311                                           2.876154             ...   \n",
       "14312                                           0.250982             ...   \n",
       "14313                                           2.515899             ...   \n",
       "14314                                           0.247209             ...   \n",
       "14315                                           2.186758             ...   \n",
       "\n",
       "       medical_score-effect-discriminative_weight-pubmed_central-(1, 3)-neg_2  \\\n",
       "0                                              57.204029                        \n",
       "1                                              57.204029                        \n",
       "2                                              57.204029                        \n",
       "3                                              57.204029                        \n",
       "4                                              57.204029                        \n",
       "...                                                  ...                        \n",
       "14311                                         276.292460                        \n",
       "14312                                          41.967722                        \n",
       "14313                                         238.498888                        \n",
       "14314                                          32.916396                        \n",
       "14315                                          81.822848                        \n",
       "\n",
       "       medical_score-effect-discriminative_weight-encyclopedia-(1, 3)-neg_2  \\\n",
       "0                                              18.803064                      \n",
       "1                                              18.803064                      \n",
       "2                                              18.803064                      \n",
       "3                                              18.803064                      \n",
       "4                                              18.803064                      \n",
       "...                                                  ...                      \n",
       "14311                                         127.148997                      \n",
       "14312                                           4.199330                      \n",
       "14313                                          94.748032                      \n",
       "14314                                           3.574758                      \n",
       "14315                                           4.511152                      \n",
       "\n",
       "       medical_score-effect-discriminative_weight-pubmed-(1, 3)-neg_1  \\\n",
       "0                                              65.054640                \n",
       "1                                              65.054640                \n",
       "2                                              65.054640                \n",
       "3                                              65.054640                \n",
       "4                                              65.054640                \n",
       "...                                                  ...                \n",
       "14311                                         417.882586                \n",
       "14312                                          27.781071                \n",
       "14313                                         284.867129                \n",
       "14314                                          16.025289                \n",
       "14315                                         112.034566                \n",
       "\n",
       "       medical_score-effect-discriminative_weight-textbook-(1, 3)-neg_1  \\\n",
       "0                                              21.417447                  \n",
       "1                                              21.417447                  \n",
       "2                                              21.417447                  \n",
       "3                                              21.417447                  \n",
       "4                                              21.417447                  \n",
       "...                                                  ...                  \n",
       "14311                                          20.809708                  \n",
       "14312                                          14.799191                  \n",
       "14313                                          58.498232                  \n",
       "14314                                           6.810649                  \n",
       "14315                                          11.223560                  \n",
       "\n",
       "       medical_score-effect-discriminative_weight-pubmed_central-(1, 3)-neg_1  \\\n",
       "0                                              57.204029                        \n",
       "1                                              57.204029                        \n",
       "2                                              57.204029                        \n",
       "3                                              57.204029                        \n",
       "4                                              57.204029                        \n",
       "...                                                  ...                        \n",
       "14311                                         286.801086                        \n",
       "14312                                          41.967722                        \n",
       "14313                                         268.957121                        \n",
       "14314                                          33.184736                        \n",
       "14315                                         109.927440                        \n",
       "\n",
       "       medical_score-effect-discriminative_weight-encyclopedia-(1, 3)-neg_1  \\\n",
       "0                                              18.803064                      \n",
       "1                                              18.803064                      \n",
       "2                                              18.803064                      \n",
       "3                                              18.803064                      \n",
       "4                                              18.803064                      \n",
       "...                                                  ...                      \n",
       "14311                                         128.436658                      \n",
       "14312                                           4.199330                      \n",
       "14313                                         102.036191                      \n",
       "14314                                           4.423016                      \n",
       "14315                                           6.176349                      \n",
       "\n",
       "       medical_score-effect-discriminative_weight-pubmed-(1, 3)-0  \\\n",
       "0                                              65.054640            \n",
       "1                                              65.054640            \n",
       "2                                              65.054640            \n",
       "3                                              65.054640            \n",
       "4                                              65.054640            \n",
       "...                                                  ...            \n",
       "14311                                         431.847518            \n",
       "14312                                          27.781071            \n",
       "14313                                         338.810479            \n",
       "14314                                          18.603061            \n",
       "14315                                         190.645837            \n",
       "\n",
       "       medical_score-effect-discriminative_weight-textbook-(1, 3)-0  \\\n",
       "0                                              21.417447              \n",
       "1                                              21.417447              \n",
       "2                                              21.417447              \n",
       "3                                              21.417447              \n",
       "4                                              21.417447              \n",
       "...                                                  ...              \n",
       "14311                                          21.764573              \n",
       "14312                                          14.799191              \n",
       "14313                                          65.615679              \n",
       "14314                                           6.827452              \n",
       "14315                                          17.352078              \n",
       "\n",
       "       medical_score-effect-discriminative_weight-pubmed_central-(1, 3)-0  \\\n",
       "0                                              57.204029                    \n",
       "1                                              57.204029                    \n",
       "2                                              57.204029                    \n",
       "3                                              57.204029                    \n",
       "4                                              57.204029                    \n",
       "...                                                  ...                    \n",
       "14311                                         298.182251                    \n",
       "14312                                          41.967722                    \n",
       "14313                                         297.029307                    \n",
       "14314                                          33.459747                    \n",
       "14315                                         175.014910                    \n",
       "\n",
       "       medical_score-effect-discriminative_weight-encyclopedia-(1, 3)-0  \n",
       "0                                              18.803064                 \n",
       "1                                              18.803064                 \n",
       "2                                              18.803064                 \n",
       "3                                              18.803064                 \n",
       "4                                              18.803064                 \n",
       "...                                                  ...                 \n",
       "14311                                         129.764249                 \n",
       "14312                                           4.199330                 \n",
       "14313                                         110.575203                 \n",
       "14314                                           6.457744                 \n",
       "14315                                          17.549714                 \n",
       "\n",
       "[14316 rows x 936 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    test_causenet_predictions = pd.read_pickle(\n",
    "        os.path.join(constants.CEPH_PATH, \"test_causenet_predictions.pkl\")\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    test_causenet_predictions = test_causenet.loc[:, [\"cause\", \"effect\", \"dataset\"]]\n",
    "test_causenet_predictions = pd.read_pickle(\n",
    "    os.path.join(constants.CEPH_PATH, \"test_causenet_predictions.pkl\")\n",
    ")\n",
    "test_causenet_predictions = test_causenet_predictions.merge(\n",
    "    test_causenet.drop(\n",
    "        [\"support\", \"count\", \"cause_origin\", \"effect_origin\", \"evaluation\"], axis=1\n",
    "    ),\n",
    "    on=[\"cause\", \"effect\", \"dataset\"],\n",
    "    how=\"outer\",\n",
    ")\n",
    "\n",
    "jaccard_thresholds = [round(thresh, 2) for thresh in np.arange(0.7, 1.01, 0.1)]\n",
    "scispacy_thresholds = [round(thresh, 2) for thresh in np.arange(0.6, 0.9, 0.1)]\n",
    "scispacy_models = [\"en_core_sci_sm\", \"en_core_sci_lg\"]\n",
    "p_values = [-float(\"inf\"), -10, -5, -2, -1, 0, 1, 2, 5, 10, float(\"inf\")]\n",
    "n_gram_sizes = [(1, 1), (1, 2), (1, 3)]\n",
    "medical_corpora = [\n",
    "    \"pubmed\",\n",
    "    \"textbook\",\n",
    "    \"pubmed_central\",\n",
    "    \"encyclopedia\",\n",
    "]\n",
    "umls_subsets = [\n",
    "    \"full\",\n",
    "    \"rx_sno\",\n",
    "]\n",
    "text_types = [\"sentence\", \"noun_phrase\"]\n",
    "contrastive_scores = [\n",
    "    \"term_domain_specificity\",\n",
    "    \"contrastive_weight\",\n",
    "    \"discriminative_weight\",\n",
    "]\n",
    "bert_names = [\"bert\", \"scibert\", \"pubmedbert\"]\n",
    "\n",
    "labels = (\n",
    "    [\n",
    "        f\"quickumls-{umls_subset}-{jaccard_threshold}-st21pv\"\n",
    "        if st21pv\n",
    "        else f\"quickumls-{umls_subset}-{jaccard_threshold}\"\n",
    "        for st21pv in [True, False]\n",
    "        for umls_subset in umls_subsets\n",
    "        for jaccard_threshold in jaccard_thresholds\n",
    "    ]\n",
    "    + [\n",
    "        f\"scispacy-{umls_subset}-{model}-{threshold}-st21pv\"\n",
    "        if st21pv\n",
    "        else f\"scispacy-{umls_subset}-{model}-{threshold}\"\n",
    "        for st21pv in [True, False]\n",
    "        for model in scispacy_models\n",
    "        for umls_subset in umls_subsets\n",
    "        for threshold in scispacy_thresholds\n",
    "    ]\n",
    "    + [\n",
    "        f\"ctakes-{umls_subset}-st21pv\" if st21pv else f\"ctakes-{umls_subset}\"\n",
    "        for st21pv in [True, False]\n",
    "        for umls_subset in umls_subsets\n",
    "    ]\n",
    "    + [\n",
    "        f\"metamap-{umls_subset}-st21pv\" if st21pv else f\"metamap-{umls_subset}\"\n",
    "        for st21pv in [True, False]\n",
    "        for umls_subset in umls_subsets\n",
    "    ]\n",
    "    + [\n",
    "        f\"{contrastive_score}-{medical_corpus}-{n_gram_size}-{p_value}\"\n",
    "        if p_value >= 0\n",
    "        else f\"{contrastive_score}-{medical_corpus}-{n_gram_size}-neg_{-1 * p_value}\"\n",
    "        for contrastive_score in contrastive_scores\n",
    "        for n_gram_size in n_gram_sizes\n",
    "        for p_value in p_values\n",
    "        for medical_corpus in medical_corpora\n",
    "    ]\n",
    "    + [\n",
    "        f\"health_bert-{name}-{medical_corpus}-{text_type}\"\n",
    "        for name in bert_names\n",
    "        for medical_corpus in [\"pubmed\", \"encyclopedia\"]\n",
    "        for text_type in text_types\n",
    "    ]\n",
    ")\n",
    "df_labels = [\n",
    "    f\"medical_score-{relation}-{label}\"\n",
    "    for relation in [\"cause\", \"effect\"]\n",
    "    for label in labels\n",
    "]\n",
    "labels = [label.split(\"-\") for label in labels]\n",
    "new_columns = []\n",
    "for label in df_labels:\n",
    "    if label not in test_causenet_predictions:\n",
    "        new_columns.append(\n",
    "            pd.Series(np.nan, index=test_causenet_predictions.index, name=label)\n",
    "        )\n",
    "test_causenet_predictions = pd.concat([test_causenet_predictions, *new_columns], axis=1)\n",
    "\n",
    "missing_rows = test_causenet_predictions[df_labels].isna().all(1)\n",
    "if missing_rows.any():\n",
    "    print(f\"parsing {missing_rows.sum()} missing rows\")\n",
    "    pg = tqdm(labels)\n",
    "    for label in pg:\n",
    "        replace_rows = classify_test_set(\n",
    "            test_causenet_predictions.loc[missing_rows, [\"cause\", \"effect\"]], label,\n",
    "        )\n",
    "        label = \"-\".join(label)\n",
    "        columns = [f\"medical_score-cause-{label}\", f\"medical_score-effect-{label}\"]\n",
    "        replace_rows = replace_rows.loc[:, columns]\n",
    "        test_causenet_predictions.loc[missing_rows, columns] = replace_rows.values\n",
    "        test_causenet_predictions.to_pickle(\n",
    "            os.path.join(constants.CEPH_PATH, \"test_causenet_predictions.pkl\")\n",
    "        )\n",
    "\n",
    "score_column = test_causenet_predictions.columns.str.startswith(\"medical_score-\")\n",
    "isna = test_causenet_predictions.isna().any(0)\n",
    "missing_columns = list(test_causenet_predictions.loc[:, score_column & isna])\n",
    "missing_columns = [\n",
    "    \"-\".join(missing_column.split(\"-\")[2:]) for missing_column in missing_columns\n",
    "]\n",
    "missing_columns = list(set(missing_columns))\n",
    "missing_columns = sorted([missing_column.split(\"-\") for missing_column in missing_columns])\n",
    "\n",
    "if missing_columns:\n",
    "    print(\"parsing missing columns\")\n",
    "    for missing_column in missing_columns:\n",
    "        print(\" \".join(missing_column))\n",
    "    pg = tqdm(missing_columns)\n",
    "    for label in pg:\n",
    "        replace_columns = classify_test_set(\n",
    "            test_causenet_predictions.loc[:, [\"cause\", \"effect\"]], label,\n",
    "        )\n",
    "        label = \"-\".join(label)\n",
    "        columns = [f\"medical_score-cause-{label}\", f\"medical_score-effect-{label}\"]\n",
    "        replace_columns = replace_columns.loc[:, columns]\n",
    "        test_causenet_predictions.loc[:, columns] = replace_columns.values\n",
    "        assert not test_causenet_predictions.loc[:, columns].isna().any().any()\n",
    "        test_causenet_predictions.to_pickle(\n",
    "            os.path.join(constants.CEPH_PATH, \"test_causenet_predictions.pkl\")\n",
    "        )\n",
    "\n",
    "assert not test_causenet_predictions.loc[:, score_column].isna().any().any()\n",
    "test_causenet.merge(\n",
    "    test_causenet_predictions, on=[\"cause\", \"effect\", \"dataset\"], how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "081afecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing missing columns\n",
      "contrastive_weight encyclopedia (1, 1) 0\n",
      "contrastive_weight encyclopedia (1, 1) neg_1\n",
      "contrastive_weight encyclopedia (1, 1) neg_10\n",
      "contrastive_weight encyclopedia (1, 1) neg_2\n",
      "contrastive_weight encyclopedia (1, 1) neg_5\n",
      "contrastive_weight encyclopedia (1, 1) neg_inf\n",
      "contrastive_weight encyclopedia (1, 2) 0\n",
      "contrastive_weight encyclopedia (1, 2) neg_1\n",
      "contrastive_weight encyclopedia (1, 2) neg_10\n",
      "contrastive_weight encyclopedia (1, 2) neg_2\n",
      "contrastive_weight encyclopedia (1, 2) neg_5\n",
      "contrastive_weight encyclopedia (1, 2) neg_inf\n",
      "contrastive_weight encyclopedia (1, 3) 0\n",
      "contrastive_weight encyclopedia (1, 3) neg_1\n",
      "contrastive_weight encyclopedia (1, 3) neg_10\n",
      "contrastive_weight encyclopedia (1, 3) neg_2\n",
      "contrastive_weight encyclopedia (1, 3) neg_5\n",
      "contrastive_weight encyclopedia (1, 3) neg_inf\n",
      "contrastive_weight pubmed (1, 1) 0\n",
      "contrastive_weight pubmed (1, 1) neg_1\n",
      "contrastive_weight pubmed (1, 1) neg_10\n",
      "contrastive_weight pubmed (1, 1) neg_2\n",
      "contrastive_weight pubmed (1, 1) neg_5\n",
      "contrastive_weight pubmed (1, 1) neg_inf\n",
      "contrastive_weight pubmed (1, 2) 0\n",
      "contrastive_weight pubmed (1, 2) neg_1\n",
      "contrastive_weight pubmed (1, 2) neg_10\n",
      "contrastive_weight pubmed (1, 2) neg_2\n",
      "contrastive_weight pubmed (1, 2) neg_5\n",
      "contrastive_weight pubmed (1, 2) neg_inf\n",
      "contrastive_weight pubmed (1, 3) 0\n",
      "contrastive_weight pubmed (1, 3) neg_1\n",
      "contrastive_weight pubmed (1, 3) neg_10\n",
      "contrastive_weight pubmed (1, 3) neg_2\n",
      "contrastive_weight pubmed (1, 3) neg_5\n",
      "contrastive_weight pubmed (1, 3) neg_inf\n",
      "contrastive_weight pubmed_central (1, 1) 0\n",
      "contrastive_weight pubmed_central (1, 1) neg_1\n",
      "contrastive_weight pubmed_central (1, 1) neg_10\n",
      "contrastive_weight pubmed_central (1, 1) neg_2\n",
      "contrastive_weight pubmed_central (1, 1) neg_5\n",
      "contrastive_weight pubmed_central (1, 1) neg_inf\n",
      "contrastive_weight pubmed_central (1, 2) 0\n",
      "contrastive_weight pubmed_central (1, 2) neg_1\n",
      "contrastive_weight pubmed_central (1, 2) neg_10\n",
      "contrastive_weight pubmed_central (1, 2) neg_2\n",
      "contrastive_weight pubmed_central (1, 2) neg_5\n",
      "contrastive_weight pubmed_central (1, 2) neg_inf\n",
      "contrastive_weight pubmed_central (1, 3) 0\n",
      "contrastive_weight pubmed_central (1, 3) neg_1\n",
      "contrastive_weight pubmed_central (1, 3) neg_10\n",
      "contrastive_weight pubmed_central (1, 3) neg_2\n",
      "contrastive_weight pubmed_central (1, 3) neg_5\n",
      "contrastive_weight pubmed_central (1, 3) neg_inf\n",
      "contrastive_weight textbook (1, 1) 0\n",
      "contrastive_weight textbook (1, 1) neg_1\n",
      "contrastive_weight textbook (1, 1) neg_10\n",
      "contrastive_weight textbook (1, 1) neg_2\n",
      "contrastive_weight textbook (1, 1) neg_5\n",
      "contrastive_weight textbook (1, 1) neg_inf\n",
      "contrastive_weight textbook (1, 2) 0\n",
      "contrastive_weight textbook (1, 2) neg_1\n",
      "contrastive_weight textbook (1, 2) neg_10\n",
      "contrastive_weight textbook (1, 2) neg_2\n",
      "contrastive_weight textbook (1, 2) neg_5\n",
      "contrastive_weight textbook (1, 2) neg_inf\n",
      "contrastive_weight textbook (1, 3) 0\n",
      "contrastive_weight textbook (1, 3) neg_1\n",
      "contrastive_weight textbook (1, 3) neg_10\n",
      "contrastive_weight textbook (1, 3) neg_2\n",
      "contrastive_weight textbook (1, 3) neg_5\n",
      "contrastive_weight textbook (1, 3) neg_inf\n",
      "discriminative_weight encyclopedia (1, 1) 0\n",
      "discriminative_weight encyclopedia (1, 1) neg_1\n",
      "discriminative_weight encyclopedia (1, 1) neg_10\n",
      "discriminative_weight encyclopedia (1, 1) neg_2\n",
      "discriminative_weight encyclopedia (1, 1) neg_5\n",
      "discriminative_weight encyclopedia (1, 1) neg_inf\n",
      "discriminative_weight encyclopedia (1, 2) 0\n",
      "discriminative_weight encyclopedia (1, 2) neg_1\n",
      "discriminative_weight encyclopedia (1, 2) neg_10\n",
      "discriminative_weight encyclopedia (1, 2) neg_2\n",
      "discriminative_weight encyclopedia (1, 2) neg_5\n",
      "discriminative_weight encyclopedia (1, 2) neg_inf\n",
      "discriminative_weight encyclopedia (1, 3) 0\n",
      "discriminative_weight encyclopedia (1, 3) neg_1\n",
      "discriminative_weight encyclopedia (1, 3) neg_10\n",
      "discriminative_weight encyclopedia (1, 3) neg_2\n",
      "discriminative_weight encyclopedia (1, 3) neg_5\n",
      "discriminative_weight encyclopedia (1, 3) neg_inf\n",
      "discriminative_weight pubmed (1, 1) 0\n",
      "discriminative_weight pubmed (1, 1) neg_1\n",
      "discriminative_weight pubmed (1, 1) neg_10\n",
      "discriminative_weight pubmed (1, 1) neg_2\n",
      "discriminative_weight pubmed (1, 1) neg_5\n",
      "discriminative_weight pubmed (1, 1) neg_inf\n",
      "discriminative_weight pubmed (1, 2) 0\n",
      "discriminative_weight pubmed (1, 2) neg_1\n",
      "discriminative_weight pubmed (1, 2) neg_10\n",
      "discriminative_weight pubmed (1, 2) neg_2\n",
      "discriminative_weight pubmed (1, 2) neg_5\n",
      "discriminative_weight pubmed (1, 2) neg_inf\n",
      "discriminative_weight pubmed (1, 3) 0\n",
      "discriminative_weight pubmed (1, 3) neg_1\n",
      "discriminative_weight pubmed (1, 3) neg_10\n",
      "discriminative_weight pubmed (1, 3) neg_2\n",
      "discriminative_weight pubmed (1, 3) neg_5\n",
      "discriminative_weight pubmed (1, 3) neg_inf\n",
      "discriminative_weight pubmed_central (1, 1) 0\n",
      "discriminative_weight pubmed_central (1, 1) neg_1\n",
      "discriminative_weight pubmed_central (1, 1) neg_10\n",
      "discriminative_weight pubmed_central (1, 1) neg_2\n",
      "discriminative_weight pubmed_central (1, 1) neg_5\n",
      "discriminative_weight pubmed_central (1, 1) neg_inf\n",
      "discriminative_weight pubmed_central (1, 2) 0\n",
      "discriminative_weight pubmed_central (1, 2) neg_1\n",
      "discriminative_weight pubmed_central (1, 2) neg_10\n",
      "discriminative_weight pubmed_central (1, 2) neg_2\n",
      "discriminative_weight pubmed_central (1, 2) neg_5\n",
      "discriminative_weight pubmed_central (1, 2) neg_inf\n",
      "discriminative_weight pubmed_central (1, 3) 0\n",
      "discriminative_weight pubmed_central (1, 3) neg_1\n",
      "discriminative_weight pubmed_central (1, 3) neg_10\n",
      "discriminative_weight pubmed_central (1, 3) neg_2\n",
      "discriminative_weight pubmed_central (1, 3) neg_5\n",
      "discriminative_weight pubmed_central (1, 3) neg_inf\n",
      "discriminative_weight textbook (1, 1) 0\n",
      "discriminative_weight textbook (1, 1) neg_1\n",
      "discriminative_weight textbook (1, 1) neg_10\n",
      "discriminative_weight textbook (1, 1) neg_2\n",
      "discriminative_weight textbook (1, 1) neg_5\n",
      "discriminative_weight textbook (1, 1) neg_inf\n",
      "discriminative_weight textbook (1, 2) 0\n",
      "discriminative_weight textbook (1, 2) neg_1\n",
      "discriminative_weight textbook (1, 2) neg_10\n",
      "discriminative_weight textbook (1, 2) neg_2\n",
      "discriminative_weight textbook (1, 2) neg_5\n",
      "discriminative_weight textbook (1, 2) neg_inf\n",
      "discriminative_weight textbook (1, 3) 0\n",
      "discriminative_weight textbook (1, 3) neg_1\n",
      "discriminative_weight textbook (1, 3) neg_10\n",
      "discriminative_weight textbook (1, 3) neg_2\n",
      "discriminative_weight textbook (1, 3) neg_5\n",
      "discriminative_weight textbook (1, 3) neg_inf\n",
      "health_bert bert encyclopedia noun_phrase\n",
      "health_bert bert encyclopedia sentence\n",
      "health_bert bert pubmed noun_phrase\n",
      "health_bert bert pubmed sentence\n",
      "health_bert pubmedbert encyclopedia noun_phrase\n",
      "health_bert pubmedbert encyclopedia sentence\n",
      "health_bert pubmedbert pubmed noun_phrase\n",
      "health_bert pubmedbert pubmed sentence\n",
      "health_bert scibert encyclopedia noun_phrase\n",
      "health_bert scibert encyclopedia sentence\n",
      "health_bert scibert pubmed noun_phrase\n",
      "health_bert scibert pubmed sentence\n",
      "term_domain_specificity encyclopedia (1, 1) 0\n",
      "term_domain_specificity encyclopedia (1, 1) neg_1\n",
      "term_domain_specificity encyclopedia (1, 1) neg_10\n",
      "term_domain_specificity encyclopedia (1, 1) neg_2\n",
      "term_domain_specificity encyclopedia (1, 1) neg_5\n",
      "term_domain_specificity encyclopedia (1, 1) neg_inf\n",
      "term_domain_specificity encyclopedia (1, 2) 0\n",
      "term_domain_specificity encyclopedia (1, 2) neg_1\n",
      "term_domain_specificity encyclopedia (1, 2) neg_10\n",
      "term_domain_specificity encyclopedia (1, 2) neg_2\n",
      "term_domain_specificity encyclopedia (1, 2) neg_5\n",
      "term_domain_specificity encyclopedia (1, 2) neg_inf\n",
      "term_domain_specificity encyclopedia (1, 3) 0\n",
      "term_domain_specificity encyclopedia (1, 3) neg_1\n",
      "term_domain_specificity encyclopedia (1, 3) neg_10\n",
      "term_domain_specificity encyclopedia (1, 3) neg_2\n",
      "term_domain_specificity encyclopedia (1, 3) neg_5\n",
      "term_domain_specificity encyclopedia (1, 3) neg_inf\n",
      "term_domain_specificity pubmed (1, 1) 0\n",
      "term_domain_specificity pubmed (1, 1) neg_1\n",
      "term_domain_specificity pubmed (1, 1) neg_10\n",
      "term_domain_specificity pubmed (1, 1) neg_2\n",
      "term_domain_specificity pubmed (1, 1) neg_5\n",
      "term_domain_specificity pubmed (1, 1) neg_inf\n",
      "term_domain_specificity pubmed (1, 2) 0\n",
      "term_domain_specificity pubmed (1, 2) neg_1\n",
      "term_domain_specificity pubmed (1, 2) neg_10\n",
      "term_domain_specificity pubmed (1, 2) neg_2\n",
      "term_domain_specificity pubmed (1, 2) neg_5\n",
      "term_domain_specificity pubmed (1, 2) neg_inf\n",
      "term_domain_specificity pubmed (1, 3) 0\n",
      "term_domain_specificity pubmed (1, 3) neg_1\n",
      "term_domain_specificity pubmed (1, 3) neg_10\n",
      "term_domain_specificity pubmed (1, 3) neg_2\n",
      "term_domain_specificity pubmed (1, 3) neg_5\n",
      "term_domain_specificity pubmed (1, 3) neg_inf\n",
      "term_domain_specificity pubmed_central (1, 1) 0\n",
      "term_domain_specificity pubmed_central (1, 1) neg_1\n",
      "term_domain_specificity pubmed_central (1, 1) neg_10\n",
      "term_domain_specificity pubmed_central (1, 1) neg_2\n",
      "term_domain_specificity pubmed_central (1, 1) neg_5\n",
      "term_domain_specificity pubmed_central (1, 1) neg_inf\n",
      "term_domain_specificity pubmed_central (1, 2) 0\n",
      "term_domain_specificity pubmed_central (1, 2) neg_1\n",
      "term_domain_specificity pubmed_central (1, 2) neg_10\n",
      "term_domain_specificity pubmed_central (1, 2) neg_2\n",
      "term_domain_specificity pubmed_central (1, 2) neg_5\n",
      "term_domain_specificity pubmed_central (1, 2) neg_inf\n",
      "term_domain_specificity pubmed_central (1, 3) 0\n",
      "term_domain_specificity pubmed_central (1, 3) neg_1\n",
      "term_domain_specificity pubmed_central (1, 3) neg_10\n",
      "term_domain_specificity pubmed_central (1, 3) neg_2\n",
      "term_domain_specificity pubmed_central (1, 3) neg_5\n",
      "term_domain_specificity pubmed_central (1, 3) neg_inf\n",
      "term_domain_specificity textbook (1, 1) 0\n",
      "term_domain_specificity textbook (1, 1) neg_1\n",
      "term_domain_specificity textbook (1, 1) neg_10\n",
      "term_domain_specificity textbook (1, 1) neg_2\n",
      "term_domain_specificity textbook (1, 1) neg_5\n",
      "term_domain_specificity textbook (1, 1) neg_inf\n",
      "term_domain_specificity textbook (1, 2) 0\n",
      "term_domain_specificity textbook (1, 2) neg_1\n",
      "term_domain_specificity textbook (1, 2) neg_10\n",
      "term_domain_specificity textbook (1, 2) neg_2\n",
      "term_domain_specificity textbook (1, 2) neg_5\n",
      "term_domain_specificity textbook (1, 2) neg_inf\n",
      "term_domain_specificity textbook (1, 3) 0\n",
      "term_domain_specificity textbook (1, 3) neg_1\n",
      "term_domain_specificity textbook (1, 3) neg_10\n",
      "term_domain_specificity textbook (1, 3) neg_2\n",
      "term_domain_specificity textbook (1, 3) neg_5\n",
      "term_domain_specificity textbook (1, 3) neg_inf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d357427eec8444b7b55404c3f99250aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cause</th>\n",
       "      <th>effect</th>\n",
       "      <th>support</th>\n",
       "      <th>count</th>\n",
       "      <th>dataset</th>\n",
       "      <th>cause_origin</th>\n",
       "      <th>effect_origin</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>sentence</th>\n",
       "      <th>manual_evaluation</th>\n",
       "      <th>...</th>\n",
       "      <th>medical_score-effect-health_bert-bert-encyclopedia-sentence</th>\n",
       "      <th>medical_score-effect-health_bert-bert-encyclopedia-noun_phrase</th>\n",
       "      <th>medical_score-effect-health_bert-scibert-pubmed-sentence</th>\n",
       "      <th>medical_score-effect-health_bert-scibert-pubmed-noun_phrase</th>\n",
       "      <th>medical_score-effect-health_bert-scibert-encyclopedia-sentence</th>\n",
       "      <th>medical_score-effect-health_bert-scibert-encyclopedia-noun_phrase</th>\n",
       "      <th>medical_score-effect-health_bert-pubmedbert-pubmed-sentence</th>\n",
       "      <th>medical_score-effect-health_bert-pubmedbert-pubmed-noun_phrase</th>\n",
       "      <th>medical_score-effect-health_bert-pubmedbert-encyclopedia-sentence</th>\n",
       "      <th>medical_score-effect-health_bert-pubmedbert-encyclopedia-noun_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accident</td>\n",
       "      <td>death</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1458.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>For example, the book also described how to di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pneumonia</td>\n",
       "      <td>death</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Pneumonia was a serious cause of death in the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disease</td>\n",
       "      <td>death</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a stage in some diseases before the sy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>illness</td>\n",
       "      <td>death</td>\n",
       "      <td>36.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>A primary cause of death was illness due to ha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heart attack</td>\n",
       "      <td>death</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>support</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>His death was announced to Lok Sabha at 14:00 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>termination of the company 's pexelizumab prog...</td>\n",
       "      <td>non-recurrence of costs</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>random_full</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>The decrease in R&amp;D expenses in 2007 reflected...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>medical neglect of a patient at another nursin...</td>\n",
       "      <td>abuse</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>random_full</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>That dispute arose when HRS found indications ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>disorder</td>\n",
       "      <td>shoulder contractions</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>random_full</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a disorder that causes severe neck and...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>car accidents</td>\n",
       "      <td>rib fracture in a child</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>random_full</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>A rib fracture in a child is mainly caused by ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>internet marketing breach</td>\n",
       "      <td>refund of its hitch database</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>random_full</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Also in september 2006, opposite loreal sublim...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 938 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  cause  \\\n",
       "0                                              accident   \n",
       "1                                             pneumonia   \n",
       "2                                               disease   \n",
       "3                                               illness   \n",
       "4                                          heart attack   \n",
       "...                                                 ...   \n",
       "2995  termination of the company 's pexelizumab prog...   \n",
       "2996  medical neglect of a patient at another nursin...   \n",
       "2997                                           disorder   \n",
       "2998                                      car accidents   \n",
       "2999                          internet marketing breach   \n",
       "\n",
       "                            effect  support   count      dataset cause_origin  \\\n",
       "0                            death     38.0  1458.0      support          NaN   \n",
       "1                            death     37.0  1185.0      support          NaN   \n",
       "2                            death     37.0  1344.0      support          NaN   \n",
       "3                            death     36.0   571.0      support          NaN   \n",
       "4                            death     36.0  1005.0      support          NaN   \n",
       "...                            ...      ...     ...          ...          ...   \n",
       "2995       non-recurrence of costs      1.0     3.0  random_full          NaN   \n",
       "2996                         abuse      1.0     2.0  random_full          NaN   \n",
       "2997         shoulder contractions      1.0     8.0  random_full          NaN   \n",
       "2998       rib fracture in a child      1.0     1.0  random_full          NaN   \n",
       "2999  refund of its hitch database      1.0     1.0  random_full          NaN   \n",
       "\n",
       "     effect_origin  evaluation  \\\n",
       "0              NaN           0   \n",
       "1              NaN           1   \n",
       "2              NaN           1   \n",
       "3              NaN           1   \n",
       "4              NaN           1   \n",
       "...            ...         ...   \n",
       "2995           NaN           0   \n",
       "2996           NaN           0   \n",
       "2997           NaN           1   \n",
       "2998           NaN           1   \n",
       "2999           NaN           0   \n",
       "\n",
       "                                               sentence  manual_evaluation  \\\n",
       "0     For example, the book also described how to di...                NaN   \n",
       "1     Pneumonia was a serious cause of death in the ...                NaN   \n",
       "2     This is a stage in some diseases before the sy...                NaN   \n",
       "3     A primary cause of death was illness due to ha...                NaN   \n",
       "4     His death was announced to Lok Sabha at 14:00 ...                NaN   \n",
       "...                                                 ...                ...   \n",
       "2995  The decrease in R&D expenses in 2007 reflected...                0.0   \n",
       "2996  That dispute arose when HRS found indications ...                0.0   \n",
       "2997  This is a disorder that causes severe neck and...                1.0   \n",
       "2998  A rib fracture in a child is mainly caused by ...                1.0   \n",
       "2999  Also in september 2006, opposite loreal sublim...                0.0   \n",
       "\n",
       "      ...  medical_score-effect-health_bert-bert-encyclopedia-sentence  \\\n",
       "0     ...                                                  0             \n",
       "1     ...                                                  0             \n",
       "2     ...                                                  0             \n",
       "3     ...                                                  0             \n",
       "4     ...                                                  0             \n",
       "...   ...                                                ...             \n",
       "2995  ...                                                  0             \n",
       "2996  ...                                                  0             \n",
       "2997  ...                                                  0             \n",
       "2998  ...                                                  0             \n",
       "2999  ...                                                  0             \n",
       "\n",
       "      medical_score-effect-health_bert-bert-encyclopedia-noun_phrase  \\\n",
       "0                                                     0                \n",
       "1                                                     0                \n",
       "2                                                     0                \n",
       "3                                                     0                \n",
       "4                                                     0                \n",
       "...                                                 ...                \n",
       "2995                                                  0                \n",
       "2996                                                  0                \n",
       "2997                                                  0                \n",
       "2998                                                  0                \n",
       "2999                                                  0                \n",
       "\n",
       "      medical_score-effect-health_bert-scibert-pubmed-sentence  \\\n",
       "0                                                     0          \n",
       "1                                                     0          \n",
       "2                                                     0          \n",
       "3                                                     0          \n",
       "4                                                     0          \n",
       "...                                                 ...          \n",
       "2995                                                  0          \n",
       "2996                                                  0          \n",
       "2997                                                  0          \n",
       "2998                                                  0          \n",
       "2999                                                  0          \n",
       "\n",
       "      medical_score-effect-health_bert-scibert-pubmed-noun_phrase  \\\n",
       "0                                                     0             \n",
       "1                                                     0             \n",
       "2                                                     0             \n",
       "3                                                     0             \n",
       "4                                                     0             \n",
       "...                                                 ...             \n",
       "2995                                                  0             \n",
       "2996                                                  0             \n",
       "2997                                                  0             \n",
       "2998                                                  0             \n",
       "2999                                                  0             \n",
       "\n",
       "      medical_score-effect-health_bert-scibert-encyclopedia-sentence  \\\n",
       "0                                                     0                \n",
       "1                                                     0                \n",
       "2                                                     0                \n",
       "3                                                     0                \n",
       "4                                                     0                \n",
       "...                                                 ...                \n",
       "2995                                                  0                \n",
       "2996                                                  0                \n",
       "2997                                                  0                \n",
       "2998                                                  0                \n",
       "2999                                                  0                \n",
       "\n",
       "      medical_score-effect-health_bert-scibert-encyclopedia-noun_phrase  \\\n",
       "0                                                     0                   \n",
       "1                                                     0                   \n",
       "2                                                     0                   \n",
       "3                                                     0                   \n",
       "4                                                     0                   \n",
       "...                                                 ...                   \n",
       "2995                                                  0                   \n",
       "2996                                                  0                   \n",
       "2997                                                  0                   \n",
       "2998                                                  0                   \n",
       "2999                                                  0                   \n",
       "\n",
       "      medical_score-effect-health_bert-pubmedbert-pubmed-sentence  \\\n",
       "0                                                     0             \n",
       "1                                                     0             \n",
       "2                                                     0             \n",
       "3                                                     0             \n",
       "4                                                     0             \n",
       "...                                                 ...             \n",
       "2995                                                  0             \n",
       "2996                                                  0             \n",
       "2997                                                  0             \n",
       "2998                                                  0             \n",
       "2999                                                  0             \n",
       "\n",
       "      medical_score-effect-health_bert-pubmedbert-pubmed-noun_phrase  \\\n",
       "0                                                     0                \n",
       "1                                                     0                \n",
       "2                                                     0                \n",
       "3                                                     0                \n",
       "4                                                     0                \n",
       "...                                                 ...                \n",
       "2995                                                  0                \n",
       "2996                                                  0                \n",
       "2997                                                  0                \n",
       "2998                                                  0                \n",
       "2999                                                  0                \n",
       "\n",
       "      medical_score-effect-health_bert-pubmedbert-encyclopedia-sentence  \\\n",
       "0                                                     0                   \n",
       "1                                                     0                   \n",
       "2                                                     0                   \n",
       "3                                                     0                   \n",
       "4                                                     0                   \n",
       "...                                                 ...                   \n",
       "2995                                                  0                   \n",
       "2996                                                  0                   \n",
       "2997                                                  0                   \n",
       "2998                                                  0                   \n",
       "2999                                                  0                   \n",
       "\n",
       "      medical_score-effect-health_bert-pubmedbert-encyclopedia-noun_phrase  \n",
       "0                                                     0                     \n",
       "1                                                     0                     \n",
       "2                                                     0                     \n",
       "3                                                     0                     \n",
       "4                                                     0                     \n",
       "...                                                 ...                     \n",
       "2995                                                  0                     \n",
       "2996                                                  0                     \n",
       "2997                                                  0                     \n",
       "2998                                                  0                     \n",
       "2999                                                  0                     \n",
       "\n",
       "[3000 rows x 938 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    sentence_test_causenet_predictions = pd.read_pickle(\n",
    "        os.path.join(constants.CEPH_PATH, \"sentence_test_causenet_predictions.pkl\")\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    sentence_test_causenet_predictions = sentence_test_causenet.loc[\n",
    "        :, [\"cause\", \"effect\", \"dataset\", \"sentence\"]\n",
    "    ]\n",
    "\n",
    "sentence_test_causenet_predictions = sentence_test_causenet_predictions.merge(\n",
    "    sentence_test_causenet.drop(\n",
    "        [\n",
    "            \"support\",\n",
    "            \"count\",\n",
    "            \"cause_origin\",\n",
    "            \"effect_origin\",\n",
    "            \"evaluation\",\n",
    "            \"manual_evaluation\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    ),\n",
    "    on=[\"cause\", \"effect\", \"sentence\", \"dataset\"],\n",
    "    how=\"outer\",\n",
    ")\n",
    "\n",
    "jaccard_thresholds = [round(thresh, 2) for thresh in np.arange(0.7, 1.01, 0.1)]\n",
    "scispacy_thresholds = [round(thresh, 2) for thresh in np.arange(0.6, 0.9, 0.1)]\n",
    "scispacy_models = [\"en_core_sci_sm\", \"en_core_sci_lg\"]\n",
    "p_values = [-float(\"inf\"), -10, -5, -2, -1, 0, 1, 2, 5, 10, float(\"inf\")]\n",
    "n_gram_sizes = [(1, 1), (1, 2), (1, 3)]\n",
    "medical_corpora = [\n",
    "    \"pubmed\",\n",
    "    \"textbook\",\n",
    "    \"pubmed_central\",\n",
    "    \"encyclopedia\",\n",
    "]\n",
    "umls_subsets = [\n",
    "    \"full\",\n",
    "    \"rx_sno\",\n",
    "]\n",
    "text_types = [\"sentence\", \"noun_phrase\"]\n",
    "contrastive_scores = [\n",
    "    \"term_domain_specificity\",\n",
    "    \"contrastive_weight\",\n",
    "    \"discriminative_weight\",\n",
    "]\n",
    "bert_names = [\"bert\", \"scibert\", \"pubmedbert\"]\n",
    "\n",
    "labels = (\n",
    "    [\n",
    "        f\"quickumls-{umls_subset}-{jaccard_threshold}-st21pv\"\n",
    "        if st21pv\n",
    "        else f\"quickumls-{umls_subset}-{jaccard_threshold}\"\n",
    "        for st21pv in [True, False]\n",
    "        for umls_subset in umls_subsets\n",
    "        for jaccard_threshold in jaccard_thresholds\n",
    "    ]\n",
    "    + [\n",
    "        f\"scispacy-{umls_subset}-{model}-{threshold}-st21pv\"\n",
    "        if st21pv\n",
    "        else f\"scispacy-{umls_subset}-{model}-{threshold}\"\n",
    "        for st21pv in [True, False]\n",
    "        for model in scispacy_models\n",
    "        for umls_subset in umls_subsets\n",
    "        for threshold in scispacy_thresholds\n",
    "    ]\n",
    "    + [\n",
    "        f\"ctakes-{umls_subset}-st21pv\" if st21pv else f\"ctakes-{umls_subset}\"\n",
    "        for st21pv in [True, False]\n",
    "        for umls_subset in umls_subsets\n",
    "    ]\n",
    "    + [\n",
    "        f\"metamap-{umls_subset}-st21pv\" if st21pv else f\"metamap-{umls_subset}\"\n",
    "        for st21pv in [True, False]\n",
    "        for umls_subset in umls_subsets\n",
    "    ]\n",
    "    + [\n",
    "        f\"{contrastive_score}-{medical_corpus}-{n_gram_size}-{p_value}\"\n",
    "        if p_value >= 0\n",
    "        else f\"{contrastive_score}-{medical_corpus}-{n_gram_size}-neg_{-1 * p_value}\"\n",
    "        for contrastive_score in contrastive_scores\n",
    "        for n_gram_size in n_gram_sizes\n",
    "        for p_value in p_values\n",
    "        for medical_corpus in medical_corpora\n",
    "    ]\n",
    "    + [\n",
    "        f\"health_bert-{name}-{medical_corpus}-{text_type}\"\n",
    "        for name in bert_names\n",
    "        for medical_corpus in [\"pubmed\", \"encyclopedia\"]\n",
    "        for text_type in text_types\n",
    "    ]\n",
    ")\n",
    "df_labels = [\n",
    "    f\"medical_score-{relation}-{label}\"\n",
    "    for relation in [\"cause\", \"effect\"]\n",
    "    for label in labels\n",
    "]\n",
    "labels = [label.split(\"-\") for label in labels]\n",
    "new_columns = []\n",
    "for label in df_labels:\n",
    "    if label not in sentence_test_causenet_predictions:\n",
    "        new_columns.append(\n",
    "            pd.Series(\n",
    "                np.nan, index=sentence_test_causenet_predictions.index, name=label\n",
    "            )\n",
    "        )\n",
    "sentence_test_causenet_predictions = pd.concat(\n",
    "    [sentence_test_causenet_predictions, *new_columns], axis=1\n",
    ")\n",
    "\n",
    "prediction_sentence_test_causenet = sentence_test_causenet_predictions.copy()\n",
    "prediction_sentence_test_causenet.cause = prediction_sentence_test_causenet.sentence\n",
    "prediction_sentence_test_causenet.effect = \"\"\n",
    "prediction_sentence_test_causenet = prediction_sentence_test_causenet.drop(\n",
    "    [\"sentence\",], axis=1,\n",
    ")\n",
    "\n",
    "missing_rows = sentence_test_causenet_predictions[df_labels].isna().all(1)\n",
    "if missing_rows.any():\n",
    "    print(f\"parsing {missing_rows.sum()} missing rows\")\n",
    "    pg = tqdm(labels)\n",
    "    for label in pg:\n",
    "        replace_rows = classify_test_set(\n",
    "            prediction_sentence_test_causenet.loc[missing_rows, [\"cause\", \"effect\"]],\n",
    "            label,\n",
    "        )\n",
    "        label = \"-\".join(label)\n",
    "        columns = [f\"medical_score-cause-{label}\", f\"medical_score-effect-{label}\"]\n",
    "        replace_rows = replace_rows.loc[:, columns]\n",
    "        sentence_test_causenet_predictions.loc[\n",
    "            missing_rows, columns\n",
    "        ] = replace_rows.values\n",
    "        \n",
    "        effect_columns = sentence_test_causenet_predictions.columns[\n",
    "            sentence_test_causenet_predictions.columns.str.startswith(\"medical_score-effect\")\n",
    "        ]\n",
    "        sentence_test_causenet_predictions.loc[:, effect_columns] = 0\n",
    "        sentence_test_causenet_predictions.to_pickle(\n",
    "            os.path.join(constants.CEPH_PATH, \"sentence_test_causenet_predictions.pkl\")\n",
    "        )\n",
    "         \n",
    "score_column = sentence_test_causenet_predictions.columns.str.startswith(\n",
    "    \"medical_score-\"\n",
    ")\n",
    "isna = sentence_test_causenet_predictions.isna().any(0)\n",
    "missing_columns = list(sentence_test_causenet_predictions.loc[:, score_column & isna])\n",
    "missing_columns = [\"-\".join(missing_column.split(\"-\")[2:]) for missing_column in missing_columns]\n",
    "missing_columns = list(set(missing_columns))\n",
    "missing_columns = sorted([missing_column.split(\"-\") for missing_column in missing_columns])\n",
    "\n",
    "if missing_columns:\n",
    "    print(\"parsing missing columns\")\n",
    "    for missing_column in missing_columns:\n",
    "        print(\" \".join(missing_column))\n",
    "    pg = tqdm(missing_columns)\n",
    "    for label in pg:\n",
    "        replace_columns = classify_test_set(\n",
    "            prediction_sentence_test_causenet.loc[:, [\"cause\", \"effect\"]], label,\n",
    "        )\n",
    "        label = \"-\".join(label)\n",
    "        columns = [f\"medical_score-cause-{label}\", f\"medical_score-effect-{label}\"]\n",
    "        replace_columns = replace_columns.loc[:, columns]\n",
    "        sentence_test_causenet_predictions.loc[:, columns] = replace_columns.values\n",
    "        assert not sentence_test_causenet_predictions.loc[:, columns].isna().any().any()\n",
    "\n",
    "        effect_columns = sentence_test_causenet_predictions.columns[\n",
    "            sentence_test_causenet_predictions.columns.str.startswith(\"medical_score-effect\")\n",
    "        ]\n",
    "        sentence_test_causenet_predictions.loc[:, effect_columns] = 0\n",
    "        sentence_test_causenet_predictions.to_pickle(\n",
    "            os.path.join(constants.CEPH_PATH, \"sentence_test_causenet_predictions.pkl\")\n",
    "        )\n",
    "        \n",
    "assert not sentence_test_causenet_predictions.loc[:, score_column].isna().any().any()\n",
    "\n",
    "sentence_test_causenet.merge(\n",
    "    sentence_test_causenet_predictions,\n",
    "    on=[\"cause\", \"effect\", \"sentence\", \"dataset\"],\n",
    "    how=\"left\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
