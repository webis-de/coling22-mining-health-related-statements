{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca3a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_path = \"..\"\n",
    "sys.path.append(os.path.abspath(parent_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b8817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from health_causenet import constants\n",
    "import json\n",
    "import pathlib\n",
    "from tqdm.autonotebook import tqdm\n",
    "import operator\n",
    "from nltk import agreement\n",
    "import nltk\n",
    "import sklearn.metrics\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedaa4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_mean_threshold_combiner(cause, effect, p):\n",
    "    if p == 0:\n",
    "        return np.sqrt(cause * effect)\n",
    "    if p == float(\"inf\"):\n",
    "        return np.maximum(cause, effect)\n",
    "    if p == -float(\"inf\"):\n",
    "        return np.minimum(cause, effect)\n",
    "    return ((cause ** p + effect ** p) / 2) ** (1 / p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288046ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_causenet = pd.read_pickle(os.path.join(constants.TEST_CAUSENET_PATH, \"test_causenet.pkl\"))\n",
    "test_causenet_predictions = pd.read_pickle(\n",
    "    os.path.join(constants.TEST_CAUSENET_PATH, \"test_causenet_predictions.pkl\")\n",
    ")\n",
    "test_causenet = test_causenet.merge(\n",
    "    test_causenet_predictions, on=[\"cause\", \"effect\", \"dataset\"], how=\"left\"\n",
    ")\n",
    "\n",
    "sentence_test_causenet = pd.read_pickle(\n",
    "    os.path.join(constants.TEST_CAUSENET_PATH, \"sentence_test_causenet.pkl\")\n",
    ")\n",
    "sentence_test_causenet_predictions = pd.read_pickle(\n",
    "    os.path.join(constants.TEST_CAUSENET_PATH, \"sentence_test_causenet_predictions.pkl\")\n",
    ")\n",
    "sentence_test_causenet = sentence_test_causenet.merge(\n",
    "    sentence_test_causenet_predictions,\n",
    "    on=[\"cause\", \"effect\", \"dataset\", \"sentence\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "assert (\n",
    "    not test_causenet.loc[:, test_causenet.columns.str.startswith(\"medical_score\")]\n",
    "    .isna()\n",
    "    .any()\n",
    "    .any()\n",
    ")\n",
    "assert (\n",
    "    not sentence_test_causenet.loc[\n",
    "        :, sentence_test_causenet.columns.str.startswith(\"medical_score\")\n",
    "    ]\n",
    "    .isna()\n",
    "    .any()\n",
    "    .any()\n",
    ")\n",
    "\n",
    "test_causenet = test_causenet.loc[test_causenet.dataset.isin([\"random_full\", \"random_support\"])]\n",
    "sentence_test_causenet = sentence_test_causenet.loc[sentence_test_causenet.dataset == \"random_full\"]\n",
    "sentence_test_causenet[\"dataset\"] = \"sentence\"\n",
    "test_causenet = pd.concat([test_causenet, sentence_test_causenet]).reset_index(drop=True)\n",
    "test_causenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f79f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_causenet.groupby(\"dataset\").evaluation.agg([\"sum\", \"mean\", \"size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = (\n",
    "    test_causenet.apply(\n",
    "        lambda x: pd.Series(x[[\"cause\", \"effect\"]].values.ravel()).map(\n",
    "            lambda y: len(nltk.tokenize.word_tokenize(y))\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    .sum(axis=1)\n",
    "    .groupby(test_causenet.dataset)\n",
    "    .mean()\n",
    "    / 2\n",
    ")\n",
    "print(\"phrases\")\n",
    "for dataset, word_count in num_words.round(2).to_dict().items():\n",
    "    print(dataset, word_count)\n",
    "print(\"-------------------\")\n",
    "print(\"sentences\")\n",
    "num_words = (\n",
    "    sentence_test_causenet.apply(\n",
    "        lambda x: pd.Series(x[[\"sentence\"]].values.ravel()).map(\n",
    "            lambda y: len(nltk.tokenize.word_tokenize(y))\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    .sum(axis=1)\n",
    "    .groupby(sentence_test_causenet.dataset)\n",
    "    .mean()\n",
    ")\n",
    "for dataset, word_count in num_words.round(2).to_dict().items():\n",
    "    print(dataset, word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(constants.MANUAL_EVALUATION_PATH, \"r\") as file:\n",
    "    manual_eval_dict = json.load(file)\n",
    "data = []\n",
    "for key, value in manual_eval_dict.items():\n",
    "    cause, effect = key.split(\"->\")\n",
    "    data.append({\"cause\": cause, \"effect\": effect, \"evaluation\": value})\n",
    "evals = pd.DataFrame(data).rename({\"evaluation\": \"rater\"}, axis=1)\n",
    "evals = evals.set_index([\"cause\", \"effect\"])\n",
    "evals = evals.add_suffix(\"-0\")\n",
    "\n",
    "iterator = enumerate(pathlib.Path(constants.BASE_PATH).glob(\"agreement_relations*.csv\"))\n",
    "\n",
    "for idx, path in iterator:\n",
    "    other_eval = pd.read_csv(path, index_col=0).set_index([\"cause\", \"effect\"])\n",
    "    other_eval = other_eval.rename({\"health-related\": \"rater\"}, axis=1)\n",
    "    other_eval = other_eval.add_suffix(f\"-{idx + 1}\")\n",
    "    evals = evals.join(other_eval, how=\"inner\")\n",
    "\n",
    "rating_data = evals.reset_index(drop=True).stack().swaplevel().reset_index().values\n",
    "rating_data = list(tuple(row) for row in rating_data)\n",
    "\n",
    "rating_task = agreement.AnnotationTask(data=rating_data)\n",
    "print(\"kappa \" + str(rating_task.kappa()))\n",
    "print(\"fleiss \" + str(rating_task.multi_kappa()))\n",
    "print(\"alpha \" + str(rating_task.alpha()))\n",
    "print(\"scotts \" + str(rating_task.pi()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78813e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = sentence_test_causenet.set_index([\"cause\", \"effect\"]).dropna(\n",
    "    subset=[\"manual_evaluation\"]\n",
    ")\n",
    "evals = evals.loc[:, [\"evaluation\", \"manual_evaluation\"]].astype(int)\n",
    "rating_data = evals.reset_index(drop=True).stack().swaplevel().reset_index().values\n",
    "rating_data = list(tuple(row) for row in rating_data)\n",
    "\n",
    "rating_task = agreement.AnnotationTask(data=rating_data)\n",
    "print(\"kappa \" + str(rating_task.kappa()))\n",
    "print(\"fleiss \" + str(rating_task.multi_kappa()))\n",
    "print(\"alpha \" + str(rating_task.alpha()))\n",
    "print(\"scotts \" + str(rating_task.pi()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c020963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((evals.loc[evals[\"evaluation\"] != evals[\"manual_evaluation\"]]).sum())\n",
    "evals.loc[evals[\"evaluation\"] != evals[\"manual_evaluation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88378af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_test_causenet_combined(test_causenet, ops):\n",
    "    test_causenet_combined = []\n",
    "\n",
    "    total = len(ops)\n",
    "\n",
    "    for op_name, op in tqdm(ops.items()):\n",
    "        column_names = sorted(test_causenet.filter(regex=\"medical_score\"))\n",
    "        filtered = test_causenet[column_names]\n",
    "        half = filtered.shape[1] // 2\n",
    "        combined = op(filtered.iloc[:, :half].values, filtered.iloc[:, half:].values,)\n",
    "        test_causenet_combined.append(\n",
    "            pd.DataFrame(\n",
    "                combined,\n",
    "                columns=list(\n",
    "                    name[20:] + \"-\" + op_name for name in filtered.iloc[:, :half]\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    test_causenet_combined = pd.concat(test_causenet_combined, axis=1)\n",
    "    return test_causenet_combined\n",
    "\n",
    "\n",
    "# or and p=inf_mean are the same\n",
    "ops = {\n",
    "    \"p=neg_inf_mean\": lambda cause, effect: p_mean_threshold_combiner(cause, effect, -float(\"inf\")),\n",
    "    \"p=neg_10_mean\": lambda cause, effect: p_mean_threshold_combiner(cause, effect, -10),\n",
    "    \"p=neg_5_mean\": lambda cause, effect: p_mean_threshold_combiner(cause, effect, -5),\n",
    "    \"p=neg_2_mean\": lambda cause, effect: p_mean_threshold_combiner(cause, effect, -2),\n",
    "    \"p=neg_1_mean\": lambda cause, effect: p_mean_threshold_combiner(cause, effect, -1),\n",
    "    \"p=0_mean\": lambda cause, effect: p_mean_threshold_combiner(cause, effect, 0),\n",
    "    \"p=1_mean\": lambda cause, effect: p_mean_threshold_combiner(cause, effect, 1),\n",
    "    \"p=2_mean\": lambda cause, effect: p_mean_threshold_combiner(cause, effect, 2),\n",
    "    \"p=5_mean\": lambda cause, effect: p_mean_threshold_combiner(cause, effect, 5),\n",
    "    \"p=10_mean\": lambda cause, effect: p_mean_threshold_combiner(cause, effect, 10),\n",
    "    \"p=inf_mean\": lambda cause, effect: p_mean_threshold_combiner(cause, effect, float(\"inf\")),\n",
    "}\n",
    "test_causenet_combined = parse_test_causenet_combined(test_causenet, ops)\n",
    "test_causenet_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c5c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_causenet_combined.to_pickle(\n",
    "    os.path.join(constants.TEST_CAUSENET_PATH, \"test_causenet_combined.pkl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77470239",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_causenet_combined = pd.read_pickle(\n",
    "    os.path.join(constants.TEST_CAUSENET_PATH, \"test_causenet_combined.pkl\")\n",
    ")\n",
    "test_causenet_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bedfe68",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_test_causenet_medical(test_causenet_combined, method_threshold_dict):\n",
    "    test_causenet_medical = []\n",
    "\n",
    "    total = 0\n",
    "    for thresholds in method_threshold_dict.values():\n",
    "        total += len(thresholds)\n",
    "\n",
    "    pg = tqdm(total=total)\n",
    "    for method_name, thresholds in method_threshold_dict.items():\n",
    "        for threshold in thresholds:\n",
    "            column_names = sorted(test_causenet_combined.filter(regex=method_name))\n",
    "            filtered = test_causenet_combined[column_names]\n",
    "            medical = filtered >= threshold\n",
    "            medical = medical.add_suffix(f\"-{threshold}-medical\")\n",
    "            test_causenet_medical.append(medical)\n",
    "            pg.update()\n",
    "    test_causenet_medical = pd.concat(test_causenet_medical, axis=1)\n",
    "    return test_causenet_medical\n",
    "\n",
    "\n",
    "taggers = [\"quickumls\", \"scispacy\", \"metamap\", \"ctakes\"]\n",
    "method_threshold_dict = {\n",
    "    \"|\".join(taggers): [round(thresh, 4) for thresh in np.linspace(0, 1, 100)],\n",
    "    \"term_domain_specificity\": [round(thresh, 4) for thresh in np.linspace(0, 6, 100)],\n",
    "    \"contrastive_weight\": [round(thresh, 4) for thresh in np.linspace(50, 140, 100)],\n",
    "    \"discriminative_weight\": [round(thresh, 4) for thresh in np.linspace(0, 1100, 100)],\n",
    "    \"health_bert\": [round(thresh, 4) for thresh in np.linspace(0, 1, 100)],\n",
    "}\n",
    "\n",
    "test_causenet_medical = parse_test_causenet_medical(\n",
    "    test_causenet_combined, method_threshold_dict\n",
    ")\n",
    "test_causenet_medical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef304e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_causenet_medical.to_pickle(\n",
    "    os.path.join(constants.TEST_CAUSENET_PATH, \"test_causenet_medical.pkl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbbb523",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_causenet_medical = pd.read_pickle(\n",
    "    os.path.join(constants.TEST_CAUSENET_PATH, \"test_causenet_medical.pkl\")\n",
    ")\n",
    "test_causenet_medical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "sample_idcs = test_causenet_medical.groupby(test_causenet.dataset).sample(800, random_state=seed).index\n",
    "\n",
    "validation_causenet_medical = test_causenet_medical.loc[sample_idcs]\n",
    "test_causenet_medical = test_causenet_medical.loc[~test_causenet_medical.index.isin(sample_idcs.values)]\n",
    "\n",
    "validation_causenet = test_causenet.loc[sample_idcs]\n",
    "test_causenet = test_causenet.loc[~test_causenet.index.isin(sample_idcs.values)]\n",
    "\n",
    "validation_causenet_combined = test_causenet_combined.loc[sample_idcs]\n",
    "test_causenet_combined = test_causenet_combined.loc[~test_causenet_combined.index.isin(sample_idcs.values)]\n",
    "\n",
    "validation_causenet.shape, test_causenet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e54707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_causenet_medical(test_causenet, test_causenet_medical):\n",
    "    _tp = []\n",
    "    _fp = []\n",
    "    _tn = []\n",
    "    _fn = []\n",
    "    test_causenet_medical = test_causenet_medical.loc[~test_causenet.evaluation.isna()]\n",
    "    test_causenet = test_causenet.loc[~test_causenet.evaluation.isna()]\n",
    "    test_causenet.evaluation = test_causenet.evaluation.astype(int)\n",
    "    for _dataset in tqdm(test_causenet.dataset.drop_duplicates().values):\n",
    "        dataset_bool = test_causenet.dataset == _dataset\n",
    "        _test_causenet_medical = test_causenet_medical.loc[dataset_bool]\n",
    "        _test_causenet = test_causenet.loc[dataset_bool]\n",
    "        _tp.append(\n",
    "            (_test_causenet_medical & _test_causenet.evaluation.values[:, np.newaxis])\n",
    "            .sum()\n",
    "            .rename(_dataset)\n",
    "        )\n",
    "        _fp.append(\n",
    "            (_test_causenet_medical & ~_test_causenet.evaluation.values[:, np.newaxis])\n",
    "            .sum()\n",
    "            .rename(_dataset)\n",
    "        )\n",
    "        _tn.append(\n",
    "            (~_test_causenet_medical & ~_test_causenet.evaluation.values[:, np.newaxis])\n",
    "            .loc[dataset_bool]\n",
    "            .sum()\n",
    "            .rename(_dataset)\n",
    "        )\n",
    "        _fn.append(\n",
    "            (~_test_causenet_medical & _test_causenet.evaluation.values[:, np.newaxis])\n",
    "            .loc[dataset_bool]\n",
    "            .sum()\n",
    "            .rename(_dataset)\n",
    "        )\n",
    "    tp = pd.concat(_tp, axis=1)\n",
    "    fp = pd.concat(_fp, axis=1)\n",
    "    tn = pd.concat(_tn, axis=1)\n",
    "    fn = pd.concat(_fn, axis=1)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    numerator = tp * tn - fp * fn\n",
    "    denominator = np.log([tp + fp, tp + fn, tn + fp, tn + fn]).sum(axis=0) * (1 / 2)\n",
    "    mcc = numerator / np.exp(denominator)\n",
    "    test_causenet_metrics = (\n",
    "        pd.concat(\n",
    "            [tp, fp, tn, fn, precision, recall, f1, accuracy, mcc],\n",
    "            keys=[\n",
    "                \"tp\",\n",
    "                \"fp\",\n",
    "                \"tn\",\n",
    "                \"fn\",\n",
    "                \"precision\",\n",
    "                \"recall\",\n",
    "                \"f1\",\n",
    "                \"accuracy\",\n",
    "                \"mcc\",\n",
    "            ],\n",
    "        )\n",
    "        .stack()\n",
    "        .unstack(0)\n",
    "        .reset_index()\n",
    "    )\n",
    "    test_causenet_metrics = test_causenet_metrics.rename(\n",
    "        {\"level_1\": \"dataset\", \"level_0\": \"method\"}, axis=1\n",
    "    )\n",
    "    test_causenet_metrics = pd.concat(\n",
    "        [\n",
    "            test_causenet_metrics.drop(\"method\", axis=1),\n",
    "            test_causenet_metrics.method.str.rsplit(\"-\", expand=True, n=3)\n",
    "            .drop(3, axis=1)\n",
    "            .rename({0: \"method\", 1: \"operator\", 2: \"threshold\"}, axis=1),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    test_causenet_metrics = test_causenet_metrics[\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"method\",\n",
    "            \"operator\",\n",
    "            \"threshold\",\n",
    "            \"tp\",\n",
    "            \"fp\",\n",
    "            \"tn\",\n",
    "            \"fn\",\n",
    "            \"precision\",\n",
    "            \"recall\",\n",
    "            \"f1\",\n",
    "            \"accuracy\",\n",
    "            \"mcc\",\n",
    "        ]\n",
    "    ]\n",
    "    test_causenet_metrics = test_causenet_metrics.replace([np.inf, -np.inf], np.nan)\n",
    "    return test_causenet_metrics\n",
    "\n",
    "\n",
    "validation_causenet_metrics = evaluate_test_causenet_medical(\n",
    "    validation_causenet.loc[:, [\"evaluation\", \"dataset\"]], validation_causenet_medical,\n",
    ")\n",
    "test_causenet_metrics = evaluate_test_causenet_medical(\n",
    "    test_causenet.loc[:, [\"evaluation\", \"dataset\"]], test_causenet_medical,\n",
    ")\n",
    "\n",
    "test_causenet_metrics = test_causenet_metrics.loc[\n",
    "    test_causenet_metrics.dataset.isin([\"random_support\", \"random_full\"]) | \n",
    "    (test_causenet_metrics.operator == \"p=inf_mean\")\n",
    "]\n",
    "validation_causenet_metrics = validation_causenet_metrics.loc[\n",
    "    validation_causenet_metrics.dataset.isin([\"random_support\", \"random_full\"]) | \n",
    "    (validation_causenet_metrics.operator == \"p=inf_mean\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_causenet_metrics = test_causenet_metrics.loc[\n",
    "    test_causenet_metrics.dataset.isin([\"random_support\", \"random_full\"]) | \n",
    "    (test_causenet_metrics.operator == \"p=inf_mean\")\n",
    "]\n",
    "validation_causenet_metrics = validation_causenet_metrics.loc[\n",
    "    validation_causenet_metrics.dataset.isin([\"random_support\", \"random_full\"]) | \n",
    "    (validation_causenet_metrics.operator == \"p=inf_mean\")\n",
    "]\n",
    "test_causenet_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f44042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_causenet_metrics.to_pickle(\n",
    "    os.path.join(constants.TEST_CAUSENET_PATH, \"test_causenet_metrics.pkl\")\n",
    ")\n",
    "validation_causenet_metrics.to_pickle(\n",
    "    os.path.join(constants.TEST_CAUSENET_PATH, \"validation_causenet_metrics.pkl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b55884",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_causenet_metrics = pd.read_pickle(\n",
    "    os.path.join(constants.TEST_CAUSENET_PATH, \"test_causenet_metrics.pkl\")\n",
    ")\n",
    "validation_causenet_metrics = pd.read_pickle(\n",
    "    os.path.join(constants.TEST_CAUSENET_PATH, \"validation_causenet_metrics.pkl\")\n",
    ")\n",
    "test_causenet_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_plot(methods, plot_ops, dataset, labels=None):\n",
    "    ax = None\n",
    "    idx = 0\n",
    "    styles = [\"solid\", \"dashed\", \"dashdot\", \"dotted\"]\n",
    "    for method in methods:\n",
    "        for op in plot_ops:\n",
    "            plot_data = test_causenet_metrics.loc[\n",
    "                (test_causenet_metrics.method == method)\n",
    "                & (test_causenet_metrics.operator == op)\n",
    "                & (test_causenet_metrics.dataset == dataset)\n",
    "            ]\n",
    "            plot_data = plot_data.groupby(\"recall\").precision.max().sort_index()\n",
    "            method_name = \" \".join(\n",
    "                name_dict.get(sub_method, sub_method.title())\n",
    "                for sub_method in method.split(\"_\")\n",
    "            )\n",
    "            if labels is None:\n",
    "                plot_data = plot_data.rename(\n",
    "                    f\"{method_name} ({' '.join(op.split('-')).title()})\"\n",
    "                )\n",
    "            else:\n",
    "                plot_data = plot_data.rename(labels[idx])\n",
    "            ax = plot_data.plot.line(\n",
    "                figsize=(8, 4), linewidth=2, linestyle=styles[idx % len(styles)]\n",
    "            )\n",
    "            idx += 1\n",
    "    ax.legend()\n",
    "    ax.set_xlim((0, 1.05))\n",
    "    ax.set_ylim(0.1, 1.05)\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    fig = ax.get_figure()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b7144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_roc_curve(methods, ops, dataset, labels=None, ax=None, x_lim=(0, 1), y_lim=(0, 1)):\n",
    "    idx = 0\n",
    "    styles = [\"solid\", \"dashed\", \"dashdot\", \"dotted\"]\n",
    "    aucs = []\n",
    "    for method in methods:\n",
    "        for op in ops:\n",
    "            plot_data = validation_causenet_metrics.loc[\n",
    "                (validation_causenet_metrics.method == method)\n",
    "                & (validation_causenet_metrics.operator == op)\n",
    "                & (validation_causenet_metrics.dataset == dataset)\n",
    "            ].copy()\n",
    "            auc = sklearn.metrics.roc_auc_score(\n",
    "                    validation_causenet.loc[validation_causenet.dataset == dataset, \"evaluation\"],\n",
    "                    validation_causenet_combined.loc[validation_causenet.dataset == dataset].filter(\n",
    "                        like=\"-\".join((method, op))\n",
    "                    ),\n",
    "                )\n",
    "            aucs.append(auc)\n",
    "            plot_data[\"false_positive_rate\"] = plot_data[\"fp\"] / (\n",
    "                plot_data[\"fp\"] + plot_data[\"tn\"]\n",
    "            )\n",
    "            plot_data = (\n",
    "                plot_data.groupby(\"false_positive_rate\").recall.max().sort_index()\n",
    "            )\n",
    "            if labels is not None:\n",
    "                plot_data = plot_data.rename(labels[idx])\n",
    "            plot_data.plot.line(\n",
    "                linewidth=3, linestyle=styles[idx % len(styles)], ax=ax, xlabel=None\n",
    "            )\n",
    "            idx += 1\n",
    "    if labels is not None:\n",
    "        ax.legend()\n",
    "    ax.set_xlim(*x_lim)\n",
    "    ax.set_ylim(*y_lim)\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    fig = ax.get_figure()\n",
    "    return fig, aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c656876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Termhood score ROC plots + AUC\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 16\n",
    "LARGE_SIZE = 18\n",
    "\n",
    "dataset = \"random_full\"\n",
    "\n",
    "plot_dict = {\n",
    "#     \"$n$-grams\": {\n",
    "#         \"methods\": [f\"discriminative_weight-encyclopedia-(1, {value})-1\" for value in (1, 2, 3)],\n",
    "#         \"ops\": [\"p=1_mean\"],\n",
    "#         \"labels\": [\"$n$=1\", \"$n$=2\", \"$n$=3\"],\n",
    "#     },\n",
    "#     \"$p_p$-values\": {\n",
    "#         \"methods\": [f\"discriminative_weight-encyclopedia-(1, 3)-{value}\" for value in (1, 2, 5, 10, \"inf\")],\n",
    "#         \"ops\": [\"p=1_mean\"],\n",
    "#         \"labels\": [\"1\", \"2\", \"5\", \"10\", \"$\\infty$\"],\n",
    "#     },\n",
    "#     \"Operators\": {\n",
    "#         \"methods\": [\"discriminative_weight-encyclopedia-(1, 3)-1\"],\n",
    "#         \"ops\": [\"and\"] + [f\"p={value}_mean\" for value in (1, 2, 5, 10, \"inf\")],\n",
    "#         \"labels\": [\"AND\", \"1\", \"2\", \"5\", \"10\", \"$\\infty$\"],\n",
    "#         \"x_lim\": (0.0, 0.6),\n",
    "#         \"y_lim\": (0.4, 1.0),\n",
    "#     },\n",
    "    \"PubMedBERT\": {\n",
    "        \"methods\": [f\"health_bert-pubmedbert-{corpus}-{text_format}\" for corpus in [\"pubmed\", \"encyclopedia\"] for text_format in [\"sentence\", \"noun_phrase\"]],\n",
    "        \"ops\": [\"p=inf_mean\"],\n",
    "        \"labels\": [\"PM, S\", \"PM, NP\", \"ENC, S\", \"ENC, NP\"],\n",
    "#         \"x_lim\": (0.0, 0.6),\n",
    "#         \"y_lim\": (0.4, 1.0),\n",
    "        \"dataset\": \"sentence\",\n",
    "    },\n",
    "    \"     Discriminative Weight\": {\n",
    "        \"methods\": [f\"discriminative_weight-{corpus}-(1, 1)-1\" for corpus in [\"pubmed\", \"pubmed_central\", \"textbook\", \"encyclopedia\"]],\n",
    "        \"ops\": [\"p=1_mean\"],\n",
    "        \"labels\": [\"PM\", \"PMC\", \"TB\", \"ENC\"],\n",
    "        \"dataset\": \"random_full\"\n",
    "#         \"x_lim\": (0.0, 0.6),\n",
    "#         \"y_lim\": (0.4, 1.0),\n",
    "    },\n",
    "}\n",
    "\n",
    "num_plots = len(plot_dict)\n",
    "fig, axes = plt.subplots(1, ncols=num_plots, figsize=(4 * num_plots, 3))\n",
    "for idx, (ax, (title, kwargs)) in enumerate(zip(axes, plot_dict.items())):\n",
    "    fig, aucs = auc_roc_curve(ax=ax, **kwargs)\n",
    "    for auc in aucs:\n",
    "        ax.plot([0, 1], [0, 1], alpha=0.0, label=f\"({auc:.2f})\")\n",
    "    _text = ax.text(0, 1.05, f\"({chr(97 + idx)})\", transform=ax.transAxes, size=LARGE_SIZE)\n",
    "    if idx % 2 == 1:\n",
    "        _text.set_in_layout(False)\n",
    "    print(aucs)\n",
    "    ax.set_xlabel(ax.get_xlabel(), size=LARGE_SIZE)\n",
    "    ax.set_title(title, size=LARGE_SIZE)\n",
    "    legend = ax.legend(fontsize=15, framealpha=0.0, ncol=2, columnspacing=-2.5)\n",
    "    ax.tick_params(labelsize=SMALL_SIZE)\n",
    "\n",
    "# set y_label\n",
    "axes[0].set_ylabel(\"True Positive Rate\", size=LARGE_SIZE)\n",
    "    \n",
    "plt.tight_layout(pad=0.2)\n",
    "fig.savefig(\"figures/roc_curves.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plot_ops = [\"and\", \"or\"] + [f\"p={value}_mean\" for value in (1, 2, 5, 10, \"inf\")]\n",
    "all_plot_op_labels = [\"And\", \"Or\", \"p=1\", \"p=2\", \"p=5\", \"p=10\", \"p=inf\"]\n",
    "all_p_value_labels = [f\"p={p_value}\" for p_value in p_values]\n",
    "plot_dict = {\n",
    "    \"umls-subset\": {\n",
    "        \"methods\": [\"mesh_1.0\", \"mesh_syn_1.0\", \"umls_1.0\"],\n",
    "        \"plot_ops\": [\"and\"],\n",
    "        \"labels\": [\"MeSH\", \"MeSH Syn\", \"UMLS\"],\n",
    "    },\n",
    "    \"jaccard-threshold\": {\n",
    "        \"methods\": [\n",
    "            f\"mesh_syn_{jaccard_threshold}\"\n",
    "            for jaccard_threshold in jaccard_thresholds\n",
    "            if len(str(jaccard_threshold))\n",
    "        ][::-1],\n",
    "        \"plot_ops\": [\"and\"],\n",
    "        \"labels\": [\n",
    "            str(jaccard_threshold)\n",
    "            for jaccard_threshold in jaccard_thresholds\n",
    "            if len(str(jaccard_threshold))\n",
    "        ][::-1][::-1],\n",
    "    },\n",
    "    \"mesh-syn-operator\": {\n",
    "        \"methods\": [\"mesh_syn_1.0\"],\n",
    "        \"plot_ops\": all_plot_ops,\n",
    "        \"labels\": all_plot_op_labels,\n",
    "    },\n",
    "    \"approach\": {\n",
    "        \"methods\": [\n",
    "            \"mesh_syn_1.0\",\n",
    "            \"contrastive_weight_pubmed_(1, 1)_1\",\n",
    "            \"term_domain_specificity_pubmed_(1, 1)_1\",\n",
    "            \"discriminative_weight_pubmed_(1, 1)_1\",\n",
    "        ],\n",
    "        \"plot_ops\": [\"and\"],\n",
    "        \"labels\": [\n",
    "            \"MeSH Syn\",\n",
    "            \"Contrastive Weight\",\n",
    "            \"Term Domain Specificity\",\n",
    "            \"Discriminative Weight\",\n",
    "        ],\n",
    "    },\n",
    "    \"discriminative-weight-corpora\": {\n",
    "        \"methods\": [\n",
    "            f\"discriminative_weight_{corpus}_(1, 1)_1\" for corpus in medical_corpora\n",
    "        ],\n",
    "        \"plot_ops\": [\"and\"],\n",
    "        \"labels\": [\" \".join(corpus.split(\"_\")).title() for corpus in medical_corpora],\n",
    "    },\n",
    "    \"discriminative-weight-operator\": {\n",
    "        \"methods\": [\"discriminative_weight_encyclopedia_(1, 1)_1\"],\n",
    "        \"plot_ops\": all_plot_ops,\n",
    "        \"labels\": all_plot_op_labels,\n",
    "    },\n",
    "    \"discriminative-weight-generalized-mean\": {\n",
    "        \"methods\": [\n",
    "            f\"discriminative_weight_encyclopedia_(1, 1)_{p_value}\"\n",
    "            for p_value in p_values\n",
    "        ],\n",
    "        \"plot_ops\": [\"p=1_mean\"],\n",
    "        \"labels\": all_p_value_labels,\n",
    "    },\n",
    "    \"discriminative-weight-n-gram\": {\n",
    "        \"methods\": [\n",
    "            \"discriminative_weight_encyclopedia_(1, 1)_1\",\n",
    "            \"discriminative_weight_encyclopedia_(1, 2)_1\",\n",
    "            \"discriminative_weight_encyclopedia_(1, 3)_1\",\n",
    "        ],\n",
    "        \"plot_ops\": [\"p=1_mean\"],\n",
    "        \"labels\": [\"(1, 1)\", \"(1, 2)\", \"(1, 3)\"],\n",
    "    },\n",
    "}\n",
    "datasets = [\"random_full\", \"wikidata\"]\n",
    "for dataset in datasets:\n",
    "    for name, plot_values in plot_dict.items():\n",
    "        print(name)\n",
    "        fig = precision_recall_plot(dataset=dataset, **plot_values)\n",
    "        fig.savefig(f\"figures/{dataset}-{name}-comparison.pdf\", bbox_inches=\"tight\")\n",
    "        fig.clear(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a913a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# methods = [\"mesh_1.0\", \"mesh_syn_1.0\", \"umls_1.0\"]\n",
    "# methods = [f\"mesh_syn_{jaccard_threshold}\" for jaccard_threshold in jaccard_thresholds if len(str(jaccard_threshold))][::-1]\n",
    "# methods = [\"mesh_syn_1.0\"]\n",
    "# methods = [\"mesh_syn_1.0\", \"contrastive_weight_pubmed_(1, 1)_1\", \"term_domain_specificity_pubmed_(1, 1)_1\", \"discriminative_weight_pubmed_(1, 1)_1\"]\n",
    "methods = [f\"discriminative_weight_{corpus}_(1, 1)_1\" for corpus in medical_corpora]\n",
    "# methods = [\"discriminative_weight_encyclopedia_(1, 1)_1\"]\n",
    "# methods = [f\"discriminative_weight_encyclopedia_(1, 1)_{p_value}\" for p_value in p_values]\n",
    "# methods = [\"discriminative_weight_encyclopedia_(1, 1)_1\", \"discriminative_weight_encyclopedia_(1, 2)_1\"]\n",
    "# methods = [\"discriminative_weight_encyclopedia_(1, 1)_1\"]\n",
    "\n",
    "\n",
    "# plot_ops = [\"and\", \"or\", \"arithmetic_mean\", \"quadratic_mean\"]\n",
    "# plot_ops = [\"and\", \"arithmetic_mean\", \"quadratic_mean\"]\n",
    "# plot_ops = [\"and\", \"or\", \"arithmetic_mean\"]\n",
    "# plot_ops = [\"and\", \"arithmetic_mean\"]\n",
    "# plot_ops = [\"arithmetic_mean\"]\n",
    "plot_ops = [\"and\"]\n",
    "# plot_ops = [\"or\"]\n",
    "dataset = \"wikidata\"\n",
    "fig = precision_recall_plot(methods, plot_ops, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"discriminative-weight-corpus\"\n",
    "fig.savefig(f\"figures/{name}-comparison.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681919c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_approach(\n",
    "    validation_metrics,\n",
    "    test_metrics,\n",
    "    patterns,\n",
    "    datasets,\n",
    "    threshold_score,\n",
    "    threshold,\n",
    "    optimization_score,\n",
    "    eval_ops,\n",
    "    macro,\n",
    "    return_val=False,\n",
    "):\n",
    "    best_approaches = []\n",
    "\n",
    "    def best_run(df):\n",
    "        if threshold_score:\n",
    "            if ((df[threshold_score] < threshold) | df[threshold_score].isna()).all():\n",
    "                df = df.sort_values(threshold_score, ascending=False).iloc[[0]]\n",
    "            else:\n",
    "                df = df.loc[df[threshold_score] >= threshold]\n",
    "        df = df.sort_values(optimization_score, ascending=False).iloc[0]\n",
    "        df = df.map(lambda x: round(x, 4) if isinstance(x, (float)) else x)\n",
    "        return df\n",
    "\n",
    "    for pattern in patterns:\n",
    "        tmp = validation_metrics.loc[validation_metrics.method.str.contains(pattern)]\n",
    "        if datasets:\n",
    "            tmp = tmp.loc[tmp.dataset.isin(datasets)]\n",
    "        print(tmp.loc[~tmp.operator.isin(eval_ops)])\n",
    "        if eval_ops:\n",
    "            tmp = tmp.loc[tmp.operator.isin(eval_ops)]\n",
    "\n",
    "        best_runs = tmp.groupby(\"dataset\").apply(best_run)\n",
    "        best_approaches.append(best_runs)\n",
    "\n",
    "    idcs = np.tile(np.arange(len(patterns)), len(datasets)) * len(datasets) + np.repeat(\n",
    "        np.arange(len(datasets)), len(patterns)\n",
    "    )\n",
    "    best_approaches = (\n",
    "        pd.concat(best_approaches).reset_index(drop=True).iloc[idcs].reset_index()\n",
    "    )\n",
    "\n",
    "    if macro:\n",
    "        for pattern in patterns:\n",
    "            tmp = validation_metrics.loc[validation_metrics.method.str.contains(pattern)]\n",
    "            if datasets:\n",
    "                tmp = tmp.loc[tmp.dataset.isin(datasets)]\n",
    "            if eval_ops:\n",
    "                tmp = tmp.loc[tmp.operator.isin(eval_ops)]\n",
    "            tmp = tmp.groupby([\"method\", \"operator\", \"threshold\"]).mean().reset_index()\n",
    "            _best_run = best_run(tmp)\n",
    "            _best_run[\"dataset\"] = \"macro\"\n",
    "            best_approaches = pd.concat([best_approaches, _best_run])\n",
    "\n",
    "    best_approaches = best_approaches.drop(\"index\", axis=1)\n",
    "    best_approaches = best_approaches.reset_index(drop=True)\n",
    "    best_approaches[\"method_class\"] = best_approaches.method.apply(lambda x: x.split(\"-\")[0])\n",
    "    best_approaches.loc[best_approaches.method_class == \"health_bert\", \"method_class\"] = best_approaches.loc[best_approaches.method_class == \"health_bert\"].method.apply(lambda x: x.split(\"-\")[1])\n",
    "    validation_best_approaches = best_approaches.copy()\n",
    "    test_best_approaches = best_approaches.merge(test_metrics, how=\"left\", on=[\"dataset\", \"method\", \"operator\", \"threshold\"], suffixes=(\"_to_drop\", \"\"))\n",
    "    test_best_approaches = test_best_approaches.drop(test_best_approaches.filter(like=\"_to_drop\"), axis=1)\n",
    "    if return_val:\n",
    "        return validation_best_approaches, test_best_approaches\n",
    "    return test_best_approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f944cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    \"quickumls\",\n",
    "    \"scispacy\",\n",
    "    \"ctakes\",\n",
    "    \"metamap\",\n",
    "    \"-bert\",\n",
    "    \"scibert\",\n",
    "    \"pubmedbert\",\n",
    "    \"contrastive\",\n",
    "    \"specificity\",\n",
    "    \"discriminative\",\n",
    "]\n",
    "# patterns = [\"mesh_[0-9]\", \"mesh_syn_[0-9]\", \"umls_[0-9]\", \"contrastive_weight_1\", \"term_domain_specificity_1\", \"discriminative_weight_1\"]\n",
    "# patterns = [\"discriminative_weight_pubmed_(1, 2)_1\"]\n",
    "# patterns = [\"contrastive\", \"specificity\", \"discriminative\"]\n",
    "threshold_score = \"precision\"\n",
    "threshold = 0.9\n",
    "optimization_score = \"recall\"\n",
    "datasets = [\n",
    "    \"random_full\",\n",
    "    \"random_support\",\n",
    "    \"sentence\",\n",
    "]\n",
    "macro = False\n",
    "eval_ops = (\n",
    "    [\"p=0_mean\"]\n",
    "    + [f\"p={p}_mean\" for p in (1, 2, 5, 10, \"inf\")]\n",
    "    + [f\"p=neg_{p}_mean\" for p in (1, 2, 5, 10, \"inf\")]\n",
    ")\n",
    "\n",
    "validation_best_approaches, test_best_approaches = best_approach(\n",
    "    validation_causenet_metrics,\n",
    "    test_causenet_metrics,\n",
    "    patterns,\n",
    "    datasets,\n",
    "    threshold_score,\n",
    "    threshold,\n",
    "    optimization_score,\n",
    "    eval_ops,\n",
    "    macro,\n",
    "    return_val=True\n",
    ")\n",
    "\n",
    "if threshold_score:\n",
    "    validation_best_approaches.to_csv(f\"validation_best_approaches_{optimization_score}_{threshold_score}_{threshold}.csv\")\n",
    "    test_best_approaches.to_csv(f\"test_best_approaches_{optimization_score}_{threshold_score}_{threshold}.csv\")\n",
    "else:\n",
    "    validation_best_approaches.to_csv(f\"validation_best_approaches_{optimization_score}.csv\")\n",
    "    test_best_approaches.to_csv(f\"test_best_approaches_{optimization_score}.csv\")\n",
    "test_best_approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a465cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = validation_causenet_metrics.loc[(validation_causenet_metrics.dataset == \"random_full\") & validation_causenet_metrics.method.str.contains(\"discri\")]\n",
    "# tmp = tmp.loc[tmp.precision > 0.9].sort_values(\"recall\")\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574afe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_best_approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9357134",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_score = \"recall\"\n",
    "threshold = 0.9\n",
    "optimization_score = \"precision\"\n",
    "pd.read_csv(f\"validation_best_approaches_{optimization_score}_{threshold_score}_{threshold}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc(samples, labels):\n",
    "    tp = (samples & labels).sum()\n",
    "    tn = (~samples & ~labels).sum()\n",
    "    fp = (samples & ~labels).sum()\n",
    "    fn = (~samples & labels).sum()\n",
    "    numerator = tp * tn - fp * fn\n",
    "    denominator = np.log([tp + fp, tp + fn, tn + fp, tn + fn]).sum() * (1 / 2)\n",
    "    mcc = numerator / np.exp(denominator)\n",
    "    return mcc\n",
    "\n",
    "def precision(samples, labels):\n",
    "    tp = (samples & labels).sum()\n",
    "    fp = (samples & ~labels).sum()\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(samples, labels):\n",
    "    tp = (samples & labels).sum()\n",
    "    fn = (~samples & labels).sum()\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "\n",
    "def bootstrap(sample_x, sample_y, labels, metric, n):\n",
    "    base_value = metric(sample_x, labels) - metric(sample_y, labels)\n",
    "    pooled = list(sample_x) + list(sample_y)\n",
    "    num_samples = len(pooled) // 2\n",
    "    metrics = []\n",
    "    for _ in tqdm(range(n), position=3, leave=False):\n",
    "        permutation = np.random.permutation(pooled)\n",
    "        sampled_x = permutation[:num_samples]\n",
    "        sampled_y = permutation[num_samples:]\n",
    "        sampled_metric = metric(sampled_x, labels) - metric(sampled_y, labels)\n",
    "        metrics.append(sampled_metric)\n",
    "    metrics = np.array(metrics)\n",
    "    percentile = sum(base_value > metrics) / n\n",
    "    return percentile\n",
    "\n",
    "def significance_test(df, optimization_func, n=5000):\n",
    "    rows = df.method.apply(lambda x: x.split(\"-\")[0])\n",
    "    percentiles = pd.DataFrame(np.nan, index=rows, columns=rows)\n",
    "    name = df.name\n",
    "    if df.name not in test_causenet.dataset.unique().tolist():\n",
    "        return percentiles\n",
    "#     print(f\"{name:<30}\", end=\"\\r\")\n",
    "    df = df.copy()\n",
    "    df[\"approach_name\"] = (\n",
    "        df.method + \"-\" + df.operator + \"-\" + df.threshold.astype(str) + \"-medical\"\n",
    "    )\n",
    "    value_bool = (test_causenet.dataset == name)\n",
    "    for approach_1_idx, (_, approach_1) in tqdm(list(enumerate(df.iterrows())), position=1, leave=False):\n",
    "        samples_1 = test_causenet_medical.loc[value_bool, approach_1.approach_name].values\n",
    "        for approach_2_idx, (_, approach_2) in tqdm(list(enumerate(df.iterrows())), position=2, leave=False):\n",
    "            if approach_1_idx >= approach_2_idx:\n",
    "                continue\n",
    "            samples_2 = test_causenet_medical.loc[value_bool, approach_2.approach_name].values\n",
    "            percentile = bootstrap(\n",
    "                samples_1, samples_2, test_causenet.loc[value_bool, \"evaluation\"], optimization_func, n\n",
    "            )\n",
    "            percentiles.iloc[approach_1_idx, approach_2_idx] = percentile\n",
    "            percentiles.iloc[approach_2_idx, approach_1_idx] = 1 - percentile\n",
    "    return percentiles\n",
    "\n",
    "\n",
    "n = 100000\n",
    "threshold_score = \"\"\n",
    "threshold = 0.9\n",
    "optimization_score = \"mcc\"\n",
    "if threshold_score:\n",
    "    significance_approaches = pd.read_csv(f\"test_best_approaches_{optimization_score}_{threshold_score}_{threshold}.csv\")\n",
    "else:\n",
    "    significance_approaches = pd.read_csv(f\"test_best_approaches_{optimization_score}.csv\")\n",
    "\n",
    "\n",
    "if optimization_score == \"precision\":\n",
    "    optimization_func = precision\n",
    "elif optimization_score == \"recall\":\n",
    "    optimization_func = recall\n",
    "elif optimization_score == \"mcc\":\n",
    "    optimization_func = mcc\n",
    "else:\n",
    "    optimization_func = None\n",
    "    \n",
    "significance = (\n",
    "    significance_approaches.groupby(\"dataset\")\n",
    "    .progress_apply(lambda x: significance_test(x, optimization_func, n=n))\n",
    "    .drop(\"macro\", level=0, errors=\"ignore\")\n",
    ")\n",
    "\n",
    "if threshold_score:\n",
    "    significance.to_csv(f\"significance_{optimization_score}_{threshold_score}_{threshold}.csv\")\n",
    "else:\n",
    "    significance.to_csv(f\"significance_{optimization_score}.csv\")\n",
    "significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4386400",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_significance = pd.read_csv(\"significance_recall_precision_0.9.csv\", index_col=[0, 1])\n",
    "precision_significance.loc[\"wikidata\", \"discriminative_weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5882441",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_significance = pd.read_csv(\"significance_precision_recall_0.9.csv\", index_col=[0, 1])\n",
    "recall_significance.loc[\"wikidata\", \"pmbert\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_significance = pd.read_csv(\"significance_mcc.csv\", index_col=[0, 1])\n",
    "mcc_significance.loc[\"wikidata\", \"pmbert\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde34824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(\n",
    "    values,\n",
    "    bold=False,\n",
    "    bold_axis=0,\n",
    "    bold_command=\"\\highlight\",\n",
    "    bold_idcs=None,\n",
    "    round_to=2,\n",
    "    direction=\"maximize\",\n",
    "):\n",
    "    for row_idx, row in enumerate(values):\n",
    "        for col_idx, value in enumerate(row):\n",
    "            try:\n",
    "                float(value)\n",
    "                values[row_idx, col_idx] = round(float(value), round_to)\n",
    "            except ValueError:\n",
    "                pass\n",
    "    if bold_idcs is None:\n",
    "        bold_idcs = []\n",
    "    for idx, bold_idx in enumerate(bold_idcs):\n",
    "        if bold_idx < 0:\n",
    "            bold_idcs[idx] = values.shape[1 - bold_axis] + bold_idx\n",
    "    if bold:\n",
    "        if bold_axis not in (0, 1):\n",
    "            raise ValueError(f\"invalid axis value, expected 0, 1, got {axis}\")\n",
    "        inf_value = float(\"-inf\") if direction == \"maximize\" else float(\"inf\")\n",
    "        numerical_values = [\n",
    "            [value if isinstance(value, (float, int)) else inf_value for value in row]\n",
    "            for row in values\n",
    "        ]\n",
    "        if direction == \"maximize\":\n",
    "            best_values = np.amax(numerical_values, axis=bold_axis)\n",
    "        else:\n",
    "            best_values = np.amin(numerical_values, axis=bold_axis)\n",
    "        if bold_axis == 1:\n",
    "            values = values.T\n",
    "        for row_idx, row in enumerate(values):\n",
    "            for col_idx, value in enumerate(row):\n",
    "                if (\n",
    "                    bold_axis == 0\n",
    "                    and values[row_idx][col_idx] == best_values[col_idx]\n",
    "                    and col_idx in bold_idcs\n",
    "                ) or (\n",
    "                    bold_axis == 1\n",
    "                    and values[row_idx][col_idx] == best_values[row_idx]\n",
    "                    and row_idx in bold_idcs\n",
    "                ):\n",
    "                    string_value = str(value)\n",
    "                    string_value += \"0\" * (round_to - len(string_value) + 2)\n",
    "                    values[row_idx][col_idx] = f\"{bold_command}{{{string_value}}}\"\n",
    "        if bold_axis == 1:\n",
    "            values = values.T\n",
    "\n",
    "    string_values = []\n",
    "    for row in values:\n",
    "        row_string_values = []\n",
    "        for value in row:\n",
    "            try:\n",
    "                float(value)\n",
    "                string_value = str(value)\n",
    "                string_value += \"0\" * (round_to - len(string_value) + 2)\n",
    "                row_string_values.append(string_value)\n",
    "            except ValueError:\n",
    "                row_string_values.append(str(value))\n",
    "        string_values.append(row_string_values)\n",
    "\n",
    "    out = \" \\\\\\\\\\n\".join(\" & \".join(values) for values in string_values)\n",
    "    out += \" \\\\\\\\\"\n",
    "\n",
    "    print(out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def small(string):\n",
    "    return f\"\\small{{{string}}}\"\n",
    "\n",
    "\n",
    "def scriptsize(string):\n",
    "    return f\"\\scriptsize{{{string}}}\"\n",
    "\n",
    "\n",
    "def tiny(string):\n",
    "    return f\"\\\\tiny{{{string}}}\"\n",
    "\n",
    "\n",
    "def rename_method(method_name, parameters=True):\n",
    "\n",
    "    name_dict = {\n",
    "        \"pubmedbert\": \"PubMedBERT\",\n",
    "        \"scibert\": \"SciBERT\",\n",
    "        \"bert\": \"BERT\",\n",
    "        \"mesh\": \"MeSH\",\n",
    "        \"quickumls\": \"QuickUMLS\",\n",
    "        \"umls\": \"UMLS\",\n",
    "        \"contrastive_weight\": \"CW\",\n",
    "        \"term_domain_specificity\": \"TDS\",\n",
    "        \"discriminative_weight\": \"DW\",\n",
    "        \"encyclopedia\": \"ENC\",\n",
    "        \"pubmed\": \"PM\",\n",
    "        \"ctakes\": \"cTakes\",\n",
    "        \"metamap\": \"MetaMap\",\n",
    "        \"scispacy\": \"ScispaCy\",\n",
    "        \"en_core_sci_\": \"\",\n",
    "        \"rx_sno\": \"RS\",\n",
    "        \"full\": \"Full\",\n",
    "        \"noun_phrase\": \"NP\",\n",
    "        \"sentence\": \"S\",\n",
    "    }\n",
    "\n",
    "    for name, replace_name in name_dict.items():\n",
    "        method_name = method_name.replace(name, replace_name)\n",
    "    method_split = method_name.split(\"-\")\n",
    "    params = []\n",
    "    if \"CW\" in method_name or \"DW\" in method_name or \"TDS\" in method_name:\n",
    "        name = method_split[0]\n",
    "        if parameters:\n",
    "            try:\n",
    "                method_split[-2] = (\n",
    "                    \"$n$=\" + method_split[-2].strip(\"()\").split(\",\")[1].strip()\n",
    "                )\n",
    "                method_split[-1] = f\"$M_{{{method_split[-1]}}}$\"\n",
    "                params = method_split[1:]\n",
    "            except IndexError:\n",
    "                pass\n",
    "    elif \"ScispaCy\" in method_name:\n",
    "        name = method_split[0]\n",
    "        if parameters:\n",
    "            try:\n",
    "                params = [method_split[1], f\"\\\\textit{{{method_split[2]}}}\", method_split[3]]\n",
    "            except IndexError:\n",
    "                pass\n",
    "    elif \"health_BERT\" in method_name:\n",
    "        name = method_split[1]\n",
    "        if parameters:\n",
    "            params = method_split[2:]\n",
    "    else:\n",
    "        name = method_split[0] \n",
    "        if parameters:\n",
    "            params = method_split[1:] \n",
    "    if params:\n",
    "        name += \" \" + scriptsize(\", \".join(params))\n",
    "    return name\n",
    "\n",
    "\n",
    "def rename_operator(operator_name):\n",
    "    operator_name = operator_name.replace(\"inf\", \"\\infty\").replace(\"neg_\", \"-\")\n",
    "    value = operator_name.split(\"=\")[1].split(\"_\")[0]\n",
    "    if value[0] == \"-\":\n",
    "        value = r\"\\texttt{--}\\!\" + value[1:]\n",
    "    return f\"$M_{{{value}}}$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a807db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"random_full\"\n",
    "# dataset = \"random_support\"\n",
    "# dataset = \"sentence\"\n",
    "# dataset = \"wikidata\"\n",
    "# dataset = \"practitioner_full\"\n",
    "# dataset = \"practitioner_sure\"\n",
    "# dataset = \"practitioner_unsure\"\n",
    "\n",
    "approaches = [\n",
    "    \"ctakes\",\n",
    "    \"metamap\",\n",
    "    \"quickumls\",\n",
    "    \"scispacy\",\n",
    "    \"health_bert\",\n",
    "    \"contrastive\",\n",
    "    \"specificity\",\n",
    "    \"discriminative\",\n",
    "]\n",
    "\n",
    "\n",
    "def key(series):\n",
    "    order = pd.Series(-1, index=series.index)\n",
    "    for idx, approach in enumerate(approaches):\n",
    "        order[series.str.contains(approach)] = idx\n",
    "    return order\n",
    "\n",
    "\n",
    "threshold_score = \"precision\"\n",
    "threshold = 0.9\n",
    "optimization_score = \"recall\"\n",
    "if threshold_score:\n",
    "    pretty_print_approaches = pd.read_csv(f\"test_best_approaches_{optimization_score}_{threshold_score}_{threshold}.csv\")\n",
    "else:\n",
    "    pretty_print_approaches = pd.read_csv(f\"test_best_approaches_{optimization_score}.csv\")\n",
    "pretty_print_approaches[\"pretty_method\"] = pretty_print_approaches.method.map(\n",
    "    rename_method\n",
    ")\n",
    "pretty_print_approaches[\"pretty_operator\"] = pretty_print_approaches.operator.map(\n",
    "    rename_operator\n",
    ")\n",
    "\n",
    "pretty_print_approaches = pretty_print_approaches.loc[\n",
    "    pretty_print_approaches.dataset == dataset\n",
    "]\n",
    "pretty_print_approaches = pretty_print_approaches.loc[\n",
    "    key(pretty_print_approaches.method) != -1\n",
    "]\n",
    "pretty_print_approaches = pretty_print_approaches.sort_values(by=\"method\", key=key)\n",
    "\n",
    "pprint(\n",
    "    pretty_print_approaches.loc[\n",
    "        :, [\n",
    "            \"pretty_method\", \n",
    "            \"pretty_operator\", \n",
    "            \"precision\", \n",
    "            \"recall\", \n",
    "#             \"f1\", \n",
    "#             \"mcc\"\n",
    "        ]\n",
    "    ].values,\n",
    "    bold=True,\n",
    "    bold_idcs=[-1, -2, -3, -4],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31cc289",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_threshold = 0.0\n",
    "dataset = \"random_full\"\n",
    "linker_approaches = pd.read_csv(\n",
    "    f\"best_approaches_{precision_threshold}.csv\", index_col=0\n",
    ")\n",
    "linker_approaches = linker_approaches.loc[linker_approaches.dataset == dataset].iloc[:4]\n",
    "linker_predictions = []\n",
    "for _, method, operator, threshold, *_ in linker_approaches.values:\n",
    "    label = \"-\".join([method, operator, str(threshold), \"medical\"])\n",
    "    linker_predictions.append(\n",
    "        test_causenet_medical.loc[test_causenet.dataset == dataset, label]\n",
    "    )\n",
    "linker_predictions = pd.concat(linker_predictions, axis=1)\n",
    "for threshold in range(1, 5):\n",
    "    combined_predictions = linker_predictions.sum(axis=1) >= threshold\n",
    "    tp = (\n",
    "        combined_predictions\n",
    "        & test_causenet.evaluation.loc[test_causenet.dataset == dataset]\n",
    "    ).sum()\n",
    "    tn = (\n",
    "        ~combined_predictions\n",
    "        & ~test_causenet.evaluation.loc[test_causenet.dataset == dataset]\n",
    "    ).sum()\n",
    "    fp = (\n",
    "        combined_predictions\n",
    "        & ~test_causenet.evaluation.loc[test_causenet.dataset == dataset]\n",
    "    ).sum()\n",
    "    fn = (\n",
    "        ~combined_predictions\n",
    "        & test_causenet.evaluation.loc[test_causenet.dataset == dataset]\n",
    "    ).sum()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    print(threshold, round(precision, 2), round(recall, 2), round(f1, 2), round(accuracy, 2))\n",
    "linker_predictions.sum(axis=1).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa451c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ablation tests\n",
    "threshold_score = \"recall\"\n",
    "threshold = 0.9\n",
    "optimization_score = \"precision\"\n",
    "datasets = [\n",
    "    \"random_full\",\n",
    "    \"random_support\",\n",
    "    \"sentence\",\n",
    "#     \"support\",\n",
    "#     \"wikidata\",\n",
    "#     \"practitioner_full\",\n",
    "#     \"practitioner_sure\",\n",
    "#     \"practitioner_unsure\",\n",
    "]\n",
    "use_test_metrics = True\n",
    "full_train = True\n",
    "if use_test_metrics:\n",
    "    ablation_test_metrics = test_causenet_metrics\n",
    "    ablation_set = \"test\"\n",
    "else:\n",
    "    ablation_test_metrics = None\n",
    "    ablation_set = \"validation\"\n",
    "val_causenet = validation_causenet_metrics.loc[:, [\"dataset\", \"method\", \"operator\"]].drop_duplicates()\n",
    "val_causenet[\"method_class\"] = val_causenet[\"method\"].map(lambda x: x.split(\"-\")[0])\n",
    "\n",
    "\n",
    "if threshold_score:\n",
    "    ablation_best_approaches = pd.read_csv(f\"{ablation_set}_best_approaches_{optimization_score}_{threshold_score}_{threshold}.csv\", index_col=0)\n",
    "else:\n",
    "    ablation_best_approaches = pd.read_csv(f\"{ablation_set}_best_approaches_{optimization_score}.csv\", index_col=0)\n",
    "\n",
    "ablation_best_approaches = ablation_best_approaches.loc[\n",
    "    ablation_best_approaches.method_class.isin([\"contrastive_weight\", \"discriminative_weight\", \"term_domain_specificity\"]),\n",
    "    [\"dataset\", \"method\", \"operator\", \"method_class\", optimization_score]\n",
    "]\n",
    "ablation_best_approaches = ablation_best_approaches.reset_index(drop=True)\n",
    "ablation_methods = ablation_best_approaches.copy().drop(optimization_score, axis=1)\n",
    "ablation_best_approaches = ablation_best_approaches.loc[:, [\"dataset\", \"method_class\", optimization_score]]\n",
    "\n",
    "def get_ablation_df(ablation_df, validation_metrics, test_metrics=None):\n",
    "    ablation_df = ablation_df.merge(validation_metrics.dropna())\n",
    "    if threshold_score:\n",
    "        ablation_df = ablation_df.loc[ablation_df[threshold_score] >= threshold]\n",
    "    ablation_df = ablation_df.groupby([\"dataset\", \"method_class\"]).apply(lambda x: x.sort_values(optimization_score).iloc[-1]).reset_index(drop=True)\n",
    "    if test_metrics is not None:\n",
    "        ablation_df = ablation_df.merge(test_metrics, how=\"left\", on=[\"dataset\", \"method\", \"operator\", \"threshold\"], suffixes=(\"_to_drop\", \"\"))\n",
    "        ablation_df = ablation_df.drop(ablation_df.filter(like=\"_to_drop\"), axis=1)\n",
    "    ablation_df = ablation_df.loc[:, [\"dataset\", \"method_class\", optimization_score]]\n",
    "    return ablation_df\n",
    "\n",
    "if full_train:\n",
    "    pattern = r\".*-\\(1, 1\\)-.*\"\n",
    "    ablation_arithmetic_mean = get_ablation_df(\n",
    "        val_causenet.loc[val_causenet.method.str.contains(pattern)],\n",
    "        validation_causenet_metrics,\n",
    "        test_causenet_metrics,\n",
    "    )\n",
    "else:\n",
    "    ablation_arithmetic_mean = ablation_methods.copy()\n",
    "    ablation_arithmetic_mean[\"method\"] = ablation_arithmetic_mean[\"method\"].map(lambda x: \"-\".join(x.split(\"-\")[:-1]) + \"-1\")\n",
    "    ablation_arithmetic_mean = get_ablation_df(ablation_arithmetic_mean, validation_causenet_metrics, test_causenet_metrics)\n",
    "ablation_best_approaches = ablation_best_approaches.merge(ablation_arithmetic_mean, on=[\"dataset\", \"method_class\"], suffixes=[\"\", \"-arithmetic_mean\"], how=\"left\")\n",
    "\n",
    "if full_train:\n",
    "    pattern = r\".*-\\(1, .\\)-1\"\n",
    "    ablation_n_gram = get_ablation_df(\n",
    "        val_causenet.loc[val_causenet.method.str.contains(pattern)],\n",
    "        validation_causenet_metrics,\n",
    "        test_causenet_metrics,\n",
    "    )\n",
    "else:\n",
    "    ablation_n_gram = ablation_methods.copy()\n",
    "    ablation_n_gram[\"method\"] = ablation_n_gram[\"method\"].map(lambda x: x.split(\")\")[0][:-1] + \"1)\" + x.split(\")\")[1])\n",
    "    ablation_n_gram = get_ablation_df(ablation_n_gram, validation_causenet_metrics, test_causenet_metrics)\n",
    "ablation_best_approaches = ablation_best_approaches.merge(ablation_n_gram, on=[\"dataset\", \"method_class\"], suffixes=[\"\", \"-n_gram\"], how=\"left\")\n",
    "\n",
    "# if full_train:\n",
    "#     pattern = r\".*-\\(1, 1\\)-1\"\n",
    "#     ablation_arithmetic_mean_n_gram = get_ablation_df(\n",
    "#         val_causenet.loc[val_causenet.method.str.contains(pattern)],\n",
    "#         validation_causenet_metrics,\n",
    "#         test_causenet_metrics,\n",
    "#     )\n",
    "# else:\n",
    "#     ablation_arithmetic_mean_n_gram = ablation_methods.copy()\n",
    "#     ablation_arithmetic_mean_n_gram[\"method\"] = ablation_arithmetic_mean_n_gram[\"method\"].map(lambda x: x.split(\")\")[0][:-1] + \"1)\" + x.split(\")\")[1])\n",
    "#     ablation_arithmetic_mean_n_gram[\"method\"] = ablation_arithmetic_mean_n_gram[\"method\"].map(lambda x: \"-\".join(x.split(\"-\")[:-1]) + \"-1\")\n",
    "#     ablation_arithmetic_mean_n_gram = get_ablation_df(ablation_arithmetic_mean_n_gram, validation_causenet_metrics, test_causenet_metrics)\n",
    "# ablation_best_approaches = ablation_best_approaches.merge(ablation_arithmetic_mean_n_gram, on=[\"dataset\", \"method_class\"], suffixes=[\"\", \"-arithmetic_mean_n_gram\"], how=\"left\")\n",
    "\n",
    "ablation_best_approaches = ablation_best_approaches.merge(\n",
    "    ablation_best_approaches.filter(like=f\"{optimization_score}-\").subtract(ablation_best_approaches[optimization_score].values, axis=0), \n",
    "    suffixes=[\"\", \"-reduction\"],\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "ablation_best_approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cafaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\", pprint(ablation_best_approaches.filter(like=\"-reduction\").values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ablation tests\n",
    "threshold_score = \"\"\n",
    "threshold = 0.0\n",
    "optimization_score = \"mcc\"\n",
    "datasets = [\n",
    "    \"random_full\",\n",
    "    \"random_support\",\n",
    "    \"sentence\",\n",
    "#     \"support\",\n",
    "#     \"wikidata\",\n",
    "    #     \"practitioner_full\",\n",
    "    #     \"practitioner_sure\",\n",
    "    #     \"practitioner_unsure\",\n",
    "]\n",
    "eval_ops = [\"p=0_mean\"] + [f\"p={p}_mean\" for p in (1, 2, 5, 10, \"inf\")] + [f\"p=neg_{p}_mean\" for p in (1, 2, 5, 10, \"inf\")]\n",
    "\n",
    "patterns = [\n",
    "    \"contrastive_weight-.*-\\(1, 1\\)-.*\",\n",
    "    \"term_domain_specificity-.*-\\(1, 1\\)-.*\",\n",
    "    \"discriminative_weight-.*-\\(1, 1\\)-.*\",\n",
    "]\n",
    "best_approaches_n_grams = best_approach(\n",
    "    validation_causenet_metrics,\n",
    "    test_causenet_metrics,\n",
    "    patterns,\n",
    "    datasets,\n",
    "    threshold_score,\n",
    "    threshold,\n",
    "    optimization_score,\n",
    "    eval_ops,\n",
    "    False,\n",
    ")\n",
    "\n",
    "patterns = [\n",
    "    \"contrastive_weight-.*-\\(1, .\\)-1\",\n",
    "    \"term_domain_specificity-.*-\\(1, .\\)-1\",\n",
    "    \"discriminative_weight-.*-\\(1, .\\)-1\",\n",
    "]\n",
    "best_approaches_arithmetic_mean = best_approach(\n",
    "    validation_causenet_metrics,\n",
    "    test_causenet_metrics,\n",
    "    patterns,\n",
    "    datasets,\n",
    "    threshold_score,\n",
    "    threshold,\n",
    "    optimization_score,\n",
    "    eval_ops,\n",
    "    False,\n",
    ")\n",
    "\n",
    "# patterns = [\n",
    "#     \"contrastive_weight-.*-\\(1, .\\)-neg_inf\",\n",
    "#     \"term_domain_specificity-.*-\\(1, .\\)-neg_inf\",\n",
    "#     \"discriminative_weight-.*-\\(1, .\\)-neg_inf\",\n",
    "# ]\n",
    "# best_approaches_and = best_approach(\n",
    "#     validation_causenet_metrics,\n",
    "#     test_causenet_metrics,\n",
    "#     patterns,\n",
    "#     datasets,\n",
    "#     threshold_score,\n",
    "#     threshold,\n",
    "#     optimization_score,\n",
    "#     eval_ops,\n",
    "#     False,\n",
    "# )\n",
    "\n",
    "# patterns = [\n",
    "#     \"contrastive_weight-.*-\\(1, .\\)-inf\",\n",
    "#     \"term_domain_specificity-.*-\\(1, .\\)-inf\",\n",
    "#     \"discriminative_weight-.*-\\(1, .\\)-inf\",\n",
    "# ]\n",
    "# best_approaches_or = best_approach(\n",
    "#     validation_causenet_metrics,\n",
    "#     test_causenet_metrics,\n",
    "#     patterns,\n",
    "#     datasets,\n",
    "#     threshold_score,\n",
    "#     threshold,\n",
    "#     optimization_score,\n",
    "#     eval_ops,\n",
    "#     False,\n",
    "# )\n",
    "\n",
    "patterns = [\n",
    "    \"contrastive_weight-.*-\\(1, 1\\)-1\",\n",
    "    \"term_domain_specificity-.*-\\(1, 1\\)-1\",\n",
    "    \"discriminative_weight-.*-\\(1, 1\\)-1\",\n",
    "]\n",
    "best_approaches_n_grams_arithmetic_mean = best_approach(\n",
    "    validation_causenet_metrics,\n",
    "    test_causenet_metrics,\n",
    "    patterns,\n",
    "    datasets,\n",
    "    threshold_score,\n",
    "    threshold,\n",
    "    optimization_score,\n",
    "    eval_ops,\n",
    "    False,\n",
    ")\n",
    "\n",
    "# patterns = [\n",
    "# #     \"metamap\",\n",
    "# #     \"ctakes\",\n",
    "# #     \"quickumls\",\n",
    "# #     \"scispacy\",\n",
    "# #     \"-bert\",\n",
    "# #     \"scibert\",\n",
    "# #     \"pubmedbert\",\n",
    "#     \"contrastive_weight-.*-\\(1, .\\)-.*\",\n",
    "#     \"term_domain_specificity-.*-\\(1, .\\)-.*\",\n",
    "#     \"discriminative_weight-.*-\\(1, .\\)-.*\",\n",
    "# ]\n",
    "# eval_ops = [\"p=neg_inf_mean\"]\n",
    "# best_approaches_and_operator = best_approach(\n",
    "#     validation_causenet_metrics,\n",
    "#     test_causenet_metrics,\n",
    "#     patterns,\n",
    "#     [dataset for dataset in datasets if dataset != \"sentence\"],\n",
    "#     threshold_score,\n",
    "#     threshold,\n",
    "#     optimization_score,\n",
    "#     eval_ops,\n",
    "#     False,\n",
    "# )\n",
    "\n",
    "# patterns = [\n",
    "# #     \"metamap\",\n",
    "# #     \"ctakes\",\n",
    "# #     \"quickumls\",\n",
    "# #     \"scispacy\",\n",
    "# #     \"-bert\",\n",
    "# #     \"scibert\",\n",
    "# #     \"pubmedbert\",\n",
    "#     \"contrastive_weight-.*-\\(1, .\\)-.*\",\n",
    "#     \"term_domain_specificity-.*-\\(1, .\\)-.*\",\n",
    "#     \"discriminative_weight-.*-\\(1, .\\)-.*\",\n",
    "# ]\n",
    "# eval_ops = [\"p=inf_mean\"]\n",
    "# best_approaches_or_operator = best_approach(\n",
    "#     validation_causenet_metrics,\n",
    "#     test_causenet_metrics,\n",
    "#     patterns,\n",
    "#     [dataset for dataset in datasets if dataset != \"sentence\"],\n",
    "#     threshold_score,\n",
    "#     threshold,\n",
    "#     optimization_score,\n",
    "#     eval_ops,\n",
    "#     False,\n",
    "# )\n",
    "\n",
    "patterns = [\n",
    "#     \"metamap\",\n",
    "#     \"ctakes\",\n",
    "#     \"quickumls\",\n",
    "#     \"scispacy\",\n",
    "#     \"-bert\",\n",
    "#     \"scibert\",\n",
    "#     \"pubmedbert\",\n",
    "    \"contrastive_weight-.*-\\(1, .\\)-.*\",\n",
    "    \"term_domain_specificity-.*-\\(1, .\\)-.*\",\n",
    "    \"discriminative_weight-.*-\\(1, .\\)-.*\",\n",
    "]\n",
    "eval_ops = [\"p=1_mean\"]\n",
    "best_approaches_arithmetic_mean_operator = best_approach(\n",
    "    validation_causenet_metrics,\n",
    "    test_causenet_metrics,\n",
    "    patterns,\n",
    "    [dataset for dataset in datasets if dataset != \"sentence\"],\n",
    "    threshold_score,\n",
    "    threshold,\n",
    "    optimization_score,\n",
    "    eval_ops,\n",
    "    False,\n",
    ")\n",
    "\n",
    "# patterns = [\n",
    "#     \"health_bert-.*-sentence\",\n",
    "# ]\n",
    "# eval_ops = [\"and\"] + [f\"p={p}_mean\" for p in (1, 2, 5, 10, \"inf\")]\n",
    "# best_approaches_sentence = best_approach(\n",
    "#     test_causenet_metrics,\n",
    "#     patterns,\n",
    "#     datasets,\n",
    "#     threshold_score,\n",
    "#     threshold,\n",
    "#     optimization_score,\n",
    "#     eval_ops,\n",
    "#     False,\n",
    "# )\n",
    "\n",
    "if threshold_score:\n",
    "    best_approaches_comparison = pd.read_csv(f\"test_best_approaches_{optimization_score}_{threshold_score}_{threshold}.csv\")\n",
    "else:\n",
    "    best_approaches_comparison = pd.read_csv(f\"test_best_approaches_{optimization_score}.csv\")\n",
    "\n",
    "best_approaches_comparison = best_approaches_comparison.loc[:, [\"method_class\", \"dataset\", optimization_score]]\n",
    "best_approaches_comparison = best_approaches_comparison.merge(best_approaches_n_grams.loc[:, [\"method_class\", \"dataset\", optimization_score]], on=[\"method_class\", \"dataset\"], suffixes=(\"\", \"-n_grams\"), how=\"inner\")\n",
    "# best_approaches_comparison = best_approaches_comparison.merge(best_approaches_and.loc[:, [\"method_class\", \"dataset\", optimization_score]], on=[\"method_class\", \"dataset\"], suffixes=(\"\", \"-and\"), how=\"outer\")\n",
    "# best_approaches_comparison = best_approaches_comparison.merge(best_approaches_or.loc[:, [\"method_class\", \"dataset\", optimization_score]], on=[\"method_class\", \"dataset\"], suffixes=(\"\", \"-or\"), how=\"outer\")\n",
    "# best_approaches_comparison = best_approaches_comparison.merge(best_approaches_and_operator.loc[:, [\"method_class\", \"dataset\", optimization_score]], on=[\"method_class\", \"dataset\"], suffixes=(\"\", \"-and_operator\"), how=\"outer\")\n",
    "# best_approaches_comparison = best_approaches_comparison.merge(best_approaches_or_operator.loc[:, [\"method_class\", \"dataset\", optimization_score]], on=[\"method_class\", \"dataset\"], suffixes=(\"\", \"-or_operator\"), how=\"outer\")\n",
    "best_approaches_comparison = best_approaches_comparison.merge(best_approaches_arithmetic_mean.loc[:, [\"method_class\", \"dataset\", optimization_score]], on=[\"method_class\", \"dataset\"], suffixes=(\"\", \"-arithmetic_mean\"), how=\"outer\")\n",
    "best_approaches_comparison = best_approaches_comparison.merge(best_approaches_n_grams_arithmetic_mean.loc[:, [\"method_class\", \"dataset\", optimization_score]], on=[\"method_class\", \"dataset\"], suffixes=(\"\", \"-n_grams_arithmetic_mean\"), how=\"outer\")\n",
    "best_approaches_comparison = best_approaches_comparison.merge(best_approaches_arithmetic_mean_operator.loc[:, [\"method_class\", \"dataset\", optimization_score]], on=[\"method_class\", \"dataset\"], suffixes=(\"\", \"-arithmetic_mean_operator\"), how=\"outer\")\n",
    "# best_approaches_comparison = best_approaches_comparison.merge(best_approaches_sentence.loc[:, [\"method_class\", \"dataset\", optimization_score]], on=[\"method_class\", \"dataset\"], suffixes=(\"\", \"-sentence\"), how=\"outer\")\n",
    "other_scores = best_approaches_comparison.filter(regex=f\"{optimization_score}-\", axis=1)\n",
    "reduction = other_scores.subtract(best_approaches_comparison[optimization_score].values, axis=0)\n",
    "best_approaches_comparison = pd.concat([best_approaches_comparison, reduction.add_suffix(\"-reduction\")], axis=1)\n",
    "best_approaches_comparison.filter(regex=f\"method_class|dataset|{optimization_score}-.*reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_causenet_metrics.loc[(validation_causenet_metrics.dataset == \"sentence\") & (validation_causenet_metrics.method.str.contains(\"term\")) & (validation_causenet_metrics.method.str.contains(\"enc\"))].dropna().sort_values(\"mcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_causenet_metrics.loc[(test_causenet_metrics.dataset == \"sentence\") & (test_causenet_metrics.method.str.contains(\"term\")) & (test_causenet_metrics.method.str.contains(\"enc\"))].dropna().sort_values(\"mcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(best_approaches_comparison.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_approaches_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8429827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_approaches_comparison.loc[best_approaches_comparison.method_class.str.contains(\"bert\")].filter(regex=\"method_class|dataset|mcc-.*reduction\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e16b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_approaches_comparison.filter(regex=\"method_class|dataset|mcc-.*reduction\").groupby(\"method_class\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a70317",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_approaches_comparison.loc[best_approaches_comparison.method_class.str.contains(\"bert\")].filter(regex=\"method_class|dataset|mcc-.*reduction\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different classifications between methods\n",
    "\n",
    "# method_1_name = \"discriminative_weight_pubmed_(1, 1)_1\"\n",
    "# method_2_name = \"discriminative_weight_pubmed_(1, 2)_1\"\n",
    "# op_1 = \"and\"\n",
    "# op_2 = \"and\"\n",
    "# threshold_1 = \"66\"\n",
    "# threshold_2 = \"66\"\n",
    "\n",
    "sort = False\n",
    "ascending=False\n",
    "dataset = \"random_full\"\n",
    "method_1 = \"term_domain_specificity\"\n",
    "method_2 = \"pubmedbert\"\n",
    "method_1_data = best_approaches.loc[best_approaches.dataset == dataset].set_index(\"method_class\").loc[method_1]\n",
    "method_2_data = best_approaches.loc[best_approaches.dataset == dataset].set_index(\"method_class\").loc[method_2]\n",
    "method_1_name = method_1_data.method\n",
    "method_2_name = method_2_data.method\n",
    "op_1 = method_1_data.operator\n",
    "op_2 = method_2_data.operator\n",
    "threshold_1 = method_1_data.threshold\n",
    "threshold_2 = method_2_data.threshold\n",
    "\n",
    "medical_label_1 = \"-\".join([method_1_name, op_1, threshold_1, \"medical\"])\n",
    "medical_label_2 = \"-\".join([method_2_name, op_2, threshold_2, \"medical\"])\n",
    "combined_label_1 = \"-\".join([method_1_name, op_1])\n",
    "combined_label_2 = \"-\".join([method_2_name, op_2])\n",
    "score_label_1 = \"-\".join([\"medical_score\", \"{}\", method_1_name])\n",
    "score_label_2 = \"-\".join([\"medical_score\", \"{}\", method_2_name])\n",
    "\n",
    "labels = [\n",
    "    \"cause\",\n",
    "    \"effect\",\n",
    "    \"evaluation\",\n",
    "    medical_label_1,\n",
    "    medical_label_2,\n",
    "    combined_label_1,\n",
    "    combined_label_2,\n",
    "    score_label_1.format(\"cause\"),\n",
    "    score_label_1.format(\"effect\"),\n",
    "    score_label_2.format(\"cause\"),\n",
    "    score_label_2.format(\"effect\"),\n",
    "]\n",
    "\n",
    "df_filter = pd.Series(True, index=test_causenet_medical.index)\n",
    "df_filter = df_filter & (test_causenet.dataset == dataset)\n",
    "df_filter = df_filter & (\n",
    "    test_causenet_medical[medical_label_1] != test_causenet_medical[medical_label_2]\n",
    ")\n",
    "\n",
    "health_causenet_errors = pd.concat(\n",
    "    [\n",
    "        test_causenet_medical.loc[df_filter, [medical_label_1, medical_label_2]],\n",
    "        test_causenet.loc[\n",
    "            df_filter, \n",
    "            [\n",
    "                \"cause\", \n",
    "                \"effect\", \n",
    "                \"evaluation\",\n",
    "                score_label_1.format(\"cause\"),\n",
    "                score_label_2.format(\"cause\"),\n",
    "                score_label_1.format(\"effect\"),\n",
    "                score_label_2.format(\"effect\"),\n",
    "            ]\n",
    "        ],\n",
    "        test_causenet_combined.loc[df_filter, [combined_label_1, combined_label_2]]\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "health_causenet_errors = health_causenet_errors.loc[:, labels]\n",
    "# if sort:\n",
    "#     sort_index = (\n",
    "#         (health_causenet_errors.iloc[:, -2] + health_causenet_errors.iloc[:, -1])\n",
    "#         .sort_values(ascending=ascending)\n",
    "#         .index\n",
    "#     )\n",
    "#     health_causenet_errors = health_causenet_errors.loc[sort_index]\n",
    "health_causenet_errors = health_causenet_errors.sort_values([\"evaluation\", medical_label_1, medical_label_2])\n",
    "# pprint(health_causenet_errors.head(40).values)\n",
    "health_causenet_errors.to_csv(f\"errors-{method_1_name}_vs_{method_2_name}.csv\")\n",
    "health_causenet_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_causenet_combined.loc[df_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32787a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascending = False\n",
    "sort = False\n",
    "method = \"discriminative_weight-encyclopedia-(1, 1)-2\"\n",
    "op = \"arithmetic_mean\"\n",
    "threshold = \"60\"\n",
    "medical_label = \"-\".join([method, op, threshold, \"medical\"])\n",
    "labels = [\n",
    "    \"cause\",\n",
    "    \"effect\",\n",
    "    \"cause_origin\",\n",
    "    \"effect_origin\",\n",
    "    f\"medical_score-cause-{method}\",\n",
    "    f\"medical_score-effect-{method}\",\n",
    "]\n",
    "dataset = \"wikidata\"\n",
    "errors = [\n",
    "    #     \"tp\",\n",
    "    \"fp\",\n",
    "    #     \"tn\",\n",
    "    #     \"fn\",\n",
    "]\n",
    "value_filter = pd.Series(True, index=test_causenet_medical.index)\n",
    "if dataset:\n",
    "    value_filter = value_filter & ((test_causenet.dataset == dataset).values)\n",
    "if errors:\n",
    "    error_filter = pd.Series(False, index=test_causenet_medical.index)\n",
    "    if \"tp\" in errors:\n",
    "        error_filter = error_filter | (test_causenet_medical[medical_label] == 1) & (\n",
    "            evaluation.values == 1\n",
    "        )\n",
    "    if \"fp\" in errors:\n",
    "        error_filter = error_filter | (test_causenet_medical[medical_label] == 1) & (\n",
    "            evaluation.values == 0\n",
    "        )\n",
    "    if \"tn\" in errors:\n",
    "        error_filter = error_filter | (test_causenet_medical[medical_label] == 0) & (\n",
    "            evaluation.values == 0\n",
    "        )\n",
    "    if \"fn\" in errors:\n",
    "        error_filter = error_filter | (test_causenet_medical[medical_label] == 0) & (\n",
    "            evaluation.values == 1\n",
    "        )\n",
    "    value_filter = value_filter & error_filter\n",
    "health_causenet_errors = pd.concat(\n",
    "    [\n",
    "        test_causenet.loc[value_filter.values, labels],\n",
    "        test_causenet_medical.loc[value_filter.values, [medical_label]],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "health_causenet_errors[\"evaluation\"] = evaluation.loc[value_filter.values].values\n",
    "health_causenet_errors = health_causenet_errors.loc[\n",
    "    :,\n",
    "    [\n",
    "        \"cause\",\n",
    "        \"effect\",\n",
    "        \"cause_origin\",\n",
    "        \"effect_origin\",\n",
    "        \"evaluation\",\n",
    "        medical_label,\n",
    "        f\"medical_score-cause-{method}\",\n",
    "        f\"medical_score-effect-{method}\",\n",
    "    ],\n",
    "]\n",
    "if sort:\n",
    "    sort_index = (\n",
    "        (health_causenet_errors.iloc[:, -2] + health_causenet_errors.iloc[:, -1])\n",
    "        .sort_values(ascending=ascending)\n",
    "        .index\n",
    "    )\n",
    "    health_causenet_errors = health_causenet_errors.loc[sort_index]\n",
    "# else:\n",
    "#     health_causenet_errors = health_causenet_errors.sample(health_causenet_errors.shape[0])\n",
    "health_causenet_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41202056",
   "metadata": {},
   "source": [
    "# Sentence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    \"quickumls\",\n",
    "    \"scispacy\",\n",
    "    \"ctakes\",\n",
    "    \"metamap\",\n",
    "    \"-bert\",\n",
    "    \"scibert\",\n",
    "    \"pubmedbert\",\n",
    "    \"contrastive\",\n",
    "    \"specificity\",\n",
    "    \"discriminative\",\n",
    "]\n",
    "\n",
    "precision_threshold = 0.0\n",
    "datasets = [\n",
    "    \"random_full\",\n",
    "    #     \"random_support\",\n",
    "    #     \"support\",\n",
    "]\n",
    "macro = False\n",
    "eval_ops = [\"p=inf_mean\"]\n",
    "optimization_score = \"mcc\"\n",
    "\n",
    "best_approaches = best_approach(\n",
    "    sentence_test_causenet_metrics,\n",
    "    patterns,\n",
    "    datasets,\n",
    "    precision_threshold,\n",
    "    optimization_score,\n",
    "    eval_ops,\n",
    "    macro,\n",
    ")\n",
    "best_approaches.to_csv(f\"sentence_best_approaches_{optimization_score}_{precision_threshold}.csv\")\n",
    "best_approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b42e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"wikidata\"\n",
    "dataset = \"random_full\"\n",
    "# dataset = \"random_support\"\n",
    "# dataset = \"practitioner_full\"\n",
    "# dataset = \"practitioner_sure\"\n",
    "# dataset = \"practitioner_unsure\"\n",
    "\n",
    "approaches = [\n",
    "    \"metamap\",\n",
    "    \"ctakes\",\n",
    "    \"quickumls\",\n",
    "    \"scispacy\",\n",
    "    \"-bert\",\n",
    "    \"scibert\",\n",
    "    \"pubmedbert\",\n",
    "    \"contrastive\",\n",
    "    \"specificity\",\n",
    "    \"discriminative\",\n",
    "]\n",
    "\n",
    "\n",
    "def key(series):\n",
    "    order = pd.Series(-1, index=series.index)\n",
    "    for idx, approach in enumerate(approaches):\n",
    "        order[series.str.contains(approach)] = idx\n",
    "    return order\n",
    "\n",
    "\n",
    "pretty_print_approaches = best_approaches.copy()\n",
    "pretty_print_approaches[\"pretty_method\"] = pretty_print_approaches.method.map(\n",
    "    lambda x: rename_method(x, False)\n",
    ")\n",
    "pretty_print_approaches[\"pretty_operator\"] = pretty_print_approaches.operator.map(\n",
    "    rename_operator\n",
    ")\n",
    "\n",
    "pretty_print_approaches = pretty_print_approaches.loc[\n",
    "    pretty_print_approaches.dataset == dataset\n",
    "]\n",
    "pretty_print_approaches = pretty_print_approaches.loc[\n",
    "    key(pretty_print_approaches.method) != -1\n",
    "]\n",
    "pretty_print_approaches = pretty_print_approaches.sort_values(by=\"method\", key=key)\n",
    "\n",
    "pprint(\n",
    "    pretty_print_approaches.loc[\n",
    "        :, [\"pretty_method\", \"precision\", \"recall\", \"f1\", \"mcc\"]\n",
    "    ].values,\n",
    "    bold=True,\n",
    "    bold_idcs=[-1, -2, -3],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e5a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"random_full\"\n",
    "# method = \"health_bert-encyclopedia-noun_phrase\"\n",
    "method = \"discriminative_weight-encyclopedia-(1, 1)-1\"\n",
    "operator = \"or\"\n",
    "# threshold = \"0.64\"\n",
    "threshold = \"35\"\n",
    "label_name = \"-\".join((method, operator, threshold)) + \"-medical\"\n",
    "score_name = \"medical_score-cause-\" + method\n",
    "filter_bool = (\n",
    "    (sentence_test_causenet.dataset == dataset)\n",
    "    & ~sentence_test_causenet_medical[label_name]\n",
    "    & sentence_test_causenet.evaluation\n",
    ")\n",
    "sentence_test_causenet.loc[filter_bool, [\"cause\", score_name, \"evaluation\"]].rename(\n",
    "    {score_name: \"medical_score\"}, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1053f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"random_full\"\n",
    "method_1 = \"health_bert-encyclopedia-noun_phrase\"\n",
    "method_2 = \"discriminative_weight-encyclopedia-(1, 1)-1\"\n",
    "threshold_1 = \"0.64\"\n",
    "threshold_2 = \"35\"\n",
    "label_name_1 = \"-\".join((method_1, \"or\", threshold)) + \"-medical\"\n",
    "label_name_2 = \"-\".join((method_2, \"or\", threshold)) + \"-medical\"\n",
    "score_name = \"medical_score-cause-\" + method\n",
    "filter_bool = (\n",
    "    (sentence_test_causenet.dataset == dataset)\n",
    "    & ~sentence_test_causenet_medical[label_name]\n",
    "    & sentence_test_causenet.evaluation\n",
    ")\n",
    "sentence_test_causenet.loc[filter_bool, [\"cause\", score_name, \"evaluation\"]].rename(\n",
    "    {score_name: \"medical_score\"}, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10558705",
   "metadata": {},
   "source": [
    "# Sentence vs Phrase Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list([method for method in test_causenet_medical if \"discriminative_weight-encyclopedia-(1, 3)\" in method])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86813049",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_threshold = 0.0\n",
    "dataset = \"random_full\"\n",
    "approach = \"health_bert\"\n",
    "phrase_approaches = pd.read_csv(\n",
    "    f\"best_approaches_{precision_threshold}.csv\", index_col=0\n",
    ")\n",
    "phrase_approaches = phrase_approaches.loc[phrase_approaches.dataset == \"random_full\"]\n",
    "sentence_approaches = pd.read_csv(\n",
    "    f\"sentence_best_approaches_{precision_threshold}.csv\", index_col=0\n",
    ")\n",
    "\n",
    "phrase_predictions = []\n",
    "sentence_predictions = []\n",
    "for _, method, operator, threshold, *_ in phrase_approaches.values:\n",
    "    try:\n",
    "        label = \"-\".join([method, operator, str(threshold), \"medical\"])\n",
    "        phrase_predictions.append(\n",
    "            test_causenet_medical.loc[test_causenet.dataset == dataset, label]\n",
    "        )\n",
    "    except KeyError:\n",
    "        threshold = int(threshold)\n",
    "        label = \"-\".join([method, operator, str(threshold), \"medical\"])\n",
    "        phrase_predictions.append(\n",
    "            test_causenet_medical.loc[test_causenet.dataset == dataset, label]\n",
    "        )\n",
    "for _, method, operator, threshold, *_ in sentence_approaches.values:\n",
    "    try:\n",
    "        label = \"-\".join([method, operator, str(threshold), \"medical\"])\n",
    "        sentence_predictions.append(\n",
    "            sentence_test_causenet_medical.loc[test_causenet.dataset == dataset, label]\n",
    "        )\n",
    "    except KeyError:\n",
    "        threshold = int(threshold)\n",
    "        label = \"-\".join([method, operator, str(threshold), \"medical\"])\n",
    "        sentence_predictions.append(\n",
    "            sentence_test_causenet_medical.loc[test_causenet.dataset == dataset, label]\n",
    "        )\n",
    "phrase_predictions = pd.concat(phrase_predictions, axis=1)\n",
    "sentence_predictions = pd.concat(sentence_predictions, axis=1)\n",
    "phrase_predictions.columns = [\n",
    "    column.split(\"-\")[0] for column in phrase_predictions.columns\n",
    "]\n",
    "sentence_predictions.columns = [\n",
    "    column.split(\"-\")[0] for column in sentence_predictions.columns\n",
    "]\n",
    "different_predictions = phrase_predictions != sentence_predictions\n",
    "different_predictions = sentence_test_causenet.loc[sentence_test_causenet.dataset == dataset].loc[\n",
    "    different_predictions[approach].values, [\"cause\", \"effect\", \"sentence\"]\n",
    "].join(phrase_predictions[approach].rename(\"phrase_prediction\")).join(\n",
    "    sentence_predictions[approach].rename(\"sentence_prediction\")\n",
    ").join(\n",
    "    test_causenet.evaluation.rename(\"phrase_label\")\n",
    ").join(\n",
    "    sentence_test_causenet.manual_evaluation.rename(\"sentence_label\")\n",
    ").loc[\n",
    "    :,\n",
    "    [\n",
    "        \"cause\",\n",
    "        \"effect\",\n",
    "        \"sentence\",\n",
    "        \"phrase_label\",\n",
    "        \"sentence_label\",\n",
    "        \"phrase_prediction\",\n",
    "        \"sentence_prediction\",\n",
    "    ],\n",
    "]\n",
    "different_predictions.to_csv(\n",
    "    f\"sentence_phrase_differences_{approach}.csv\"\n",
    ")\n",
    "print(\"number of health related predictions:\")\n",
    "print(different_predictions[[\"phrase_prediction\", \"sentence_prediction\"]].sum())\n",
    "print()\n",
    "print(\"number of incorrect health related predictions:\")\n",
    "print(pd.Series([(different_predictions.phrase_prediction != different_predictions.phrase_label).sum(), \n",
    "     (different_predictions.sentence_prediction != different_predictions.sentence_label).sum()], index=[\"phrase_prediction\", \"sentence_prediction\"]))\n",
    "print()\n",
    "different_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efee958",
   "metadata": {},
   "source": [
    "# Wikidata Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_causenet_errors.loc[health_causenet_errors.cause == \"influenza virus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(health_causenet_errors.iloc[:, [0, -2, 1, -1]].values[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_causenet.loc[\n",
    "    (test_causenet.cause_origin == \"wd:Q87075524\")\n",
    "    | (test_causenet.effect_origin == \"wd:Q87075524\"),\n",
    "    [\"cause\", \"effect\", \"cause_origin\", \"effect_origin\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which origins are exclusive\n",
    "health_causenet_errors.loc[\n",
    "    (health_causenet_errors.cause_origin == \"wd:Q87075524\")\n",
    "    | (health_causenet_errors.effect_origin == \"wd:Q87075524\")\n",
    "].head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = health_causenet_errors.iloc[:, 5]\n",
    "fn = ~health_causenet_errors.iloc[:, 5]\n",
    "error_origin = (\n",
    "    test_wikidata.set_index([\"cause\", \"effect\"])\n",
    "    .loc[\n",
    "        health_causenet_errors.set_index([\"cause\", \"effect\"]).index,\n",
    "        [\"cause_origin\", \"effect_origin\"],\n",
    "    ]\n",
    "    .reset_index()\n",
    ")\n",
    "error_origin[\"label\"] = \"tp\"\n",
    "error_origin.loc[fn.values, \"label\"] = \"fn\"\n",
    "error_cause_origin = (\n",
    "    error_origin.set_index([\"label\", \"cause\"])\n",
    "    .cause_origin.str.split(\"|\", expand=True)\n",
    "    .stack()\n",
    "    .reset_index()\n",
    "    .drop(\"level_2\", axis=1)\n",
    "    .drop_duplicates()\n",
    "    .rename({0: \"origin\"}, axis=1)\n",
    ")\n",
    "error_effect_origin = (\n",
    "    error_origin.set_index([\"label\", \"effect\"])\n",
    "    .cause_origin.str.split(\"|\", expand=True)\n",
    "    .stack()\n",
    "    .reset_index()\n",
    "    .drop(\"level_2\", axis=1)\n",
    "    .drop_duplicates()\n",
    "    .rename({0: \"origin\"}, axis=1)\n",
    ")\n",
    "origin_value_counts = (\n",
    "    error_cause_origin.groupby(\"label\").origin.value_counts()\n",
    "    + error_effect_origin.groupby(\"label\").origin.value_counts()\n",
    ").unstack(0)\n",
    "origin_value_counts[\"perc\"] = origin_value_counts.tp / origin_value_counts.sum(axis=1)\n",
    "origin_value_counts.sort_values(\"perc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
